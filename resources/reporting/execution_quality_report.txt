================================================================================
EXECUTION QUALITY & TRANSACTION COST ANALYSIS (TCA) REPORTING
High-Frequency Trading Platform
================================================================================

OVERVIEW:
Comprehensive execution quality analysis including Transaction Cost Analysis
(TCA), slippage monitoring, market impact analysis, venue performance,
fill rate analysis, price improvement, and execution benchmarking.

================================================================================
1. TCA DATA MODEL
================================================================================

-- PostgreSQL Schema for Transaction Cost Analysis

-- Execution Analytics
CREATE TABLE execution_analytics (
    execution_id BIGSERIAL PRIMARY KEY,
    trade_date DATE NOT NULL,
    execution_time TIMESTAMP NOT NULL,

    -- Order Details
    order_id VARCHAR(50) NOT NULL,
    parent_order_id VARCHAR(50),
    strategy_id VARCHAR(50) NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    venue VARCHAR(20) NOT NULL,

    -- Order Characteristics
    order_type VARCHAR(20),  -- 'market', 'limit', 'ioc', 'fok', etc.
    side CHAR(1),  -- 'B' or 'S'
    order_quantity BIGINT,
    filled_quantity BIGINT,
    remaining_quantity BIGINT,
    avg_fill_price NUMERIC(12,6),

    -- Benchmark Prices
    decision_price NUMERIC(12,6),       -- Price when decision made
    submission_price NUMERIC(12,6),     -- Price at order submission
    arrival_price NUMERIC(12,6),        -- Price at market arrival
    vwap_price NUMERIC(12,6),           -- Volume-weighted average price
    twap_price NUMERIC(12,6),           -- Time-weighted average price
    close_price NUMERIC(12,6),          -- Closing price

    -- Cost Components (in basis points)
    implementation_shortfall_bps NUMERIC(8,4),
    arrival_cost_bps NUMERIC(8,4),
    timing_cost_bps NUMERIC(8,4),
    market_impact_bps NUMERIC(8,4),
    spread_cost_bps NUMERIC(8,4),
    slippage_bps NUMERIC(8,4),
    opportunity_cost_bps NUMERIC(8,4),

    -- Explicit Costs
    commission NUMERIC(12,4),
    exchange_fees NUMERIC(12,4),
    clearing_fees NUMERIC(12,4),
    regulatory_fees NUMERIC(12,4),
    rebates NUMERIC(12,4),

    -- Execution Metrics
    fill_rate NUMERIC(6,4),
    time_to_fill_ms BIGINT,
    num_partial_fills INTEGER,
    price_improvement_bps NUMERIC(8,4),

    -- Liquidity Metrics
    order_book_depth_bbo BIGINT,
    spread_at_submission_bps NUMERIC(8,4),
    market_volume_during_execution BIGINT,
    participation_rate NUMERIC(8,6),

    -- Venue Performance
    venue_latency_us INTEGER,
    ack_latency_us INTEGER,
    fill_latency_us INTEGER,

    -- Quality Flags
    is_aggressive BOOLEAN,
    is_passive BOOLEAN,
    crossed_spread BOOLEAN,
    hit_limit BOOLEAN,

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_execution_analytics_date ON execution_analytics(trade_date DESC);
CREATE INDEX idx_execution_analytics_strategy ON execution_analytics(strategy_id, trade_date DESC);
CREATE INDEX idx_execution_analytics_symbol ON execution_analytics(symbol, trade_date DESC);
CREATE INDEX idx_execution_analytics_venue ON execution_analytics(venue, trade_date DESC);
CREATE INDEX idx_execution_analytics_order ON execution_analytics(order_id);

-- Venue Performance Metrics
CREATE TABLE venue_performance (
    performance_id BIGSERIAL PRIMARY KEY,
    report_date DATE NOT NULL,
    venue VARCHAR(20) NOT NULL,
    symbol VARCHAR(20),

    -- Volume Statistics
    total_orders INTEGER DEFAULT 0,
    total_fills INTEGER DEFAULT 0,
    total_volume BIGINT DEFAULT 0,
    total_notional NUMERIC(18,2) DEFAULT 0,

    -- Fill Quality
    avg_fill_rate NUMERIC(6,4),
    complete_fill_rate NUMERIC(6,4),
    partial_fill_rate NUMERIC(6,4),
    no_fill_rate NUMERIC(6,4),

    -- Execution Quality
    avg_implementation_shortfall_bps NUMERIC(8,4),
    avg_slippage_bps NUMERIC(8,4),
    avg_spread_cost_bps NUMERIC(8,4),
    avg_market_impact_bps NUMERIC(8,4),
    avg_price_improvement_bps NUMERIC(8,4),

    -- Speed Metrics
    avg_latency_us INTEGER,
    p50_latency_us INTEGER,
    p95_latency_us INTEGER,
    p99_latency_us INTEGER,
    avg_time_to_fill_ms BIGINT,

    -- Costs
    total_commission NUMERIC(12,4),
    total_fees NUMERIC(12,4),
    total_rebates NUMERIC(12,4),
    net_cost NUMERIC(12,4),

    -- Reject/Error Statistics
    reject_count INTEGER DEFAULT 0,
    cancel_count INTEGER DEFAULT 0,
    timeout_count INTEGER DEFAULT 0,
    error_count INTEGER DEFAULT 0,
    reject_rate NUMERIC(6,4),

    -- Market Making Metrics
    passive_fill_rate NUMERIC(6,4),
    aggressive_fill_rate NUMERIC(6,4),

    created_at TIMESTAMP DEFAULT NOW(),

    CONSTRAINT uk_venue_performance UNIQUE (report_date, venue, symbol)
);

CREATE INDEX idx_venue_performance_date ON venue_performance(report_date DESC);
CREATE INDEX idx_venue_performance_venue ON venue_performance(venue, report_date DESC);

-- Slippage Analysis
CREATE TABLE slippage_analysis (
    slippage_id BIGSERIAL PRIMARY KEY,
    trade_date DATE NOT NULL,
    strategy_id VARCHAR(50) NOT NULL,
    symbol VARCHAR(20) NOT NULL,
    venue VARCHAR(20),

    -- Order Characteristics
    order_size_category VARCHAR(20),  -- 'small', 'medium', 'large', 'block'
    order_urgency VARCHAR(20),        -- 'low', 'medium', 'high'
    market_condition VARCHAR(20),     -- 'calm', 'volatile', 'trending'

    -- Slippage Breakdown
    total_slippage_bps NUMERIC(8,4),
    price_slippage_bps NUMERIC(8,4),
    timing_slippage_bps NUMERIC(8,4),
    volume_slippage_bps NUMERIC(8,4),

    -- Impact Analysis
    permanent_impact_bps NUMERIC(8,4),
    temporary_impact_bps NUMERIC(8,4),
    decay_time_seconds INTEGER,

    -- Context
    pre_trade_volatility NUMERIC(8,6),
    pre_trade_spread_bps NUMERIC(8,4),
    market_volume BIGINT,

    created_at TIMESTAMP DEFAULT NOW()
);

CREATE INDEX idx_slippage_analysis_date ON slippage_analysis(trade_date DESC);
CREATE INDEX idx_slippage_analysis_strategy ON slippage_analysis(strategy_id, trade_date DESC);

-- Algo Performance Tracking
CREATE TABLE algo_performance (
    algo_id BIGSERIAL PRIMARY KEY,
    report_date DATE NOT NULL,
    algo_name VARCHAR(50) NOT NULL,
    strategy_id VARCHAR(50),

    -- Execution Statistics
    total_orders INTEGER DEFAULT 0,
    successful_orders INTEGER DEFAULT 0,
    failed_orders INTEGER DEFAULT 0,
    avg_execution_time_ms BIGINT,

    -- Quality Metrics
    avg_implementation_shortfall_bps NUMERIC(8,4),
    std_implementation_shortfall_bps NUMERIC(8,4),
    avg_slippage_bps NUMERIC(8,4),
    avg_market_impact_bps NUMERIC(8,4),

    -- Benchmark Performance
    vs_vwap_bps NUMERIC(8,4),
    vs_twap_bps NUMERIC(8,4),
    vs_arrival_bps NUMERIC(8,4),

    -- Efficiency
    completion_rate NUMERIC(6,4),
    avg_fill_rate NUMERIC(6,4),
    participation_rate NUMERIC(6,4),

    created_at TIMESTAMP DEFAULT NOW(),

    CONSTRAINT uk_algo_performance UNIQUE (report_date, algo_name, strategy_id)
);

================================================================================
2. SQL QUERIES FOR EXECUTION ANALYSIS
================================================================================

-- Daily Execution Quality Summary
SELECT
    trade_date,
    strategy_id,
    COUNT(*) as total_executions,
    AVG(fill_rate) as avg_fill_rate,
    AVG(implementation_shortfall_bps) as avg_impl_shortfall,
    AVG(slippage_bps) as avg_slippage,
    AVG(market_impact_bps) as avg_market_impact,
    AVG(spread_cost_bps) as avg_spread_cost,
    AVG(price_improvement_bps) as avg_price_improvement,
    AVG(time_to_fill_ms) as avg_fill_time_ms,
    SUM(commission + exchange_fees + clearing_fees + regulatory_fees - rebates) as total_costs
FROM execution_analytics
WHERE trade_date = CURRENT_DATE
GROUP BY trade_date, strategy_id
ORDER BY avg_impl_shortfall DESC;

-- Venue Comparison Analysis
SELECT
    venue,
    COUNT(*) as order_count,
    AVG(fill_rate) as avg_fill_rate,
    AVG(implementation_shortfall_bps) as avg_is_bps,
    AVG(slippage_bps) as avg_slippage_bps,
    AVG(price_improvement_bps) as avg_price_improv_bps,
    AVG(venue_latency_us) as avg_latency_us,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY venue_latency_us) as p95_latency_us,
    AVG(time_to_fill_ms) as avg_fill_time_ms,
    SUM(CASE WHEN fill_rate >= 1.0 THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100 as complete_fill_pct
FROM execution_analytics
WHERE trade_date = CURRENT_DATE
GROUP BY venue
ORDER BY avg_is_bps ASC;

-- Slippage by Order Size
WITH order_sizes AS (
    SELECT
        *,
        CASE
            WHEN order_quantity < 1000 THEN 'small'
            WHEN order_quantity < 5000 THEN 'medium'
            WHEN order_quantity < 10000 THEN 'large'
            ELSE 'block'
        END as size_category
    FROM execution_analytics
    WHERE trade_date >= CURRENT_DATE - INTERVAL '7 days'
)
SELECT
    size_category,
    COUNT(*) as order_count,
    AVG(slippage_bps) as avg_slippage,
    STDDEV(slippage_bps) as std_slippage,
    AVG(market_impact_bps) as avg_impact,
    AVG(fill_rate) as avg_fill_rate
FROM order_sizes
GROUP BY size_category
ORDER BY FIELD(size_category, 'small', 'medium', 'large', 'block');

-- Best Execution Analysis (Reg NMS)
SELECT
    e.symbol,
    e.venue,
    COUNT(*) as execution_count,
    AVG(e.avg_fill_price) as avg_price,
    AVG(e.implementation_shortfall_bps) as avg_is_bps,
    AVG(e.spread_at_submission_bps) as avg_spread_bps,
    SUM(CASE WHEN e.price_improvement_bps > 0 THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100 as price_improv_pct,
    AVG(e.commission + e.exchange_fees + e.clearing_fees + e.regulatory_fees - e.rebates) as avg_net_cost
FROM execution_analytics e
WHERE e.trade_date = CURRENT_DATE
GROUP BY e.symbol, e.venue
HAVING COUNT(*) >= 10
ORDER BY e.symbol, avg_is_bps ASC;

-- Intraday Execution Quality Trends
SELECT
    DATE_TRUNC('hour', execution_time) as hour,
    strategy_id,
    COUNT(*) as execution_count,
    AVG(implementation_shortfall_bps) as avg_is_bps,
    AVG(slippage_bps) as avg_slippage_bps,
    AVG(market_impact_bps) as avg_impact_bps,
    AVG(fill_rate) as avg_fill_rate,
    AVG(time_to_fill_ms) as avg_time_to_fill
FROM execution_analytics
WHERE trade_date = CURRENT_DATE
GROUP BY DATE_TRUNC('hour', execution_time), strategy_id
ORDER BY hour, strategy_id;

-- Adverse Selection Analysis
WITH price_moves AS (
    SELECT
        order_id,
        symbol,
        side,
        avg_fill_price,
        close_price,
        CASE
            WHEN side = 'B' THEN (close_price - avg_fill_price) / avg_fill_price * 10000
            ELSE (avg_fill_price - close_price) / avg_fill_price * 10000
        END as post_trade_drift_bps
    FROM execution_analytics
    WHERE trade_date >= CURRENT_DATE - INTERVAL '30 days'
        AND close_price IS NOT NULL
)
SELECT
    symbol,
    COUNT(*) as trade_count,
    AVG(post_trade_drift_bps) as avg_drift_bps,
    STDDEV(post_trade_drift_bps) as std_drift_bps,
    PERCENTILE_CONT(0.05) WITHIN GROUP (ORDER BY post_trade_drift_bps) as p5_drift,
    PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY post_trade_drift_bps) as p95_drift,
    SUM(CASE WHEN post_trade_drift_bps < 0 THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100 as adverse_selection_pct
FROM price_moves
GROUP BY symbol
HAVING COUNT(*) >= 50
ORDER BY adverse_selection_pct DESC;

-- Fill Rate Analysis by Time of Day
SELECT
    EXTRACT(HOUR FROM execution_time) as hour_of_day,
    venue,
    COUNT(*) as order_count,
    AVG(fill_rate) as avg_fill_rate,
    SUM(CASE WHEN fill_rate = 1.0 THEN 1 ELSE 0 END)::NUMERIC / COUNT(*) * 100 as complete_fill_pct,
    AVG(time_to_fill_ms) as avg_time_to_fill
FROM execution_analytics
WHERE trade_date >= CURRENT_DATE - INTERVAL '7 days'
    AND EXTRACT(HOUR FROM execution_time) BETWEEN 9 AND 16
GROUP BY EXTRACT(HOUR FROM execution_time), venue
ORDER BY hour_of_day, venue;

================================================================================
3. C++ EXECUTION QUALITY ANALYZER
================================================================================

#ifndef EXECUTION_QUALITY_ANALYZER_HPP
#define EXECUTION_QUALITY_ANALYZER_HPP

#include <string>
#include <vector>
#include <map>
#include <memory>
#include <chrono>
#include <numeric>
#include <algorithm>
#include <cmath>

namespace hft {
namespace tca {

struct ExecutionData {
    std::string order_id;
    std::string symbol;
    std::string venue;
    char side;
    int64_t order_quantity;
    int64_t filled_quantity;
    double avg_fill_price;

    // Benchmark prices
    double decision_price;
    double arrival_price;
    double vwap_price;
    double twap_price;

    // Timestamps
    std::chrono::system_clock::time_point decision_time;
    std::chrono::system_clock::time_point submission_time;
    std::chrono::system_clock::time_point first_fill_time;
    std::chrono::system_clock::time_point last_fill_time;

    // Market context
    double spread_at_submission;
    int64_t book_depth_bbo;
    double volatility;

    // Costs
    double commission;
    double exchange_fees;
    double rebates;
};

struct TCAMetrics {
    // Implementation Shortfall Components
    double implementation_shortfall_bps;
    double arrival_cost_bps;
    double timing_cost_bps;
    double market_impact_bps;

    // Other costs
    double spread_cost_bps;
    double slippage_bps;
    double explicit_cost_bps;

    // Performance metrics
    double fill_rate;
    int64_t time_to_fill_ms;
    double price_improvement_bps;

    // Benchmark comparison
    double vs_vwap_bps;
    double vs_twap_bps;
};

class ExecutionQualityAnalyzer {
public:
    ExecutionQualityAnalyzer() = default;

    // Calculate comprehensive TCA metrics
    TCAMetrics calculateTCA(const ExecutionData& exec) {
        TCAMetrics metrics;

        // Fill rate
        metrics.fill_rate = static_cast<double>(exec.filled_quantity) / exec.order_quantity;

        // Time to fill
        auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(
            exec.last_fill_time - exec.submission_time);
        metrics.time_to_fill_ms = duration.count();

        // Calculate costs in basis points
        double signed_direction = (exec.side == 'B') ? 1.0 : -1.0;

        // Implementation Shortfall = (Execution Price - Decision Price) * Direction
        metrics.implementation_shortfall_bps =
            (exec.avg_fill_price - exec.decision_price) / exec.decision_price * 10000.0 * signed_direction;

        // Arrival Cost = (Execution Price - Arrival Price) * Direction
        metrics.arrival_cost_bps =
            (exec.avg_fill_price - exec.arrival_price) / exec.arrival_price * 10000.0 * signed_direction;

        // Timing Cost = (Arrival Price - Decision Price) * Direction
        metrics.timing_cost_bps =
            (exec.arrival_price - exec.decision_price) / exec.decision_price * 10000.0 * signed_direction;

        // Market Impact (permanent + temporary)
        metrics.market_impact_bps = estimateMarketImpact(exec);

        // Spread Cost
        metrics.spread_cost_bps = exec.spread_at_submission / exec.arrival_price * 10000.0;

        // Slippage
        metrics.slippage_bps = metrics.arrival_cost_bps - (metrics.spread_cost_bps / 2.0);

        // Explicit Costs
        double total_cost = (exec.commission + exec.exchange_fees - exec.rebates);
        double notional = exec.filled_quantity * exec.avg_fill_price;
        metrics.explicit_cost_bps = (total_cost / notional) * 10000.0;

        // Benchmark comparisons
        metrics.vs_vwap_bps =
            (exec.avg_fill_price - exec.vwap_price) / exec.vwap_price * 10000.0 * signed_direction;
        metrics.vs_twap_bps =
            (exec.avg_fill_price - exec.twap_price) / exec.twap_price * 10000.0 * signed_direction;

        // Price Improvement (negative is good)
        metrics.price_improvement_bps = -metrics.arrival_cost_bps;

        return metrics;
    }

    // Estimate market impact using square-root model
    double estimateMarketImpact(const ExecutionData& exec) {
        // Simplified market impact model
        // Impact ~ volatility * sqrt(order_size / average_daily_volume)

        // This is a placeholder - real implementation would use historical ADV
        double participation_rate = 0.05;  // Assume 5% of volume
        double impact_coefficient = 0.1;   // Calibrated parameter

        double impact = impact_coefficient * exec.volatility *
                       std::sqrt(participation_rate) * 10000.0;

        return impact;
    }

    // Analyze venue performance
    struct VenueMetrics {
        std::string venue;
        int execution_count;
        double avg_fill_rate;
        double avg_implementation_shortfall_bps;
        double avg_latency_us;
        double complete_fill_percentage;
    };

    std::vector<VenueMetrics> analyzeVenuePerformance(
        const std::vector<ExecutionData>& executions) {

        std::map<std::string, std::vector<TCAMetrics>> venue_metrics;
        std::map<std::string, std::vector<int64_t>> venue_latencies;

        for (const auto& exec : executions) {
            TCAMetrics tca = calculateTCA(exec);
            venue_metrics[exec.venue].push_back(tca);

            // Calculate latency
            auto latency = std::chrono::duration_cast<std::chrono::microseconds>(
                exec.first_fill_time - exec.submission_time);
            venue_latencies[exec.venue].push_back(latency.count());
        }

        std::vector<VenueMetrics> results;

        for (const auto& [venue, metrics] : venue_metrics) {
            VenueMetrics vm;
            vm.venue = venue;
            vm.execution_count = metrics.size();

            // Calculate averages
            vm.avg_fill_rate = 0.0;
            vm.avg_implementation_shortfall_bps = 0.0;
            int complete_fills = 0;

            for (const auto& m : metrics) {
                vm.avg_fill_rate += m.fill_rate;
                vm.avg_implementation_shortfall_bps += m.implementation_shortfall_bps;
                if (m.fill_rate >= 0.9999) complete_fills++;
            }

            vm.avg_fill_rate /= metrics.size();
            vm.avg_implementation_shortfall_bps /= metrics.size();
            vm.complete_fill_percentage = (complete_fills * 100.0) / metrics.size();

            // Calculate average latency
            const auto& latencies = venue_latencies[venue];
            vm.avg_latency_us = std::accumulate(latencies.begin(), latencies.end(), 0.0)
                               / latencies.size();

            results.push_back(vm);
        }

        // Sort by implementation shortfall (best execution)
        std::sort(results.begin(), results.end(),
                 [](const VenueMetrics& a, const VenueMetrics& b) {
                     return a.avg_implementation_shortfall_bps < b.avg_implementation_shortfall_bps;
                 });

        return results;
    }

    // Slippage analysis
    struct SlippageBreakdown {
        double total_slippage_bps;
        double price_slippage_bps;
        double timing_slippage_bps;
        double impact_slippage_bps;
        double spread_slippage_bps;
    };

    SlippageBreakdown analyzeSlippage(const ExecutionData& exec) {
        TCAMetrics tca = calculateTCA(exec);

        SlippageBreakdown breakdown;
        breakdown.total_slippage_bps = tca.slippage_bps;
        breakdown.timing_slippage_bps = tca.timing_cost_bps;
        breakdown.impact_slippage_bps = tca.market_impact_bps;
        breakdown.spread_slippage_bps = tca.spread_cost_bps / 2.0;

        // Price slippage is the remainder
        breakdown.price_slippage_bps = breakdown.total_slippage_bps -
                                       breakdown.timing_slippage_bps -
                                       breakdown.impact_slippage_bps -
                                       breakdown.spread_slippage_bps;

        return breakdown;
    }

    // Generate execution quality report
    std::string generateExecutionReport(const std::vector<ExecutionData>& executions) {
        if (executions.empty()) {
            return "No executions to report";
        }

        std::ostringstream report;
        report << "=== EXECUTION QUALITY REPORT ===\n\n";

        // Calculate aggregate metrics
        double total_is = 0.0;
        double total_slippage = 0.0;
        double total_fill_rate = 0.0;
        int total_fills = 0;

        for (const auto& exec : executions) {
            TCAMetrics tca = calculateTCA(exec);
            total_is += tca.implementation_shortfall_bps;
            total_slippage += tca.slippage_bps;
            total_fill_rate += tca.fill_rate;
            if (tca.fill_rate >= 0.9999) total_fills++;
        }

        report << "Total Executions: " << executions.size() << "\n";
        report << "Complete Fills: " << total_fills << " ("
               << (total_fills * 100.0 / executions.size()) << "%)\n";
        report << "Avg Fill Rate: " << std::fixed << std::setprecision(2)
               << (total_fill_rate / executions.size() * 100.0) << "%\n";
        report << "Avg Implementation Shortfall: "
               << (total_is / executions.size()) << " bps\n";
        report << "Avg Slippage: " << (total_slippage / executions.size()) << " bps\n\n";

        // Venue analysis
        report << "=== VENUE PERFORMANCE ===\n";
        auto venue_metrics = analyzeVenuePerformance(executions);

        for (const auto& vm : venue_metrics) {
            report << "Venue: " << vm.venue << "\n";
            report << "  Executions: " << vm.execution_count << "\n";
            report << "  Avg Fill Rate: " << (vm.avg_fill_rate * 100.0) << "%\n";
            report << "  Avg IS: " << vm.avg_implementation_shortfall_bps << " bps\n";
            report << "  Avg Latency: " << vm.avg_latency_us << " us\n";
            report << "  Complete Fills: " << vm.complete_fill_percentage << "%\n\n";
        }

        return report.str();
    }
};

} // namespace tca
} // namespace hft

#endif // EXECUTION_QUALITY_ANALYZER_HPP

================================================================================
4. PYTHON TCA ANALYSIS SCRIPTS
================================================================================

#!/usr/bin/env python3
"""
Transaction Cost Analysis (TCA) Module
"""

import psycopg2
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from typing import Dict, List
import scipy.stats as stats

class TCAAnalyzer:
    def __init__(self, db_config: Dict[str, str]):
        self.conn = psycopg2.connect(**db_config)

    def get_execution_data(self, start_date: str, end_date: str) -> pd.DataFrame:
        """Fetch execution analytics data"""
        query = """
            SELECT * FROM execution_analytics
            WHERE trade_date BETWEEN %s AND %s
            ORDER BY execution_time
        """
        return pd.read_sql_query(query, self.conn, params=(start_date, end_date))

    def calculate_implementation_shortfall(self, df: pd.DataFrame) -> pd.DataFrame:
        """Calculate implementation shortfall decomposition"""
        # IS = (Execution Price - Decision Price) / Decision Price * 10000
        df['is_bps_calc'] = np.where(
            df['side'] == 'B',
            (df['avg_fill_price'] - df['decision_price']) / df['decision_price'] * 10000,
            (df['decision_price'] - df['avg_fill_price']) / df['decision_price'] * 10000
        )

        return df

    def venue_comparison_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Compare execution quality across venues"""
        venue_stats = df.groupby('venue').agg({
            'order_id': 'count',
            'fill_rate': 'mean',
            'implementation_shortfall_bps': ['mean', 'std', 'median'],
            'slippage_bps': ['mean', 'std'],
            'price_improvement_bps': 'mean',
            'time_to_fill_ms': ['mean', 'median'],
            'venue_latency_us': ['mean', 'median', lambda x: np.percentile(x, 95)]
        }).round(4)

        venue_stats.columns = ['_'.join(col).strip() for col in venue_stats.columns.values]
        venue_stats = venue_stats.rename(columns={'order_id_count': 'execution_count'})

        # Add complete fill rate
        complete_fills = df[df['fill_rate'] >= 0.9999].groupby('venue').size()
        total_orders = df.groupby('venue').size()
        venue_stats['complete_fill_pct'] = (complete_fills / total_orders * 100).fillna(0)

        return venue_stats.sort_values('implementation_shortfall_bps_mean')

    def slippage_attribution(self, df: pd.DataFrame) -> pd.DataFrame:
        """Attribute slippage to various components"""
        slippage_breakdown = df.groupby('strategy_id').agg({
            'slippage_bps': 'mean',
            'timing_cost_bps': 'mean',
            'market_impact_bps': 'mean',
            'spread_cost_bps': 'mean'
        }).round(4)

        # Calculate residual (unexplained slippage)
        slippage_breakdown['residual_bps'] = (
            slippage_breakdown['slippage_bps'] -
            slippage_breakdown['timing_cost_bps'] -
            slippage_breakdown['market_impact_bps'] -
            (slippage_breakdown['spread_cost_bps'] / 2.0)
        )

        return slippage_breakdown

    def analyze_price_improvement(self, df: pd.DataFrame) -> Dict:
        """Analyze price improvement statistics"""
        price_improv = df['price_improvement_bps'].dropna()

        return {
            'mean_improvement': price_improv.mean(),
            'median_improvement': price_improv.median(),
            'pct_with_improvement': (price_improv > 0).sum() / len(price_improv) * 100,
            'total_improvement_value': (price_improv * df['filled_quantity'] *
                                       df['avg_fill_price'] / 10000).sum()
        }

    def time_of_day_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Analyze execution quality by time of day"""
        df['hour'] = pd.to_datetime(df['execution_time']).dt.hour

        hourly_stats = df.groupby('hour').agg({
            'order_id': 'count',
            'implementation_shortfall_bps': ['mean', 'std'],
            'fill_rate': 'mean',
            'time_to_fill_ms': 'mean',
            'spread_at_submission_bps': 'mean',
            'market_volume_during_execution': 'mean'
        }).round(4)

        hourly_stats.columns = ['_'.join(col).strip() for col in hourly_stats.columns.values]

        return hourly_stats

    def adverse_selection_analysis(self, df: pd.DataFrame) -> pd.DataFrame:
        """Analyze adverse selection (post-trade drift)"""
        # Calculate post-trade drift
        df['post_trade_drift_bps'] = np.where(
            df['side'] == 'B',
            (df['close_price'] - df['avg_fill_price']) / df['avg_fill_price'] * 10000,
            (df['avg_fill_price'] - df['close_price']) / df['avg_fill_price'] * 10000
        )

        adverse_selection = df.groupby('strategy_id').agg({
            'post_trade_drift_bps': ['mean', 'std', 'median'],
            'order_id': 'count'
        }).round(4)

        adverse_selection.columns = ['_'.join(col).strip() for col in adverse_selection.columns.values]

        # Calculate percentage of adverse trades
        adverse_pct = df[df['post_trade_drift_bps'] < 0].groupby('strategy_id').size()
        total_trades = df.groupby('strategy_id').size()
        adverse_selection['adverse_pct'] = (adverse_pct / total_trades * 100).fillna(0)

        return adverse_selection

    def visualize_tca(self, df: pd.DataFrame):
        """Create comprehensive TCA visualization"""
        fig, axes = plt.subplots(3, 2, figsize=(16, 14))

        # 1. Implementation Shortfall Distribution
        axes[0, 0].hist(df['implementation_shortfall_bps'].dropna(), bins=50,
                       alpha=0.7, edgecolor='black')
        axes[0, 0].axvline(df['implementation_shortfall_bps'].mean(),
                          color='red', linestyle='--', label='Mean')
        axes[0, 0].set_xlabel('Implementation Shortfall (bps)')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Implementation Shortfall Distribution')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)

        # 2. Venue Comparison
        venue_is = df.groupby('venue')['implementation_shortfall_bps'].mean().sort_values()
        axes[0, 1].barh(venue_is.index, venue_is.values)
        axes[0, 1].set_xlabel('Avg Implementation Shortfall (bps)')
        axes[0, 1].set_title('Venue Performance Comparison')
        axes[0, 1].grid(True, alpha=0.3)

        # 3. Fill Rate vs Implementation Shortfall
        axes[1, 0].scatter(df['fill_rate'], df['implementation_shortfall_bps'], alpha=0.5)
        axes[1, 0].set_xlabel('Fill Rate')
        axes[1, 0].set_ylabel('Implementation Shortfall (bps)')
        axes[1, 0].set_title('Fill Rate vs IS')
        axes[1, 0].grid(True, alpha=0.3)

        # 4. Time to Fill Distribution
        axes[1, 1].hist(df['time_to_fill_ms'].dropna(), bins=50, alpha=0.7, edgecolor='black')
        axes[1, 1].set_xlabel('Time to Fill (ms)')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Time to Fill Distribution')
        axes[1, 1].grid(True, alpha=0.3)

        # 5. Slippage Components
        slippage_components = df[['timing_cost_bps', 'market_impact_bps',
                                  'spread_cost_bps']].mean()
        axes[2, 0].bar(range(len(slippage_components)), slippage_components.values)
        axes[2, 0].set_xticks(range(len(slippage_components)))
        axes[2, 0].set_xticklabels(['Timing', 'Impact', 'Spread'], rotation=0)
        axes[2, 0].set_ylabel('Average Cost (bps)')
        axes[2, 0].set_title('Slippage Components')
        axes[2, 0].grid(True, alpha=0.3)

        # 6. Intraday Execution Quality
        df['hour'] = pd.to_datetime(df['execution_time']).dt.hour
        hourly_is = df.groupby('hour')['implementation_shortfall_bps'].mean()
        axes[2, 1].plot(hourly_is.index, hourly_is.values, marker='o')
        axes[2, 1].set_xlabel('Hour of Day')
        axes[2, 1].set_ylabel('Avg Implementation Shortfall (bps)')
        axes[2, 1].set_title('Intraday Execution Quality')
        axes[2, 1].grid(True, alpha=0.3)

        plt.tight_layout()
        return fig

    def generate_tca_report(self, start_date: str, end_date: str) -> str:
        """Generate comprehensive TCA HTML report"""
        df = self.get_execution_data(start_date, end_date)

        if df.empty:
            return "<p>No execution data available</p>"

        # Calculate summary statistics
        total_executions = len(df)
        avg_is = df['implementation_shortfall_bps'].mean()
        avg_slippage = df['slippage_bps'].mean()
        avg_fill_rate = df['fill_rate'].mean()

        # Venue comparison
        venue_comparison = self.venue_comparison_analysis(df)

        # Price improvement
        price_improv_stats = self.analyze_price_improvement(df)

        html = f"""
        <!DOCTYPE html>
        <html>
        <head>
            <title>Transaction Cost Analysis Report</title>
            <style>
                body {{ font-family: Arial, sans-serif; margin: 20px; }}
                h1 {{ color: #1976d2; }}
                h2 {{ color: #666; margin-top: 30px; }}
                table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}
                th, td {{ border: 1px solid #ddd; padding: 12px; text-align: right; }}
                th {{ background-color: #1976d2; color: white; }}
                tr:nth-child(even) {{ background-color: #f2f2f2; }}
                .good {{ color: green; font-weight: bold; }}
                .bad {{ color: red; font-weight: bold; }}
            </style>
        </head>
        <body>
            <h1>Transaction Cost Analysis Report</h1>
            <p>Period: {start_date} to {end_date}</p>

            <h2>Executive Summary</h2>
            <table style="width: 60%;">
                <tr><td>Total Executions:</td><td>{total_executions:,}</td></tr>
                <tr><td>Avg Implementation Shortfall:</td><td class="{'good' if avg_is < 5 else 'bad'}">{avg_is:.2f} bps</td></tr>
                <tr><td>Avg Slippage:</td><td>{avg_slippage:.2f} bps</td></tr>
                <tr><td>Avg Fill Rate:</td><td>{avg_fill_rate*100:.2f}%</td></tr>
                <tr><td>Avg Price Improvement:</td><td class="good">{price_improv_stats['mean_improvement']:.2f} bps</td></tr>
                <tr><td>% Orders with Price Improvement:</td><td>{price_improv_stats['pct_with_improvement']:.1f}%</td></tr>
            </table>

            <h2>Venue Performance Comparison</h2>
            {venue_comparison.to_html()}

        </body>
        </html>
        """

        return html

    def close(self):
        self.conn.close()

if __name__ == '__main__':
    db_config = {
        'host': 'localhost',
        'database': 'hft_trading',
        'user': 'trading_user',
        'password': 'secure_password'
    }

    analyzer = TCAAnalyzer(db_config)

    # Generate TCA report
    end_date = datetime.now().strftime('%Y-%m-%d')
    start_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')

    html_report = analyzer.generate_tca_report(start_date, end_date)

    # Save report
    with open(f'tca_report_{end_date}.html', 'w') as f:
        f.write(html_report)

    print("TCA report generated successfully")

    analyzer.close()

================================================================================
END OF EXECUTION QUALITY REPORT DOCUMENTATION
================================================================================
