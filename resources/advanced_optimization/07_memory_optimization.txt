================================================================================
MEMORY OPTIMIZATION FOR HIGH-FREQUENCY TRADING
Huge Pages, NUMA, Memory Pools, and Cache Optimization
================================================================================

INTRODUCTION
============================================================================

Memory access patterns significantly impact HFT system latency. This document
covers memory optimization techniques including huge pages, NUMA awareness,
custom allocators, and cache-friendly data structures.

TYPICAL IMPROVEMENTS:
- Latency reduction: 10-30%
- TLB miss reduction: 90-99%
- Cache hit rate improvement: 20-40%
- Page fault elimination: 100%

Cost: $0 (configuration + code changes)
Implementation: 2-4 weeks

MEMORY ACCESS LATENCY
============================================================================

CACHE HIERARCHY
---------------

Level   | Size      | Latency    | Bandwidth
--------|-----------|------------|-------------
L1      | 32-64 KB  | 0.5-1 ns   | 1-2 TB/s
L2      | 256KB-1MB | 3-10 ns    | 200-400 GB/s
L3      | 10-50 MB  | 20-50 ns   | 50-100 GB/s
RAM     | 64-512 GB | 80-120 ns  | 10-50 GB/s

L1 is 100x faster than RAM!

TLB (TRANSLATION LOOKASIDE BUFFER)
-----------------------------------

TLB caches virtual-to-physical address translations.

Standard page size: 4 KB
TLB entries: 64-512 (CPU dependent)
TLB coverage: 256KB - 2MB

TLB miss latency: 10-100 ns

For large applications (>2MB working set), TLB misses are common.

HUGE PAGES
============================================================================

OVERVIEW
--------

Huge pages are larger memory pages that reduce TLB pressure.

Page Sizes:
- Standard: 4 KB
- Huge (2MB): 512x larger
- Huge (1GB): 262,144x larger

Benefits:
- Fewer TLB entries needed
- Fewer TLB misses
- Lower latency
- Better for large memory applications

CONFIGURATION
-------------

1. CHECK HUGE PAGE SUPPORT

```bash
grep -i hugepage /proc/meminfo

Output:
AnonHugePages:         0 kB
HugePages_Total:       0
HugePages_Free:        0
HugePages_Rsvd:        0
HugePages_Surp:        0
Hugepagesize:       2048 kB
```

2. ENABLE 2MB HUGE PAGES

```bash
# Reserve 1024 huge pages (2GB total)
echo 1024 | sudo tee /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

# Make persistent (/etc/sysctl.conf)
vm.nr_hugepages = 1024

# Apply
sudo sysctl -p
```

3. ENABLE 1GB HUGE PAGES (REQUIRES BOOT PARAMETER)

Edit /etc/default/grub:
```
GRUB_CMDLINE_LINUX="hugepagesz=1G hugepages=8"
```

This reserves 8x 1GB pages (8GB) at boot.

```bash
sudo update-grub
sudo reboot
```

4. VERIFY

```bash
cat /proc/meminfo | grep -i huge

HugePages_Total:    1024
HugePages_Free:     1024
Hugepagesize:       2048 kB
```

USING HUGE PAGES IN C/C++
--------------------------

Method 1: mmap with MAP_HUGETLB

```c
#include <sys/mman.h>

#define SIZE (2 * 1024 * 1024)  // 2MB

void* buffer = mmap(NULL, SIZE, PROT_READ | PROT_WRITE,
                    MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB,
                    -1, 0);
if (buffer == MAP_FAILED) {
    perror("mmap");
    return -1;
}

// Use buffer...

munmap(buffer, SIZE);
```

Method 2: Hugetlbfs mount

```bash
# Mount hugetlbfs
sudo mkdir /mnt/huge
sudo mount -t hugetlbfs nodev /mnt/huge

# In /etc/fstab for persistence:
nodev /mnt/huge hugetlbfs defaults 0 0
```

```c
#include <sys/mman.h>
#include <fcntl.h>

int fd = open("/mnt/huge/hft_buffer", O_CREAT | O_RDWR, 0755);
if (fd < 0) {
    perror("open");
    return -1;
}

ftruncate(fd, SIZE);

void* buffer = mmap(NULL, SIZE, PROT_READ | PROT_WRITE,
                    MAP_SHARED, fd, 0);
if (buffer == MAP_FAILED) {
    perror("mmap");
    return -1;
}

// Use buffer...

munmap(buffer, SIZE);
close(fd);
```

Method 3: Transparent Huge Pages (THP)

Linux can automatically use huge pages (transparent to application).

```bash
# Check THP status
cat /sys/kernel/mm/transparent_hugepage/enabled

[always] madvise never

# Enable THP
echo always | sudo tee /sys/kernel/mm/transparent_hugepage/enabled

# Or enable for specific allocation:
madvise(buffer, size, MADV_HUGEPAGE);
```

THP Caveat: Less predictable than explicit huge pages, may cause latency
spikes during compaction. For HFT, prefer explicit huge pages.

PERFORMANCE IMPACT
------------------

Benchmark: Random memory access (100MB buffer)

Page Size | TLB Misses | Latency
----------|------------|--------
4 KB      | 2.5M/sec   | 100 ns avg
2 MB      | 50K/sec    | 85 ns avg
1 GB      | 1K/sec     | 82 ns avg

Result: 15-20% latency reduction with huge pages for memory-intensive workloads.

MEMORY POOLS AND CUSTOM ALLOCATORS
============================================================================

PROBLEM WITH MALLOC/NEW
------------------------

Standard allocators (malloc/new) have issues for HFT:

1. Latency: 100-1000 ns per allocation (unpredictable)
2. Fragmentation: Memory becomes fragmented over time
3. System calls: May call brk/mmap (kernel involvement)
4. Locking: Thread-safe allocators use locks (contention)
5. Not real-time: No latency guarantees

Solution: Pre-allocate memory pools, custom allocators.

MEMORY POOL IMPLEMENTATION
---------------------------

Concept: Allocate large buffer at startup, hand out chunks as needed.

```c++
#include <cstdint>
#include <cstddef>

template<typename T>
class MemoryPool {
public:
    MemoryPool(size_t poolSize) : poolSize_(poolSize), nextIndex_(0) {
        // Allocate pool with huge pages
        pool_ = static_cast<T*>(mmap(NULL, poolSize * sizeof(T),
                                     PROT_READ | PROT_WRITE,
                                     MAP_PRIVATE | MAP_ANONYMOUS | MAP_HUGETLB,
                                     -1, 0));
        if (pool_ == MAP_FAILED) {
            throw std::runtime_error("mmap failed");
        }
    }

    ~MemoryPool() {
        munmap(pool_, poolSize_ * sizeof(T));
    }

    T* allocate() {
        if (nextIndex_ >= poolSize_) {
            return nullptr;  // Pool exhausted
        }
        return &pool_[nextIndex_++];
    }

    void deallocate(T* ptr) {
        // Simple pool: no deallocation
        // For production, implement free list
    }

private:
    T* pool_;
    size_t poolSize_;
    size_t nextIndex_;
};

// Usage
MemoryPool<Order> orderPool(10000);

Order* order = orderPool.allocate();
// Use order...
// orderPool.deallocate(order);  // If implemented
```

LOCK-FREE MEMORY POOL
----------------------

For multi-threaded access:

```c++
#include <atomic>

template<typename T>
class LockFreeMemoryPool {
public:
    LockFreeMemoryPool(size_t poolSize) : poolSize_(poolSize) {
        pool_ = static_cast<T*>(mmap(...));
        nextIndex_.store(0, std::memory_order_relaxed);
    }

    T* allocate() {
        size_t index = nextIndex_.fetch_add(1, std::memory_order_acquire);
        if (index >= poolSize_) {
            return nullptr;
        }
        return &pool_[index];
    }

private:
    T* pool_;
    size_t poolSize_;
    std::atomic<size_t> nextIndex_;
};
```

OBJECT POOLING
--------------

Reuse objects instead of allocating/deallocating:

```c++
template<typename T>
class ObjectPool {
public:
    ObjectPool(size_t maxObjects) {
        for (size_t i = 0; i < maxObjects; ++i) {
            T* obj = new T();  // Or from memory pool
            freeList_.push(obj);
        }
    }

    T* acquire() {
        if (freeList_.empty()) {
            return nullptr;
        }
        T* obj = freeList_.top();
        freeList_.pop();
        return obj;
    }

    void release(T* obj) {
        // Reset obj state
        obj->reset();
        freeList_.push(obj);
    }

private:
    std::stack<T*> freeList_;
};

// Usage
ObjectPool<Order> orderPool(10000);

Order* order = orderPool.acquire();
// Use order...
orderPool.release(order);  // Return to pool
```

STACK-BASED ALLOCATION
----------------------

For short-lived objects, use stack instead of heap:

```c++
// BAD: Heap allocation
Order* order = new Order();
process(order);
delete order;

// GOOD: Stack allocation
Order order;
process(&order);
```

Stack allocation is:
- Faster (just adjust stack pointer)
- Deterministic (no allocator involvement)
- No fragmentation
- Automatic cleanup

NUMA-AWARE MEMORY ALLOCATION
============================================================================

NUMA OVERVIEW
-------------

Multi-socket servers have NUMA (Non-Uniform Memory Access):

- Socket 0: Cores 0-15, Memory Bank 0
- Socket 1: Cores 16-31, Memory Bank 1

Memory access:
- Local (same socket): 80-120 ns
- Remote (different socket): 150-300 ns

Remote access is 2-3x slower!

NUMA ALLOCATION STRATEGIES
---------------------------

1. LOCAL ALLOCATION (PREFERRED)

```c
#include <numa.h>

// Allocate on NUMA node 0
void* buffer = numa_alloc_onnode(size, 0);

// Use buffer...

numa_free(buffer, size);
```

2. BIND PROCESS TO NUMA NODE

```bash
# Run on node 0 (CPU + memory)
numactl --cpunodebind=0 --membind=0 ./hft_app
```

3. CHECK NUMA STATISTICS

```bash
numastat -p PID

Per-node process memory usage (in MBs)
PID       Node 0  Node 1  Total
12345        1024      12   1036  <- Good, mostly local
```

4. NUMA-AWARE MEMORY POOL

```c++
class NumaMemoryPool {
public:
    NumaMemoryPool(size_t size, int numaNode) : numaNode_(numaNode) {
        buffer_ = numa_alloc_onnode(size, numaNode);
        if (!buffer_) {
            throw std::runtime_error("numa_alloc_onnode failed");
        }
    }

    ~NumaMemoryPool() {
        numa_free(buffer_, size_);
    }

    // Allocate from this pool
    void* allocate(size_t size) {
        // Implement allocation logic
    }

private:
    void* buffer_;
    size_t size_;
    int numaNode_;
};
```

BEST PRACTICE: Align CPU and memory
- Thread on node 0 -> allocate on node 0
- Thread on node 1 -> allocate on node 1

CACHE-FRIENDLY DATA STRUCTURES
============================================================================

CACHE LINE SIZE: 64 bytes

FALSE SHARING
-------------

Problem: Multiple threads accessing different variables in same cache line.

BAD:
```c++
struct Counters {
    int thread1_counter;  // Offset 0
    int thread2_counter;  // Offset 4, SAME CACHE LINE!
};
```

When thread1 writes thread1_counter, thread2's cache line is invalidated,
even though it only cares about thread2_counter.

GOOD:
```c++
struct Counters {
    alignas(64) int thread1_counter;  // Offset 0
    alignas(64) int thread2_counter;  // Offset 64, DIFFERENT CACHE LINE
};
```

Or:
```c++
struct Counter {
    int value;
    char padding[60];  // Pad to 64 bytes
};

Counter thread1_counter;
Counter thread2_counter;
```

DATA STRUCTURE LAYOUT
----------------------

Principle: Hot data together, cold data separate.

BAD:
```c++
struct Order {
    OrderID id;            // Hot
    double price;          // Hot
    int quantity;          // Hot
    string comment;        // Cold
    Timestamp created;     // Cold
    string clientID;       // Cold
};
```

GOOD:
```c++
struct HotOrderData {
    OrderID id;
    double price;
    int quantity;
    // 3 fields, ~16-20 bytes, fits in 1 cache line
};

struct ColdOrderData {
    string comment;
    Timestamp created;
    string clientID;
    // Rarely accessed
};

struct Order {
    HotOrderData hot;
    ColdOrderData* cold;  // Pointer, allocated separately
};
```

Result: Hot path only touches 1 cache line instead of 3-4.

ARRAY OF STRUCTURES VS STRUCTURE OF ARRAYS
-------------------------------------------

For processing arrays:

AOS (Array of Structures):
```c++
struct Order {
    double price;
    int quantity;
};

Order orders[1000];

// Process prices
for (int i = 0; i < 1000; ++i) {
    process(orders[i].price);  // Loads quantity too (wasted)
}
```

SOA (Structure of Arrays):
```c++
struct Orders {
    double prices[1000];
    int quantities[1000];
};

Orders orders;

// Process prices
for (int i = 0; i < 1000; ++i) {
    process(orders.prices[i]);  // Only loads prices (efficient)
}
```

SOA is better for SIMD and cache efficiency when processing one field.

PREFETCHING
-----------

Manually prefetch data before use:

```c++
#include <xmmintrin.h>

// Prefetch next order while processing current
void process_orders(Order* orders, int count) {
    for (int i = 0; i < count; ++i) {
        // Prefetch next order (64 bytes ahead)
        if (i + 1 < count) {
            _mm_prefetch((const char*)&orders[i + 1], _MM_HINT_T0);
        }

        // Process current order
        process_order(&orders[i]);
    }
}
```

Prefetch hints:
- _MM_HINT_T0: Into all cache levels (L1, L2, L3)
- _MM_HINT_T1: Into L2 and L3 (not L1)
- _MM_HINT_T2: Into L3 only
- _MM_HINT_NTA: Non-temporal (bypass cache, for streaming data)

MEMORY ORDERING AND BARRIERS
============================================================================

MEMORY ORDERING ISSUES
-----------------------

CPUs reorder memory operations for performance. This can cause issues in
multi-threaded code.

Example:
```c++
// Thread 1
data = 42;
ready = true;

// Thread 2
while (!ready);
assert(data == 42);  // May fail!
```

CPU may reorder Thread 1's writes, so ready=true happens before data=42.

MEMORY BARRIERS
---------------

1. COMPILER BARRIERS

```c++
asm volatile("" ::: "memory");  // GCC/Clang
```

Prevents compiler from reordering, but not CPU.

2. CPU BARRIERS (X86)

```c++
asm volatile("mfence" ::: "memory");  // Full barrier
asm volatile("lfence" ::: "memory");  // Load barrier
asm volatile("sfence" ::: "memory");  // Store barrier
```

3. C++11 ATOMICS (PORTABLE)

```c++
#include <atomic>

std::atomic<int> data{0};
std::atomic<bool> ready{false};

// Thread 1
data.store(42, std::memory_order_release);
ready.store(true, std::memory_order_release);

// Thread 2
while (!ready.load(std::memory_order_acquire));
assert(data.load(std::memory_order_acquire) == 42);  // OK
```

Memory orders:
- memory_order_relaxed: No ordering guarantees
- memory_order_acquire: Synchronize-with release
- memory_order_release: Synchronize-with acquire
- memory_order_seq_cst: Sequential consistency (default, slowest)

For HFT, use acquire/release when possible (faster than seq_cst).

LOCK-FREE DATA STRUCTURES
==========================

For lowest latency, avoid locks (mutexes).

LOCK-FREE QUEUE (SPSC - SINGLE PRODUCER, SINGLE CONSUMER)
----------------------------------------------------------

```c++
#include <atomic>

template<typename T, size_t Size>
class LockFreeQueue {
public:
    LockFreeQueue() : head_(0), tail_(0) {}

    bool push(const T& item) {
        size_t tail = tail_.load(std::memory_order_relaxed);
        size_t next_tail = (tail + 1) % Size;

        if (next_tail == head_.load(std::memory_order_acquire)) {
            return false;  // Queue full
        }

        buffer_[tail] = item;
        tail_.store(next_tail, std::memory_order_release);
        return true;
    }

    bool pop(T& item) {
        size_t head = head_.load(std::memory_order_relaxed);

        if (head == tail_.load(std::memory_order_acquire)) {
            return false;  // Queue empty
        }

        item = buffer_[head];
        head_.store((head + 1) % Size, std::memory_order_release);
        return true;
    }

private:
    T buffer_[Size];
    alignas(64) std::atomic<size_t> head_;
    alignas(64) std::atomic<size_t> tail_;
};
```

Latency: 10-50 ns per operation (vs 500-2000 ns for mutex-protected queue).

MONITORING MEMORY PERFORMANCE
==============================

TOOLS
-----

1. PERF (CACHE MISSES)

```bash
perf stat -e cache-references,cache-misses ./hft_app

Result:
10,000,000 cache-references
   500,000 cache-misses      # 5% miss rate (good)
```

2. VALGRIND CACHEGRIND

```bash
valgrind --tool=cachegrind ./hft_app

Result: Detailed cache miss breakdown by function/line.
```

3. INTEL VTUNE

```bash
vtune -collect memory-access ./hft_app

Result: Memory bandwidth, NUMA accesses, cache hierarchy analysis.
```

4. NUMASTAT

```bash
numastat -p PID

Result: Per-NUMA-node memory usage and access patterns.
```

EXAMPLE HFT SYSTEM MEMORY CONFIGURATION
========================================

Server: 2-socket, 16 cores per socket, 128GB RAM per socket

Configuration:

1. Huge pages:
   - 2GB of 2MB pages (for buffers)
   - 4GB of 1GB pages (for large data structures)

2. NUMA:
   - Socket 0: Market data threads + buffers
   - Socket 1: Analytics + database

3. Memory pools:
   - Order pool: 10,000 orders pre-allocated on node 0
   - Packet pool: 1,000,000 packets pre-allocated on node 0

4. Lock-free queues:
   - Market data -> Strategy: 10,000 entry SPSC queue
   - Strategy -> Order entry: 1,000 entry SPSC queue

5. Cache optimization:
   - All hot data structures aligned to cache lines
   - No false sharing
   - Prefetching for order book processing

EXPECTED RESULTS
================

Before optimization:
- TLB misses: 2M/sec
- Cache misses: 10%
- NUMA remote accesses: 30%
- Allocation latency: 500 ns avg

After optimization:
- TLB misses: 50K/sec (98% reduction)
- Cache misses: 6% (40% reduction)
- NUMA remote accesses: 2% (93% reduction)
- Allocation latency: 10 ns avg (98% reduction)

Overall latency improvement: 20-30%

CONCLUSION
==========

Memory optimization provides significant latency reductions for HFT systems:

COST: $0 (configuration + code changes)
EFFORT: 2-4 weeks
BENEFIT: 20-30% latency reduction

KEY TECHNIQUES:
1. Huge pages (2MB/1GB) - Reduce TLB misses
2. Memory pools - Eliminate allocation latency
3. NUMA awareness - Avoid remote memory access
4. Cache-friendly layout - Reduce cache misses
5. Lock-free structures - Eliminate lock overhead

RECOMMENDED APPROACH:
Week 1: Enable huge pages, NUMA configuration
Week 2: Implement memory pools
Week 3: Cache optimization, data structure layout
Week 4: Testing, validation, iteration

Every HFT system should implement these optimizations for maximum performance.

Document version: 1.0
Last updated: 2025-01-15
