================================================================================
RDMA AND INFINIBAND FOR HIGH-FREQUENCY TRADING
Ultra-Low Latency Inter-Process Communication
================================================================================

INTRODUCTION
================================================================================

RDMA (Remote Direct Memory Access) is a technology that allows direct memory
access from one computer to another without involving the operating system,
CPU, or cache of either system. For HFT, RDMA enables sub-microsecond
inter-process communication between components.

KEY BENEFITS:
- Latency: 0.5-2 microseconds (vs 10-50 microseconds with TCP)
- Zero-copy: No data copies
- CPU offload: No CPU involved in data transfer
- Bandwidth: 100-400 Gbps (InfiniBand NDR)

TYPICAL USE CASES IN HFT:
- Inter-component communication (market data -> strategy -> order entry)
- Multi-server coordination (distributed order books)
- Historical data access (low-latency database queries)
- Replication and backup (hot-hot failover)

This document covers RDMA technologies, InfiniBand, RoCE, implementation,
and cost-benefit analysis.

RDMA FUNDAMENTALS
================================================================================

TRADITIONAL NETWORK COMMUNICATION
----------------------------------

Traditional TCP socket communication:

1. Application calls send()
2. Data copied to kernel buffer (copy #1)
3. Kernel processes TCP/IP stack
4. Data copied to NIC (copy #2)
5. NIC sends data over network
6. Remote NIC receives data
7. Interrupt to remote CPU
8. Data copied to kernel buffer (copy #3)
9. Kernel processes TCP/IP stack
10. Data copied to application buffer (copy #4)
11. Application processes data

Total: 4 memory copies, 2 CPUs involved, 2 context switches
Latency: 10-50 microseconds

RDMA COMMUNICATION
------------------

RDMA one-sided operation (RDMA WRITE):

1. Application posts RDMA WRITE request to NIC
2. NIC DMAs data directly from application memory
3. NIC sends data over network with RDMA protocol
4. Remote NIC receives data
5. Remote NIC DMAs data directly to remote application memory
6. Optional: Remote application notified (polling or interrupt)

Total: 0 memory copies, 0 CPU involvement (after setup)
Latency: 0.5-2 microseconds

SPEEDUP: 10-50x

RDMA KEY CONCEPTS
-----------------

1. MEMORY REGISTRATION
   Before RDMA operations, memory must be registered with the NIC:
   - Pins memory (prevents swapping)
   - Maps virtual to physical addresses
   - Gives NIC permission to access memory
   - One-time setup overhead

2. QUEUE PAIRS (QPs)
   Communication endpoint:
   - Send Queue (SQ): For sending operations
   - Receive Queue (RQ): For receiving operations
   - One QP per connection
   - Multiple QPs per NIC

3. COMPLETION QUEUES (CQs)
   Notifications of completed operations:
   - Send CQ: SEND/WRITE completed
   - Receive CQ: RECV completed
   - Application polls CQ for completions

4. VERBS API
   Programming interface for RDMA:
   - Create QP, CQ
   - Register memory
   - Post SEND/RECV/READ/WRITE operations
   - Poll CQ for completions

RDMA OPERATIONS
---------------

1. RDMA SEND/RECV (Two-sided)
   - Requires pre-posted RECV buffers
   - Like sockets but zero-copy
   - Receiver must be ready

2. RDMA WRITE (One-sided)
   - Write directly to remote memory
   - Remote CPU not involved
   - Requires remote memory address and key

3. RDMA READ (One-sided)
   - Read directly from remote memory
   - Remote CPU not involved
   - Slightly higher latency than WRITE

4. RDMA ATOMIC (One-sided)
   - Atomic operations (compare-and-swap, fetch-and-add)
   - For lock-free algorithms
   - Low latency synchronization

INFINIBAND VS ROCE
================================================================================

INFINIBAND (IB)
---------------

InfiniBand is a dedicated high-performance network standard designed for
data centers and HPC.

Characteristics:
- Dedicated fabric (not Ethernet)
- Lossless network (flow control built-in)
- Ultra-low latency (< 1 microsecond)
- High bandwidth (200-400 Gbps)
- RDMA native
- Requires InfiniBand switches and NICs

Generations:
- SDR (Single Data Rate): 10 Gbps (obsolete)
- DDR (Double Data Rate): 20 Gbps (obsolete)
- QDR (Quad Data Rate): 40 Gbps (obsolete)
- FDR (Fourteen Data Rate): 56 Gbps (legacy)
- EDR (Enhanced Data Rate): 100 Gbps (current)
- HDR (High Data Rate): 200 Gbps (current)
- NDR (Next Data Rate): 400 Gbps (latest)

Latency:
- Message latency: 0.5-1.0 microsecond
- MPI latency: 0.6 microseconds (typical)

ROCE (RDMA OVER CONVERGED ETHERNET)
------------------------------------

RoCE runs RDMA over standard Ethernet, allowing RDMA on existing networks.

Versions:

1. RoCE v1 (Ethernet Layer)
   - RDMA over Ethernet Layer 2
   - Same subnet only
   - No IP routing
   - Rarely used

2. RoCE v2 (UDP/IP Layer)
   - RDMA over UDP/IP
   - Routable (can cross subnets)
   - More flexible
   - Industry standard

Latency:
- Message latency: 1-2 microseconds
- Slightly higher than InfiniBand

Bandwidth:
- 10/25/40/50/100 Gbps (Ethernet speeds)

COMPARISON: INFINIBAND VS ROCE V2
----------------------------------

Feature              | InfiniBand    | RoCE v2
---------------------|---------------|------------------
Latency              | 0.5-1.0 us    | 1-2 us
Bandwidth            | Up to 400Gbps | Up to 100Gbps
Protocol             | IB            | Ethernet/UDP/IP
Switching            | IB switches   | Ethernet switches
Interoperability     | IB only       | Standard Ethernet
Maturity             | Very mature   | Mature
Cost (NICs)          | $1K-3K        | $800-2K
Cost (Switches)      | $10K-100K     | $5K-50K (DCB req.)
Lossless Network     | Built-in      | Requires DCB/PFC
Reliability          | Very high     | High (with proper config)
HPC Adoption         | Dominant      | Growing
HFT Adoption         | Medium        | Low (but growing)

DECISION GUIDE:
- Need < 1 us latency? -> InfiniBand
- Want maximum bandwidth (200-400G)? -> InfiniBand
- Have existing Ethernet? -> RoCE v2
- Cost-sensitive? -> RoCE v2
- Mixed workloads? -> RoCE v2
- Pure HFT, dedicated? -> InfiniBand

MELLANOX (NVIDIA) PRODUCTS
================================================================================

Mellanox (acquired by NVIDIA in 2020) is the dominant RDMA vendor.

CONNECTX SERIES (NICS)
-----------------------

CONNECTX-5 (EDR InfiniBand / 100Gbps Ethernet)
Price: $800-1,500
Specs:
- 100 Gbps Ethernet or 100 Gbps InfiniBand (EDR)
- Single or dual port
- PCIe Gen3 x16
- RDMA, RoCE v2
- Hardware timestamping

CONNECTX-6 (HDR InfiniBand / 200Gbps Ethernet)
Price: $1,500-2,500
Specs:
- 200 Gbps InfiniBand (HDR) or 100/200 Gbps Ethernet
- Dual port
- PCIe Gen3/4 x16
- Advanced RDMA features
- Hardware offloads

CONNECTX-7 (NDR InfiniBand / 400Gbps Ethernet)
Price: $2,500-4,000
Specs:
- 400 Gbps InfiniBand (NDR) or 100/200/400 Gbps Ethernet
- Dual port
- PCIe Gen5 x16
- Lowest latency
- Latest generation

Recommendation for HFT:
- ConnectX-6: Best price/performance (200 Gbps HDR)
- ConnectX-7: Cutting-edge (400 Gbps NDR)

INFINIBAND SWITCHES
-------------------

MELLANOX/NVIDIA SN2700 (32-port 100G)
Price: $15,000-25,000
- 32x 100Gbps ports (EDR or 100GbE)
- Sub-microsecond latency
- Small/medium deployments

MELLANOX/NVIDIA SN4600 (64-port 200G)
Price: $50,000-80,000
- 64x 200Gbps ports (HDR or 200GbE)
- Ultra-low latency
- Large deployments

Typical HFT setup:
- Small: 1x SN2700 (10-20 servers)
- Medium: 2x SN4600 (30-50 servers)
- Large: Spine-leaf topology (50+ servers)

PROGRAMMING RDMA
================================================================================

VERBS API (LOW-LEVEL)
---------------------

The standard RDMA programming interface.

Example: RDMA SEND/RECV

```c
#include <infiniband/verbs.h>

// 1. Get device list
int num_devices;
struct ibv_device **device_list = ibv_get_device_list(&num_devices);
struct ibv_device *device = device_list[0];

// 2. Open device
struct ibv_context *context = ibv_open_device(device);

// 3. Allocate protection domain
struct ibv_pd *pd = ibv_alloc_pd(context);

// 4. Register memory
char *buffer = malloc(4096);
struct ibv_mr *mr = ibv_reg_mr(pd, buffer, 4096,
    IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE);

// 5. Create completion queue
struct ibv_cq *cq = ibv_create_cq(context, 10, NULL, NULL, 0);

// 6. Create queue pair
struct ibv_qp_init_attr qp_init_attr = {
    .send_cq = cq,
    .recv_cq = cq,
    .cap = {
        .max_send_wr = 10,
        .max_recv_wr = 10,
        .max_send_sge = 1,
        .max_recv_sge = 1,
    },
    .qp_type = IBV_QPT_RC, // Reliable Connection
};
struct ibv_qp *qp = ibv_create_qp(pd, &qp_init_attr);

// 7. Modify QP to RTR (Ready To Receive) and RTS (Ready To Send)
// ... (connection establishment, exchange QP info)

// 8. Post receive buffer
struct ibv_sge recv_sge = {
    .addr = (uintptr_t)buffer,
    .length = 4096,
    .lkey = mr->lkey,
};
struct ibv_recv_wr recv_wr = {
    .wr_id = 1,
    .next = NULL,
    .sg_list = &recv_sge,
    .num_sge = 1,
};
struct ibv_recv_wr *bad_recv_wr;
ibv_post_recv(qp, &recv_wr, &bad_recv_wr);

// 9. Send data
strcpy(buffer, "Hello RDMA");
struct ibv_sge send_sge = {
    .addr = (uintptr_t)buffer,
    .length = strlen(buffer) + 1,
    .lkey = mr->lkey,
};
struct ibv_send_wr send_wr = {
    .wr_id = 2,
    .next = NULL,
    .sg_list = &send_sge,
    .num_sge = 1,
    .opcode = IBV_WR_SEND,
    .send_flags = IBV_SEND_SIGNALED,
};
struct ibv_send_wr *bad_send_wr;
ibv_post_send(qp, &send_wr, &bad_send_wr);

// 10. Poll for completion
struct ibv_wc wc;
int ne;
while ((ne = ibv_poll_cq(cq, 1, &wc)) == 0);
if (ne < 0) {
    // Error
}
if (wc.status != IBV_WC_SUCCESS) {
    // Operation failed
}
// Success!
```

LIBFABRIC (HIGH-LEVEL)
----------------------

Libfabric provides a higher-level, more portable API.

Example: SEND/RECV with libfabric

```c
#include <rdma/fabric.h>
#include <rdma/fi_domain.h>
#include <rdma/fi_endpoint.h>

// 1. Get fabric info
struct fi_info *hints = fi_allocinfo();
hints->ep_attr->type = FI_EP_RDM; // Reliable datagram
hints->caps = FI_MSG;
struct fi_info *info;
fi_getinfo(FI_VERSION(1, 9), NULL, NULL, 0, hints, &info);

// 2. Open fabric
struct fid_fabric *fabric;
fi_fabric(info->fabric_attr, &fabric, NULL);

// 3. Open domain
struct fid_domain *domain;
fi_domain(fabric, info, &domain, NULL);

// 4. Register memory
char *buffer = malloc(4096);
struct fid_mr *mr;
fi_mr_reg(domain, buffer, 4096, FI_SEND | FI_RECV, 0, 0, 0, &mr, NULL);

// 5. Create endpoint
struct fid_ep *ep;
fi_endpoint(domain, info, &ep, NULL);

// 6. Create completion queue
struct fi_cq_attr cq_attr = {0};
cq_attr.format = FI_CQ_FORMAT_MSG;
cq_attr.size = 10;
struct fid_cq *cq;
fi_cq_open(domain, &cq_attr, &cq, NULL);

// 7. Bind endpoint
fi_ep_bind(ep, &cq->fid, FI_SEND | FI_RECV);
fi_enable(ep);

// 8. Post receive
struct fi_msg recv_msg = {0};
struct iovec recv_iov = {
    .iov_base = buffer,
    .iov_len = 4096,
};
recv_msg.msg_iov = &recv_iov;
recv_msg.iov_count = 1;
fi_recvmsg(ep, &recv_msg, 0);

// 9. Send data
strcpy(buffer, "Hello libfabric");
struct fi_msg send_msg = {0};
struct iovec send_iov = {
    .iov_base = buffer,
    .iov_len = strlen(buffer) + 1,
};
send_msg.msg_iov = &send_iov;
send_msg.iov_count = 1;
fi_sendmsg(ep, &send_msg, 0);

// 10. Poll for completion
struct fi_cq_msg_entry comp;
int ret;
while ((ret = fi_cq_read(cq, &comp, 1)) == -FI_EAGAIN);
if (ret < 0) {
    // Error
}
// Success!
```

COMPARISON: VERBS VS LIBFABRIC
-------------------------------

Aspect           | Verbs API       | Libfabric
-----------------|-----------------|-------------------
Abstraction      | Low-level       | Higher-level
Portability      | InfiniBand/RoCE | Multiple fabrics
Performance      | Maximum         | Slightly lower
Complexity       | High            | Medium
Learning Curve   | Steep           | Moderate
Vendor Support   | Mellanox        | Intel, Mellanox, others

Recommendation:
- Maximum performance, InfiniBand-only: Verbs
- Portability, easier development: Libfabric

USE CASES IN HFT
================================================================================

USE CASE 1: COMPONENT COMMUNICATION
------------------------------------

Architecture: Multi-component trading system

Components:
1. Market Data Handler (MDH)
2. Strategy Engine (SE)
3. Order Entry Gateway (OEG)
4. Risk Manager (RM)

Flow:
MDH -> SE -> OEG (critical path)
RM monitors all (parallel)

Traditional approach (shared memory or TCP):
- MDH -> SE: 2-5 microseconds
- SE -> OEG: 2-5 microseconds
- Total: 4-10 microseconds

RDMA approach (InfiniBand):
- MDH -> SE: 0.7 microseconds
- SE -> OEG: 0.7 microseconds
- Total: 1.4 microseconds

Improvement: 3-7x faster

Implementation:
- Each component has RDMA QPs to others
- Use RDMA WRITE for one-way data (market data)
- Use SEND/RECV for request/reply (orders)
- Polling mode for lowest latency

Cost:
- 4 servers with ConnectX-6: 4 x $2K = $8K
- 1 InfiniBand switch: $20K
- Development: $150K (3 months)
- Total: $178K

Benefit:
- 3-7 microsecond latency reduction
- Value: Depends on strategy
- If $10 per microsecond: $30-70 per trade
- 1000 trades/day: $30K-70K/day = $7.5M-17.5M/year
- ROI: 4,000-9,000%

USE CASE 2: DISTRIBUTED ORDER BOOK
-----------------------------------

Problem: Multiple servers need access to shared order book

Traditional approach:
- Replicate order book to each server (consistency issues)
- Or centralized database (high latency)

RDMA approach:
- Order book in memory on dedicated server
- Other servers use RDMA READ to query book
- RDMA WRITE to update book
- Sub-microsecond access

Latency:
- RDMA READ: 1-2 microseconds
- Local memory: 0.1 microseconds
- Trade-off: 10-20x slower than local, but consistent

Benefits:
- Single source of truth
- No replication lag
- Low latency
- Scalable (many readers)

Implementation:
- Order book server with large memory
- RDMA registered memory for book
- Lock-free data structures
- RDMA atomic operations for synchronization

USE CASE 3: HOT-HOT FAILOVER
-----------------------------

Problem: Zero-downtime failover

Traditional approach:
- Primary and backup servers
- Async replication (lag)
- Failover time: seconds to minutes

RDMA approach:
- Sync replication via RDMA
- Sub-microsecond replication latency
- Immediate failover (< 1 second)

Implementation:
- Every state change RDMA WRITE to backup
- Backup maintains identical state
- Heartbeat via RDMA
- Automatic failover

Replication latency: 1-2 microseconds (negligible)

USE CASE 4: LOW-LATENCY DATABASE ACCESS
----------------------------------------

Problem: Historical data queries (e.g., recent prices)

Traditional approach:
- Database (PostgreSQL, MySQL)
- Query latency: 1-10 milliseconds
- Network overhead

RDMA approach:
- In-memory database (Redis, Memcached with RDMA)
- Or custom RDMA-based key-value store
- Query latency: 5-20 microseconds

Example: VoltDB with RDMA
- Latency: 10-50 microseconds
- vs 1-5 milliseconds without RDMA
- 20-100x faster

CONFIGURATION AND TUNING
================================================================================

SYSTEM CONFIGURATION
--------------------

1. KERNEL PARAMETERS

```bash
# Enable RDMA
modprobe rdma_cm
modprobe ib_core
modprobe mlx5_core

# Increase locked memory limit (for memory registration)
ulimit -l unlimited

# In /etc/security/limits.conf:
* soft memlock unlimited
* hard memlock unlimited
```

2. HUGE PAGES

```bash
# 1GB huge pages for RDMA buffers
echo 8 > /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
```

3. CPU ISOLATION

```bash
# Isolate cores for RDMA polling threads
# In /etc/default/grub:
GRUB_CMDLINE_LINUX="isolcpus=2-7"
```

4. NUMA

```bash
# Check NUMA topology
numactl --hardware

# Bind RDMA threads to same NUMA node as NIC
numactl --cpunodebind=0 --membind=0 ./rdma_app
```

RDMA TUNING
-----------

1. QUEUE PAIR PARAMETERS

```c
// Larger QP for higher throughput
qp_init_attr.cap.max_send_wr = 1024;
qp_init_attr.cap.max_recv_wr = 1024;

// Fewer for lower latency (less queue processing)
qp_init_attr.cap.max_send_wr = 16;
qp_init_attr.cap.max_recv_wr = 16;
```

2. COMPLETION QUEUE PARAMETERS

```c
// Polling mode (lowest latency)
struct ibv_cq *cq = ibv_create_cq(context, cq_size, NULL, NULL, 0);

// Poll continuously
while (1) {
    ne = ibv_poll_cq(cq, 1, &wc);
    if (ne > 0) {
        // Process completion
    }
}
```

3. SEND FLAGS

```c
// Inline data (for small messages < 256 bytes)
send_wr.send_flags = IBV_SEND_INLINE | IBV_SEND_SIGNALED;

// Reduces latency by ~0.1-0.2 microseconds
```

4. MEMORY REGISTRATION

```c
// Register memory once at startup (not per operation)
// Use memory pools for efficiency

// For frequent small allocations, use huge buffers
char *huge_buffer = mmap(NULL, 1GB, ...);
ibv_reg_mr(pd, huge_buffer, 1GB, access_flags);
// Allocate from huge_buffer as needed
```

ROCE V2 SPECIFIC CONFIGURATION
-------------------------------

For RoCE, enable Data Center Bridging (DCB) / Priority Flow Control (PFC):

```bash
# Enable PFC on Mellanox NIC
mlnx_qos -i mlx5_0 --trust dscp
mlnx_qos -i mlx5_0 --pfc 0,0,0,1,0,0,0,0

# On switch (example for Mellanox SN2700)
# Configure PFC via switch CLI
```

Without PFC, RoCE is not lossless and will experience packet drops under load.

PERFORMANCE BENCHMARKING
========================

LATENCY MEASUREMENT
-------------------

Use perftest suite (included with OFED drivers):

```bash
# Server side
ib_write_lat

# Client side
ib_write_lat server_hostname

Results:
#bytes #iterations    t_min[usec]    t_max[usec]  t_typical[usec]
    2       1000          0.52           1.23          0.58
    4       1000          0.53           1.25          0.59
    8       1000          0.54           1.27          0.60
```

Interpretation:
- t_typical: Average latency (0.58 us for 2-byte RDMA WRITE)
- t_min: Best case (0.52 us)
- t_max: Worst case (1.23 us)

BANDWIDTH MEASUREMENT
---------------------

```bash
# Server side
ib_send_bw

# Client side
ib_send_bw server_hostname

Results:
#bytes     #iterations    BW peak[Gb/sec]    BW average[Gb/sec]
8388608         1000           96.42              96.38
```

Interpretation:
- Peak: 96.42 Gbps (close to 100 Gbps line rate)
- Average: 96.38 Gbps (consistent)

COST-BENEFIT ANALYSIS
================================================================================

INVESTMENT BREAKDOWN
--------------------

Small Setup (Single rack, 10 servers):
- NICs: 10x ConnectX-6 @ $2K = $20K
- Switch: 1x SN2700 = $20K
- Cables: $2K
- Development: $150K (RDMA integration)
- Total: $192K

Medium Setup (Multiple racks, 30 servers):
- NICs: 30x ConnectX-6 @ $2K = $60K
- Switches: 2x SN4600 @ $60K = $120K
- Cables: $10K
- Development: $200K
- Total: $390K

Large Setup (50+ servers):
- NICs: 50x ConnectX-7 @ $3K = $150K
- Switches: Spine-leaf = $300K
- Cables: $20K
- Development: $300K
- Total: $770K

ROI CALCULATION
---------------

Example: Medium setup ($390K investment)

Benefit: 5 microsecond latency reduction (component communication)

Value: $5 per microsecond (conservative)
Trades: 3000 per day

Daily benefit: 3000 * 5 * $5 = $75K
Annual benefit: $75K * 250 = $18.75M

ROI: ($18.75M - $50K maintenance) / $390K = 4,700%
Payback: 7 days

Even with conservative assumptions, ROI is excellent.

COMPARISON TO ALTERNATIVES
---------------------------

Technology       | Latency       | Cost      | Complexity | Best For
-----------------|---------------|-----------|------------|------------------
Shared Memory    | 0.1-1 us      | $0        | Low        | Same server only
TCP/IP           | 10-50 us      | $0        | Low        | Not HFT
Kernel Bypass    | 2-5 us        | $50K      | Medium     | Single server
RDMA/InfiniBand  | 0.5-2 us      | $200K-1M  | High       | Multi-server, ultra-low latency
FPGA             | 0.3-1 us      | $500K-2M  | Very high  | Special cases

Decision Guide:
- Single server: Shared memory
- Multi-server, < 5 us target: RDMA
- Multi-server, 5-20 us acceptable: Kernel bypass (DPDK + TCP)
- Ultra-low (<< 1 us): FPGA

CONCLUSION
================================================================================

RDMA and InfiniBand provide unmatched low-latency inter-server communication
for HFT systems. Key takeaways:

1. LATENCY: 0.5-2 microseconds (10-50x faster than TCP)
2. COST: $200K-1M for typical HFT deployment
3. COMPLEXITY: High (requires RDMA expertise)
4. ROI: Excellent (payback typically < 1 month for latency-sensitive strategies)

WHEN TO USE RDMA:
- Multi-component architecture requiring low-latency IPC
- Distributed order books or shared state
- Hot-hot failover with sync replication
- Latency budget < 5 microseconds for component communication

WHEN NOT TO USE:
- Single-server application (use shared memory)
- Latency requirements > 20 microseconds (TCP sufficient)
- Budget constraints
- Lack of RDMA expertise

RECOMMENDATIONS:
1. Start with InfiniBand (lowest latency, proven)
2. Consider RoCE v2 if cost-sensitive or want Ethernet compatibility
3. Use ConnectX-6 (HDR 200G) for best price/performance
4. Plan for 3-6 months development time
5. Invest in training and expertise

RDMA is a powerful tool for serious HFT operations requiring multi-server
architectures with ultra-low latency. The complexity is justified by the
significant performance gains.

Document version: 1.0
Last updated: 2025-01-15
