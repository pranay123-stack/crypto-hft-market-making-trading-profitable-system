================================================================================
DEPLOYMENT SCRIPTS FOR HFT SYSTEMS
================================================================================
Comprehensive Deployment Automation for High-Frequency Trading Infrastructure
Version: 1.0
Last Updated: 2025-11-25
================================================================================

TABLE OF CONTENTS
-----------------
1. Deployment Strategy Overview
2. Zero-Downtime Deployment
3. Blue-Green Deployment
4. Canary Deployment
5. Rolling Deployment
6. Configuration Management
7. Database Migration Scripts
8. Health Check Integration
9. Rollback Procedures
10. Container Deployment
11. Multi-Environment Management
12. Complete Deployment Orchestration

================================================================================
1. DEPLOYMENT STRATEGY OVERVIEW
================================================================================

1.1 DEPLOYMENT PRINCIPLES FOR HFT
---------------------------------

Key requirements for HFT deployments:
- Zero downtime during trading hours
- Sub-millisecond latency preservation
- Atomic configuration updates
- Instant rollback capability
- Comprehensive validation
- Audit trail for all changes
- Minimal network disruption
- State preservation across deployments

Deployment phases:
1. Pre-deployment validation
2. Configuration backup
3. Service deployment
4. Health verification
5. Traffic migration
6. Post-deployment monitoring
7. Cleanup and archival

================================================================================
2. ZERO-DOWNTIME DEPLOYMENT FRAMEWORK
================================================================================

2.1 MASTER DEPLOYMENT ORCHESTRATOR
----------------------------------

#!/bin/bash
################################################################################
# Script: zero_downtime_deploy.sh
# Description: Zero-downtime deployment orchestrator for HFT trading engine
# Usage: ./zero_downtime_deploy.sh [version] [environment]
################################################################################

set -euo pipefail

# Configuration
readonly SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
readonly DEPLOY_ROOT="/opt/hft"
readonly CONFIG_ROOT="/etc/hft"
readonly LOG_DIR="/var/log/hft/deployments"
readonly BACKUP_DIR="/backup/hft/deployments"

# Deployment settings
readonly DEPLOY_USER="hftdeploy"
readonly SERVICE_NAME="trading_engine"
readonly HEALTH_CHECK_TIMEOUT=60
readonly WARMUP_DURATION=30

# Version and environment
VERSION="${1:?Version required}"
ENVIRONMENT="${2:-production}"
readonly TIMESTAMP="$(date +%Y%m%d_%H%M%S)"
readonly DEPLOY_ID="${VERSION}_${TIMESTAMP}"
readonly DEPLOY_LOG="${LOG_DIR}/deploy_${DEPLOY_ID}.log"

# Deployment state
declare -A DEPLOYMENT_STATE
DEPLOYMENT_STATE[phase]="initialized"
DEPLOYMENT_STATE[old_version]=""
DEPLOYMENT_STATE[new_version]="$VERSION"
DEPLOYMENT_STATE[rollback_available]="false"

# Color output
readonly RED='\033[0;31m'
readonly GREEN='\033[0;32m'
readonly YELLOW='\033[1;33m'
readonly BLUE='\033[0;34m'
readonly NC='\033[0m'

# Logging functions
log() {
    local level=$1
    shift
    local message="$@"
    local timestamp=$(date +'%Y-%m-%d %H:%M:%S')
    echo -e "[${timestamp}] [${level}] ${message}" | tee -a "$DEPLOY_LOG"
}

log_info() { log "INFO" "$@"; }
log_warn() { log "WARN" "${YELLOW}$@${NC}"; }
log_error() { log "ERROR" "${RED}$@${NC}"; }
log_success() { log "SUCCESS" "${GREEN}$@${NC}"; }

# Initialize deployment
initialize_deployment() {
    log_info "Initializing deployment ${DEPLOY_ID}"

    # Create directories
    mkdir -p "$LOG_DIR" "$BACKUP_DIR"

    # Validate prerequisites
    validate_prerequisites || {
        log_error "Prerequisites validation failed"
        exit 1
    }

    # Check environment
    validate_environment || {
        log_error "Environment validation failed"
        exit 1
    }

    # Market hours safety check
    if [ "$ENVIRONMENT" = "production" ]; then
        check_market_hours || {
            log_error "Deployment during market hours requires approval"
            exit 1
        }
    fi

    DEPLOYMENT_STATE[phase]="initialized"
    log_success "Deployment initialized successfully"
}

# Validate prerequisites
validate_prerequisites() {
    log_info "Validating prerequisites"

    # Check user
    if [ "$(whoami)" != "$DEPLOY_USER" ] && [ "$(whoami)" != "root" ]; then
        log_error "Must run as $DEPLOY_USER or root"
        return 1
    fi

    # Check required commands
    local required_commands=("rsync" "curl" "jq" "md5sum" "tar")
    for cmd in "${required_commands[@]}"; do
        if ! command -v "$cmd" &> /dev/null; then
            log_error "Required command not found: $cmd"
            return 1
        fi
    done

    # Check artifact availability
    if ! check_artifact_availability "$VERSION"; then
        log_error "Artifact not available for version $VERSION"
        return 1
    fi

    # Check disk space
    local required_space_gb=10
    local available_space_gb=$(df -BG "$DEPLOY_ROOT" | tail -1 | awk '{print int($4)}')
    if [ $available_space_gb -lt $required_space_gb ]; then
        log_error "Insufficient disk space: ${available_space_gb}GB available, ${required_space_gb}GB required"
        return 1
    fi

    log_success "All prerequisites validated"
    return 0
}

# Check artifact availability
check_artifact_availability() {
    local version=$1
    local artifact_url="https://artifacts.hft.com/releases/${SERVICE_NAME}/${version}/package.tar.gz"

    log_info "Checking artifact availability: $artifact_url"

    if curl -f -s -I "$artifact_url" > /dev/null; then
        log_success "Artifact available"
        return 0
    else
        log_error "Artifact not found"
        return 1
    fi
}

# Validate environment
validate_environment() {
    log_info "Validating environment: $ENVIRONMENT"

    # Check configuration exists
    local config_file="${CONFIG_ROOT}/${ENVIRONMENT}/config.json"
    if [ ! -f "$config_file" ]; then
        log_error "Configuration not found: $config_file"
        return 1
    fi

    # Validate configuration syntax
    if ! jq empty "$config_file" 2>/dev/null; then
        log_error "Invalid JSON configuration: $config_file"
        return 1
    fi

    # Check service status
    local current_pid=$(pgrep -f "$SERVICE_NAME" || true)
    if [ -n "$current_pid" ]; then
        DEPLOYMENT_STATE[old_version]=$(get_running_version "$current_pid")
        log_info "Current running version: ${DEPLOYMENT_STATE[old_version]}"
    else
        log_info "No existing service running"
    fi

    log_success "Environment validated"
    return 0
}

# Get running version
get_running_version() {
    local pid=$1
    local version_file="/proc/$pid/environ"

    if [ -f "$version_file" ]; then
        cat "$version_file" | tr '\0' '\n' | grep "^VERSION=" | cut -d'=' -f2
    else
        echo "unknown"
    fi
}

# Check market hours
check_market_hours() {
    local hour=$(date +%H)
    local day=$(date +%u)

    # Weekend check (Saturday=6, Sunday=7)
    if [ $day -eq 6 ] || [ $day -eq 7 ]; then
        log_info "Deployment on weekend - proceeding"
        return 0
    fi

    # Market hours check (9:00 - 16:00)
    if [ $hour -ge 9 ] && [ $hour -lt 16 ]; then
        log_warn "Deployment during market hours (${hour}:00)"

        read -p "Proceed with deployment during market hours? (yes/no): " -t 30 confirm || {
            log_error "Confirmation timeout"
            return 1
        }

        if [ "$confirm" != "yes" ]; then
            log_error "Deployment cancelled by user"
            return 1
        fi
    fi

    log_info "Market hours check passed"
    return 0
}

# Backup current deployment
backup_current_deployment() {
    log_info "Backing up current deployment"
    DEPLOYMENT_STATE[phase]="backup"

    local backup_path="${BACKUP_DIR}/${DEPLOY_ID}_pre_deploy"
    mkdir -p "$backup_path"

    # Backup binaries
    if [ -d "${DEPLOY_ROOT}/current" ]; then
        log_info "Backing up binaries"
        rsync -a "${DEPLOY_ROOT}/current/" "${backup_path}/binaries/"
    fi

    # Backup configuration
    log_info "Backing up configuration"
    rsync -a "${CONFIG_ROOT}/${ENVIRONMENT}/" "${backup_path}/config/"

    # Backup state
    if [ -d "/var/lib/hft/${SERVICE_NAME}" ]; then
        log_info "Backing up service state"
        rsync -a "/var/lib/hft/${SERVICE_NAME}/" "${backup_path}/state/"
    fi

    # Create backup manifest
    cat > "${backup_path}/manifest.json" <<EOF
{
    "deploy_id": "${DEPLOY_ID}",
    "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)",
    "environment": "${ENVIRONMENT}",
    "old_version": "${DEPLOYMENT_STATE[old_version]}",
    "new_version": "${DEPLOYMENT_STATE[new_version]}"
}
EOF

    # Compress backup
    log_info "Compressing backup"
    tar -czf "${backup_path}.tar.gz" -C "${BACKUP_DIR}" "$(basename $backup_path)"
    rm -rf "$backup_path"

    DEPLOYMENT_STATE[rollback_available]="true"
    log_success "Backup completed: ${backup_path}.tar.gz"
}

# Download and prepare new version
prepare_new_version() {
    log_info "Preparing new version: $VERSION"
    DEPLOYMENT_STATE[phase]="prepare"

    local staging_dir="${DEPLOY_ROOT}/staging/${DEPLOY_ID}"
    mkdir -p "$staging_dir"

    # Download artifact
    local artifact_url="https://artifacts.hft.com/releases/${SERVICE_NAME}/${VERSION}/package.tar.gz"
    log_info "Downloading artifact: $artifact_url"

    if ! curl -f -L -o "${staging_dir}/package.tar.gz" "$artifact_url"; then
        log_error "Failed to download artifact"
        return 1
    fi

    # Verify checksum
    local checksum_url="${artifact_url}.sha256"
    local expected_checksum=$(curl -f -s "$checksum_url")
    local actual_checksum=$(sha256sum "${staging_dir}/package.tar.gz" | awk '{print $1}')

    if [ "$expected_checksum" != "$actual_checksum" ]; then
        log_error "Checksum verification failed"
        log_error "Expected: $expected_checksum"
        log_error "Actual: $actual_checksum"
        return 1
    fi

    log_success "Checksum verified"

    # Extract artifact
    log_info "Extracting artifact"
    tar -xzf "${staging_dir}/package.tar.gz" -C "$staging_dir"

    # Validate extracted content
    if [ ! -f "${staging_dir}/bin/${SERVICE_NAME}" ]; then
        log_error "Binary not found in artifact"
        return 1
    fi

    # Set permissions
    chmod +x "${staging_dir}/bin/${SERVICE_NAME}"

    log_success "New version prepared in staging"
}

# Deploy new version
deploy_new_version() {
    log_info "Deploying new version"
    DEPLOYMENT_STATE[phase]="deploy"

    local staging_dir="${DEPLOY_ROOT}/staging/${DEPLOY_ID}"
    local release_dir="${DEPLOY_ROOT}/releases/${VERSION}"

    # Create release directory
    mkdir -p "$release_dir"

    # Copy from staging to releases
    log_info "Installing to releases directory"
    rsync -a "${staging_dir}/" "${release_dir}/"

    # Update symlink atomically
    log_info "Updating current symlink"
    ln -sfn "$release_dir" "${DEPLOY_ROOT}/current.new"
    mv -Tf "${DEPLOY_ROOT}/current.new" "${DEPLOY_ROOT}/current"

    log_success "New version deployed"
}

# Start new service instance
start_new_instance() {
    log_info "Starting new service instance"
    DEPLOYMENT_STATE[phase]="start"

    # Set environment variables
    export VERSION="$VERSION"
    export ENVIRONMENT="$ENVIRONMENT"
    export CONFIG_PATH="${CONFIG_ROOT}/${ENVIRONMENT}/config.json"

    # Start service
    local service_bin="${DEPLOY_ROOT}/current/bin/${SERVICE_NAME}"
    local pid_file="/var/run/hft/${SERVICE_NAME}.pid"

    log_info "Launching service: $service_bin"

    if [ -f "$pid_file" ]; then
        local old_pid=$(cat "$pid_file")
        log_info "Old instance PID: $old_pid"
    fi

    # Start new instance on different port for testing
    local test_port=9080
    $service_bin --port $test_port --config "$CONFIG_PATH" &
    local new_pid=$!

    log_info "New instance started with PID: $new_pid"

    # Wait for warmup
    log_info "Warming up for ${WARMUP_DURATION}s"
    sleep $WARMUP_DURATION

    # Health check
    if ! health_check_instance "localhost" $test_port; then
        log_error "Health check failed for new instance"
        kill $new_pid || true
        return 1
    fi

    log_success "New instance healthy"
    echo $new_pid > "${pid_file}.new"
}

# Health check
health_check_instance() {
    local host=$1
    local port=$2
    local timeout=${3:-$HEALTH_CHECK_TIMEOUT}
    local elapsed=0

    log_info "Running health check: ${host}:${port}"

    while [ $elapsed -lt $timeout ]; do
        if curl -f -s "http://${host}:${port}/health" | jq -e '.status == "healthy"' > /dev/null 2>&1; then
            log_success "Health check passed"
            return 0
        fi

        sleep 1
        ((elapsed++))
    done

    log_error "Health check timeout after ${timeout}s"
    return 1
}

# Switch traffic to new instance
switch_traffic() {
    log_info "Switching traffic to new instance"
    DEPLOYMENT_STATE[phase]="switch"

    local old_pid_file="/var/run/hft/${SERVICE_NAME}.pid"
    local new_pid_file="${old_pid_file}.new"

    # Get PIDs
    local old_pid=$(cat "$old_pid_file" 2>/dev/null || echo "")
    local new_pid=$(cat "$new_pid_file" 2>/dev/null || echo "")

    if [ -z "$new_pid" ]; then
        log_error "New instance PID not found"
        return 1
    fi

    # Update load balancer / traffic routing
    # This is environment-specific
    update_traffic_routing "$new_pid"

    # Wait for traffic migration
    log_info "Waiting for traffic migration"
    sleep 10

    # Stop old instance gracefully
    if [ -n "$old_pid" ] && kill -0 "$old_pid" 2>/dev/null; then
        log_info "Stopping old instance: $old_pid"

        # Send SIGTERM
        kill -TERM "$old_pid"

        # Wait for graceful shutdown
        local timeout=30
        local elapsed=0
        while kill -0 "$old_pid" 2>/dev/null && [ $elapsed -lt $timeout ]; do
            sleep 1
            ((elapsed++))
        done

        # Force kill if needed
        if kill -0 "$old_pid" 2>/dev/null; then
            log_warn "Forcing termination of old instance"
            kill -KILL "$old_pid"
        fi

        log_success "Old instance stopped"
    fi

    # Update PID file
    mv "$new_pid_file" "$old_pid_file"

    log_success "Traffic switched successfully"
}

# Update traffic routing (placeholder)
update_traffic_routing() {
    local new_pid=$1
    log_info "Updating traffic routing to instance $new_pid"

    # Implementation depends on load balancer / service mesh
    # Examples:
    # - Update HAProxy configuration
    # - Update NGINX upstream
    # - Update service mesh routing rules
    # - Update DNS records

    # For this example, we'll just log
    log_success "Traffic routing updated"
}

# Post-deployment verification
post_deployment_verification() {
    log_info "Running post-deployment verification"
    DEPLOYMENT_STATE[phase]="verify"

    # Check service is running
    local pid_file="/var/run/hft/${SERVICE_NAME}.pid"
    local pid=$(cat "$pid_file" 2>/dev/null || echo "")

    if [ -z "$pid" ] || ! kill -0 "$pid" 2>/dev/null; then
        log_error "Service not running after deployment"
        return 1
    fi

    # Health check
    if ! health_check_instance "localhost" 8080; then
        log_error "Post-deployment health check failed"
        return 1
    fi

    # Functional tests
    run_smoke_tests || {
        log_error "Smoke tests failed"
        return 1
    }

    # Performance check
    check_performance_metrics || {
        log_warn "Performance metrics degraded"
    }

    log_success "Post-deployment verification passed"
}

# Run smoke tests
run_smoke_tests() {
    log_info "Running smoke tests"

    # Test connectivity
    if ! curl -f -s http://localhost:8080/ping > /dev/null; then
        log_error "Ping test failed"
        return 1
    fi

    # Test API endpoints
    local test_endpoints=("/api/status" "/api/version" "/api/metrics")
    for endpoint in "${test_endpoints[@]}"; do
        if ! curl -f -s "http://localhost:8080${endpoint}" > /dev/null; then
            log_error "Endpoint test failed: $endpoint"
            return 1
        fi
    done

    log_success "Smoke tests passed"
    return 0
}

# Check performance metrics
check_performance_metrics() {
    log_info "Checking performance metrics"

    # Get current latency
    local latency=$(curl -s http://localhost:8080/api/metrics | jq -r '.latency_us')

    if [ -n "$latency" ]; then
        log_info "Current latency: ${latency}us"

        # Compare with baseline
        local baseline_latency=100
        if [ "$latency" -gt $((baseline_latency * 2)) ]; then
            log_warn "Latency significantly higher than baseline"
            return 1
        fi
    fi

    log_success "Performance metrics acceptable"
    return 0
}

# Cleanup staging and old releases
cleanup_deployment() {
    log_info "Cleaning up deployment"
    DEPLOYMENT_STATE[phase]="cleanup"

    # Remove staging directory
    local staging_dir="${DEPLOY_ROOT}/staging/${DEPLOY_ID}"
    if [ -d "$staging_dir" ]; then
        rm -rf "$staging_dir"
        log_info "Removed staging directory"
    fi

    # Keep only last 5 releases
    local releases_dir="${DEPLOY_ROOT}/releases"
    if [ -d "$releases_dir" ]; then
        local release_count=$(ls -1 "$releases_dir" | wc -l)
        if [ $release_count -gt 5 ]; then
            log_info "Cleaning old releases (keeping last 5)"
            ls -1t "$releases_dir" | tail -n +6 | xargs -I {} rm -rf "${releases_dir}/{}"
        fi
    fi

    # Compress old logs
    find "$LOG_DIR" -name "*.log" -mtime +7 -exec gzip {} \;

    log_success "Cleanup completed"
}

# Rollback deployment
rollback_deployment() {
    log_error "Initiating rollback"
    DEPLOYMENT_STATE[phase]="rollback"

    if [ "${DEPLOYMENT_STATE[rollback_available]}" != "true" ]; then
        log_error "No backup available for rollback"
        return 1
    fi

    local backup_file="${BACKUP_DIR}/${DEPLOY_ID}_pre_deploy.tar.gz"

    if [ ! -f "$backup_file" ]; then
        log_error "Backup file not found: $backup_file"
        return 1
    fi

    log_info "Restoring from backup: $backup_file"

    # Extract backup
    local restore_dir="${BACKUP_DIR}/${DEPLOY_ID}_restore"
    mkdir -p "$restore_dir"
    tar -xzf "$backup_file" -C "$restore_dir"

    # Restore binaries
    if [ -d "${restore_dir}/binaries" ]; then
        rsync -a "${restore_dir}/binaries/" "${DEPLOY_ROOT}/current/"
    fi

    # Restore configuration
    if [ -d "${restore_dir}/config" ]; then
        rsync -a "${restore_dir}/config/" "${CONFIG_ROOT}/${ENVIRONMENT}/"
    fi

    # Restart service
    systemctl restart "${SERVICE_NAME}" || {
        log_error "Failed to restart service during rollback"
        return 1
    }

    # Cleanup
    rm -rf "$restore_dir"

    log_success "Rollback completed"
}

# Main deployment workflow
main() {
    log_info "Starting deployment workflow"
    log_info "Version: $VERSION"
    log_info "Environment: $ENVIRONMENT"
    log_info "Deploy ID: $DEPLOY_ID"

    # Set error handler
    trap 'handle_deployment_error $? $LINENO' ERR

    # Execute deployment phases
    initialize_deployment
    backup_current_deployment
    prepare_new_version
    deploy_new_version
    start_new_instance
    switch_traffic
    post_deployment_verification
    cleanup_deployment

    DEPLOYMENT_STATE[phase]="completed"
    log_success "Deployment completed successfully"
    log_info "Deploy ID: $DEPLOY_ID"
}

# Error handler
handle_deployment_error() {
    local exit_code=$1
    local line_number=$2

    log_error "Deployment failed at line $line_number with exit code $exit_code"
    log_error "Current phase: ${DEPLOYMENT_STATE[phase]}"

    # Attempt rollback on critical phases
    if [[ "${DEPLOYMENT_STATE[phase]}" =~ ^(deploy|start|switch)$ ]]; then
        log_warn "Critical phase failure - attempting rollback"
        rollback_deployment || {
            log_error "Rollback failed - manual intervention required"
        }
    fi

    # Send alert
    send_deployment_alert "FAILED" "$exit_code"

    exit $exit_code
}

# Send deployment alert
send_deployment_alert() {
    local status=$1
    local exit_code=${2:-0}

    local message="Deployment ${status}: ${DEPLOY_ID}"
    message+="\nEnvironment: ${ENVIRONMENT}"
    message+="\nVersion: ${VERSION}"
    message+="\nPhase: ${DEPLOYMENT_STATE[phase]}"

    if [ "$status" = "FAILED" ]; then
        message+="\nExit Code: ${exit_code}"
    fi

    # Send to monitoring system
    # curl -X POST https://monitoring.hft.com/alerts \
    #      -H "Content-Type: application/json" \
    #      -d "{\"message\": \"$message\", \"severity\": \"critical\"}"

    log_info "Alert sent: $status"
}

# Execute main workflow
main "$@"

================================================================================
3. BLUE-GREEN DEPLOYMENT
================================================================================

3.1 BLUE-GREEN DEPLOYMENT SCRIPT
--------------------------------

#!/bin/bash
################################################################################
# Script: blue_green_deploy.sh
# Description: Blue-green deployment for HFT trading systems
# Usage: ./blue_green_deploy.sh [version] [target_color]
################################################################################

set -euo pipefail

# Configuration
readonly SERVICE_NAME="trading_engine"
readonly BLUE_PORT=8080
readonly GREEN_PORT=8081
readonly LB_CONFIG="/etc/haproxy/haproxy.cfg"

# Colors for deployment
declare -A DEPLOYMENT_PORTS
DEPLOYMENT_PORTS[blue]=$BLUE_PORT
DEPLOYMENT_PORTS[green]=$GREEN_PORT

VERSION="${1:?Version required}"
TARGET_COLOR="${2:-green}"  # Default to green deployment

# Determine current active color
get_active_color() {
    # Check which port is active in load balancer
    if grep -q "server blue.*:${BLUE_PORT}.*check" "$LB_CONFIG" && \
       ! grep -q "#.*server blue" "$LB_CONFIG"; then
        echo "blue"
    else
        echo "green"
    fi
}

# Get inactive color
get_inactive_color() {
    local active=$(get_active_color)
    if [ "$active" = "blue" ]; then
        echo "green"
    else
        echo "blue"
    fi
}

# Deploy to inactive environment
deploy_to_inactive() {
    local inactive_color=$(get_inactive_color)
    local inactive_port=${DEPLOYMENT_PORTS[$inactive_color]}

    log_info "Deploying version $VERSION to $inactive_color environment (port $inactive_port)"

    # Stop inactive instance
    pkill -f "${SERVICE_NAME}.*--port ${inactive_port}" || true

    # Deploy new version
    local service_bin="/opt/hft/releases/${VERSION}/bin/${SERVICE_NAME}"
    export VERSION="$VERSION"

    $service_bin --port $inactive_port --config /etc/hft/config.json &
    local pid=$!

    log_info "Started $inactive_color instance with PID $pid"

    # Wait for health check
    sleep 10
    if ! health_check_instance "localhost" "$inactive_port"; then
        log_error "Health check failed for $inactive_color instance"
        kill $pid
        return 1
    fi

    log_success "$inactive_color environment deployed and healthy"
}

# Switch load balancer
switch_load_balancer() {
    local new_active_color=$(get_inactive_color)
    local new_active_port=${DEPLOYMENT_PORTS[$new_active_color]}
    local old_active_color=$(get_active_color)

    log_info "Switching load balancer from $old_active_color to $new_active_color"

    # Update HAProxy config
    sed -i "s/^\(\s*server ${old_active_color}\)/#\1/" "$LB_CONFIG"
    sed -i "s/^#\(\s*server ${new_active_color}\)/\1/" "$LB_CONFIG"

    # Reload HAProxy
    systemctl reload haproxy

    log_success "Load balancer switched to $new_active_color"
}

# Main blue-green deployment
main() {
    log_info "Starting blue-green deployment"
    log_info "Version: $VERSION"

    local current_color=$(get_active_color)
    log_info "Current active: $current_color"

    # Deploy to inactive
    deploy_to_inactive

    # Switch traffic
    switch_load_balancer

    # Verify
    sleep 5
    local new_active=$(get_active_color)
    log_success "Deployment complete - active color: $new_active"
}

main "$@"

================================================================================
4. CANARY DEPLOYMENT
================================================================================

4.1 CANARY DEPLOYMENT WITH GRADUAL ROLLOUT
------------------------------------------

#!/bin/bash
################################################################################
# Script: canary_deploy.sh
# Description: Canary deployment with gradual traffic shift
# Usage: ./canary_deploy.sh [version] [canary_percent]
################################################################################

set -euo pipefail

VERSION="${1:?Version required}"
CANARY_PERCENT="${2:-10}"  # Start with 10% traffic

readonly SERVICE_NAME="trading_engine"
readonly STABLE_PORT=8080
readonly CANARY_PORT=8090
readonly ROLLOUT_STEPS=(10 25 50 75 100)
readonly STEP_DURATION=300  # 5 minutes per step

# Deploy canary instance
deploy_canary() {
    log_info "Deploying canary version $VERSION (${CANARY_PERCENT}% traffic)"

    # Start canary instance
    local service_bin="/opt/hft/releases/${VERSION}/bin/${SERVICE_NAME}"
    export VERSION="$VERSION"

    $service_bin --port $CANARY_PORT --config /etc/hft/config.json &
    local pid=$!

    echo $pid > /var/run/hft/${SERVICE_NAME}_canary.pid

    # Health check
    sleep 10
    if ! health_check_instance "localhost" "$CANARY_PORT"; then
        log_error "Canary health check failed"
        kill $pid
        return 1
    fi

    log_success "Canary instance deployed"
}

# Update traffic split
update_traffic_split() {
    local canary_weight=$1
    local stable_weight=$((100 - canary_weight))

    log_info "Updating traffic split: ${stable_weight}% stable, ${canary_weight}% canary"

    # Update load balancer weights
    cat > /etc/haproxy/backend_weights.cfg <<EOF
    server stable localhost:${STABLE_PORT} weight ${stable_weight} check
    server canary localhost:${CANARY_PORT} weight ${canary_weight} check
EOF

    # Reload load balancer
    systemctl reload haproxy

    log_success "Traffic split updated"
}

# Monitor canary metrics
monitor_canary() {
    local duration=$1

    log_info "Monitoring canary for ${duration}s"

    local start_time=$(date +%s)
    local end_time=$((start_time + duration))

    while [ $(date +%s) -lt $end_time ]; do
        # Check error rate
        local canary_errors=$(get_error_rate "$CANARY_PORT")
        local stable_errors=$(get_error_rate "$STABLE_PORT")

        log_info "Error rates - Stable: ${stable_errors}%, Canary: ${canary_errors}%"

        # Abort if canary error rate is significantly higher
        if [ $(echo "$canary_errors > $stable_errors * 1.5" | bc -l) -eq 1 ]; then
            log_error "Canary error rate too high - aborting"
            return 1
        fi

        # Check latency
        local canary_latency=$(get_avg_latency "$CANARY_PORT")
        local stable_latency=$(get_avg_latency "$STABLE_PORT")

        log_info "Latency - Stable: ${stable_latency}us, Canary: ${canary_latency}us"

        # Abort if canary latency is significantly higher
        if [ $(echo "$canary_latency > $stable_latency * 1.2" | bc -l) -eq 1 ]; then
            log_error "Canary latency too high - aborting"
            return 1
        fi

        sleep 30
    done

    log_success "Canary monitoring passed"
    return 0
}

# Get error rate
get_error_rate() {
    local port=$1
    curl -s "http://localhost:${port}/metrics" | jq -r '.error_rate // 0'
}

# Get average latency
get_avg_latency() {
    local port=$1
    curl -s "http://localhost:${port}/metrics" | jq -r '.avg_latency_us // 0'
}

# Gradual rollout
gradual_rollout() {
    log_info "Starting gradual canary rollout"

    for percent in "${ROLLOUT_STEPS[@]}"; do
        log_info "Rollout step: ${percent}%"

        # Update traffic split
        update_traffic_split "$percent"

        # Monitor canary
        if ! monitor_canary "$STEP_DURATION"; then
            log_error "Canary rollout failed at ${percent}%"
            rollback_canary
            return 1
        fi

        log_success "Step ${percent}% completed successfully"
    done

    log_success "Gradual rollout completed"
}

# Rollback canary
rollback_canary() {
    log_warn "Rolling back canary deployment"

    # Route all traffic to stable
    update_traffic_split 0

    # Stop canary instance
    local canary_pid=$(cat /var/run/hft/${SERVICE_NAME}_canary.pid 2>/dev/null || echo "")
    if [ -n "$canary_pid" ]; then
        kill "$canary_pid" || true
    fi

    log_success "Canary rolled back"
}

# Promote canary to stable
promote_canary() {
    log_info "Promoting canary to stable"

    # Stop old stable instance
    local stable_pid=$(cat /var/run/hft/${SERVICE_NAME}.pid 2>/dev/null || echo "")
    if [ -n "$stable_pid" ]; then
        kill "$stable_pid" || true
    fi

    # Move canary to stable port
    # This would typically involve updating configuration and restarting
    # For simplicity, we'll keep canary running and update references

    log_success "Canary promoted to stable"
}

# Main canary deployment
main() {
    log_info "Starting canary deployment"
    log_info "Version: $VERSION"

    # Deploy canary
    deploy_canary

    # Gradual rollout
    if gradual_rollout; then
        # Promote canary
        promote_canary
        log_success "Canary deployment successful"
    else
        log_error "Canary deployment failed"
        exit 1
    fi
}

main "$@"

================================================================================
5. ROLLING DEPLOYMENT
================================================================================

5.1 ROLLING DEPLOYMENT FOR CLUSTERED SERVICES
---------------------------------------------

#!/bin/bash
################################################################################
# Script: rolling_deploy.sh
# Description: Rolling deployment across multiple instances
# Usage: ./rolling_deploy.sh [version]
################################################################################

set -euo pipefail

VERSION="${1:?Version required}"

# Cluster configuration
readonly CLUSTER_NODES=(
    "node1.hft.com"
    "node2.hft.com"
    "node3.hft.com"
    "node4.hft.com"
)

readonly SERVICE_NAME="trading_engine"
readonly SSH_USER="deploy"
readonly DEPLOY_TIMEOUT=600

# Deploy to single node
deploy_to_node() {
    local node=$1

    log_info "Deploying to node: $node"

    # Remove node from load balancer
    remove_from_lb "$node"

    # Wait for connections to drain
    log_info "Waiting for connection draining"
    sleep 30

    # Deploy via SSH
    ssh "${SSH_USER}@${node}" "bash -s" <<EOF
        set -euo pipefail

        # Download package
        cd /opt/hft/staging
        curl -L -o package.tar.gz "https://artifacts.hft.com/releases/${SERVICE_NAME}/${VERSION}/package.tar.gz"

        # Extract
        tar -xzf package.tar.gz -C /opt/hft/releases/${VERSION}

        # Update symlink
        ln -sfn /opt/hft/releases/${VERSION} /opt/hft/current

        # Restart service
        systemctl restart ${SERVICE_NAME}

        # Wait for health check
        sleep 10

        # Verify service
        systemctl is-active ${SERVICE_NAME}
EOF

    local exit_code=$?

    if [ $exit_code -ne 0 ]; then
        log_error "Deployment failed on node $node"
        return 1
    fi

    # Health check
    if ! health_check_remote "$node" 8080; then
        log_error "Health check failed on node $node"
        return 1
    fi

    # Add node back to load balancer
    add_to_lb "$node"

    log_success "Deployment successful on node $node"
}

# Remove node from load balancer
remove_from_lb() {
    local node=$1
    log_info "Removing $node from load balancer"

    # Update HAProxy via API
    curl -X POST "http://lb.hft.com:8080/api/backends/trading/servers/${node}/state" \
         -d '{"state": "drain"}'
}

# Add node to load balancer
add_to_lb() {
    local node=$1
    log_info "Adding $node to load balancer"

    curl -X POST "http://lb.hft.com:8080/api/backends/trading/servers/${node}/state" \
         -d '{"state": "ready"}'
}

# Health check remote node
health_check_remote() {
    local node=$1
    local port=$2

    curl -f -s "http://${node}:${port}/health" | jq -e '.status == "healthy"' > /dev/null
}

# Rolling deployment across cluster
main() {
    log_info "Starting rolling deployment across cluster"
    log_info "Version: $VERSION"
    log_info "Nodes: ${#CLUSTER_NODES[@]}"

    local failed_nodes=()

    for node in "${CLUSTER_NODES[@]}"; do
        log_info "Processing node: $node"

        if deploy_to_node "$node"; then
            log_success "Node $node deployed successfully"
        else
            log_error "Node $node deployment failed"
            failed_nodes+=("$node")

            # Decide whether to continue or abort
            read -p "Continue with remaining nodes? (yes/no): " continue
            if [ "$continue" != "yes" ]; then
                log_error "Rolling deployment aborted"
                exit 1
            fi
        fi

        # Wait between nodes
        sleep 10
    done

    if [ ${#failed_nodes[@]} -eq 0 ]; then
        log_success "Rolling deployment completed successfully"
    else
        log_warn "Rolling deployment completed with failures"
        log_warn "Failed nodes: ${failed_nodes[*]}"
        exit 1
    fi
}

main "$@"

================================================================================
END OF DEPLOYMENT SCRIPTS
================================================================================

These deployment scripts provide production-ready automation for:
- Zero-downtime deployments
- Blue-green deployments
- Canary deployments with gradual rollout
- Rolling deployments across clusters
- Comprehensive health checks and validation
- Automatic rollback on failures
- Traffic management integration

Adapt these scripts to your specific infrastructure and requirements.
