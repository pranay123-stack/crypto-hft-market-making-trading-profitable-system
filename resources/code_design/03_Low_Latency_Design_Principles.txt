================================================================================
LOW-LATENCY DESIGN PRINCIPLES FOR HIGH-FREQUENCY TRADING
================================================================================

This document covers critical low-latency design principles essential for
achieving microsecond and sub-microsecond performance in HFT systems.

================================================================================
TABLE OF CONTENTS
================================================================================
1. Zero-Allocation Design
2. Lock-Free Data Structures
3. Cache-Friendly Design
4. NUMA-Aware Programming
5. Branch Prediction Optimization
6. False Sharing Prevention
7. Memory Prefetching
8. CPU Affinity and Isolation
9. Busy-Wait vs Sleep Trade-offs
10. Hardware Timestamping

================================================================================
1. ZERO-ALLOCATION DESIGN
================================================================================

THEORY:
-------
Dynamic memory allocation (malloc/new) is unpredictable and slow (100-1000ns).
In HFT, we must eliminate all allocations in the critical path by:
- Pre-allocating memory pools
- Using stack/static storage
- Employing object recycling
- Avoiding STL containers that allocate

WHEN TO USE:
-----------
- ALL critical path operations (order entry, market data processing)
- Message encoding/decoding
- Order book updates
- Strategy calculations
- Risk checks

PERFORMANCE IMPACT:
------------------
malloc/new: 100-1000ns (unpredictable, causes page faults)
Object pool: 5-20ns (predictable, cache-friendly)
Stack allocation: 0-1ns (register/cache only)

COMPLETE HFT EXAMPLE - Zero-Allocation Order Pool:
---------------------------------------------------

#pragma once
#include <array>
#include <atomic>
#include <cstdint>
#include <cstring>
#include <new>

namespace hft::zero_alloc {

// Order structure
struct Order {
    uint64_t order_id;
    uint32_t symbol_id;
    double price;
    int64_t quantity;
    int64_t filled_quantity;
    uint64_t timestamp_ns;
    uint8_t side;  // 0=Buy, 1=Sell
    uint8_t order_type;  // 0=Limit, 1=Market, 2=Stop
    uint8_t status;  // 0=New, 1=Partial, 2=Filled, 3=Cancelled

    void reset() noexcept {
        std::memset(this, 0, sizeof(Order));
    }
};

// Lock-free object pool using CAS operations
template<typename T, size_t Capacity>
class LockFreePool {
private:
    struct alignas(64) Node {  // Cache line aligned
        T object;
        std::atomic<uint32_t> next;
        std::atomic<bool> in_use;
    };

    std::array<Node, Capacity> pool_;
    alignas(64) std::atomic<uint32_t> free_head_{0};
    alignas(64) std::atomic<size_t> allocated_count_{0};

public:
    LockFreePool() {
        // Initialize free list
        for (size_t i = 0; i < Capacity - 1; ++i) {
            pool_[i].next.store(i + 1, std::memory_order_relaxed);
            pool_[i].in_use.store(false, std::memory_order_relaxed);
        }
        pool_[Capacity - 1].next.store(UINT32_MAX, std::memory_order_relaxed);
        pool_[Capacity - 1].in_use.store(false, std::memory_order_relaxed);
    }

    // Acquire object from pool (lock-free)
    [[nodiscard]] T* acquire() noexcept {
        while (true) {
            uint32_t head = free_head_.load(std::memory_order_acquire);
            if (head == UINT32_MAX) {
                return nullptr;  // Pool exhausted
            }

            Node& node = pool_[head];
            uint32_t next = node.next.load(std::memory_order_acquire);

            // Try to update head
            if (free_head_.compare_exchange_weak(head, next,
                std::memory_order_release, std::memory_order_acquire)) {

                node.in_use.store(true, std::memory_order_release);
                allocated_count_.fetch_add(1, std::memory_order_relaxed);

                // Use placement new to construct object
                return new (&node.object) T();
            }
            // CAS failed, retry
        }
    }

    // Release object back to pool (lock-free)
    void release(T* obj) noexcept {
        if (!obj) return;

        // Calculate node index from pointer
        Node* node = reinterpret_cast<Node*>(
            reinterpret_cast<char*>(obj) - offsetof(Node, object)
        );
        ptrdiff_t index = node - &pool_[0];

        if (index < 0 || index >= static_cast<ptrdiff_t>(Capacity)) {
            return;  // Invalid pointer
        }

        // Destroy object
        obj->~T();

        // Add back to free list
        while (true) {
            uint32_t head = free_head_.load(std::memory_order_acquire);
            node->next.store(head, std::memory_order_release);

            if (free_head_.compare_exchange_weak(head, static_cast<uint32_t>(index),
                std::memory_order_release, std::memory_order_acquire)) {

                node->in_use.store(false, std::memory_order_release);
                allocated_count_.fetch_sub(1, std::memory_order_relaxed);
                break;
            }
        }
    }

    [[nodiscard]] size_t size() const noexcept {
        return allocated_count_.load(std::memory_order_relaxed);
    }

    [[nodiscard]] size_t capacity() const noexcept {
        return Capacity;
    }

    [[nodiscard]] size_t available() const noexcept {
        return Capacity - size();
    }
};

// RAII wrapper for pool objects
template<typename T, size_t Capacity>
class PoolPtr {
    LockFreePool<T, Capacity>* pool_;
    T* ptr_;

public:
    explicit PoolPtr(LockFreePool<T, Capacity>* pool) noexcept
        : pool_(pool), ptr_(pool ? pool->acquire() : nullptr) {}

    ~PoolPtr() {
        if (pool_ && ptr_) {
            pool_->release(ptr_);
        }
    }

    PoolPtr(const PoolPtr&) = delete;
    PoolPtr& operator=(const PoolPtr&) = delete;

    PoolPtr(PoolPtr&& other) noexcept
        : pool_(other.pool_), ptr_(other.ptr_) {
        other.ptr_ = nullptr;
    }

    PoolPtr& operator=(PoolPtr&& other) noexcept {
        if (this != &other) {
            if (pool_ && ptr_) {
                pool_->release(ptr_);
            }
            pool_ = other.pool_;
            ptr_ = other.ptr_;
            other.ptr_ = nullptr;
        }
        return *this;
    }

    T* get() noexcept { return ptr_; }
    const T* get() const noexcept { return ptr_; }
    T* operator->() noexcept { return ptr_; }
    const T* operator->() const noexcept { return ptr_; }
    T& operator*() noexcept { return *ptr_; }
    const T& operator*() const noexcept { return *ptr_; }

    explicit operator bool() const noexcept { return ptr_ != nullptr; }
};

// Global order pool (initialized at startup)
inline LockFreePool<Order, 100000> g_order_pool;

// Usage example
inline Order* createOrder(uint64_t order_id, uint32_t symbol_id,
                         double price, int64_t quantity, uint8_t side) noexcept {
    Order* order = g_order_pool.acquire();
    if (order) {
        order->order_id = order_id;
        order->symbol_id = symbol_id;
        order->price = price;
        order->quantity = quantity;
        order->filled_quantity = 0;
        order->timestamp_ns = __builtin_ia32_rdtsc();
        order->side = side;
        order->order_type = 0;  // Limit
        order->status = 0;  // New
    }
    return order;
}

inline void destroyOrder(Order* order) noexcept {
    g_order_pool.release(order);
}

} // namespace hft::zero_alloc

// USAGE:
/*
using namespace hft::zero_alloc;

// Critical path - zero allocation
Order* order = createOrder(12345, 100, 150.25, 1000, 0);
if (order) {
    // Process order...
    destroyOrder(order);
}

// Alternative with RAII
PoolPtr<Order, 100000> order_ptr(&g_order_pool);
if (order_ptr) {
    order_ptr->order_id = 12345;
    // ... automatically returned to pool on scope exit
}
*/

================================================================================
2. LOCK-FREE DATA STRUCTURES
================================================================================

THEORY:
-------
Locks (mutexes) cause context switches and priority inversion. Lock-free
structures use atomic CAS operations to eliminate locks, providing:
- Guaranteed progress (no deadlocks)
- No context switches
- Better scalability
- Predictable latency

WHEN TO USE:
-----------
- Message queues between threads
- Order book updates (single writer)
- Market data distribution
- Shared counters/statistics
- Producer-consumer patterns

COMPLETE HFT EXAMPLE - Lock-Free Queue:
----------------------------------------

namespace hft::lockfree {

// Single-producer, single-consumer lock-free queue
template<typename T, size_t Capacity>
class SPSCQueue {
    static_assert((Capacity & (Capacity - 1)) == 0, "Capacity must be power of 2");

private:
    struct alignas(64) CachePadded {
        std::array<T, Capacity> buffer;
    };

    CachePadded data_;

    // Separate cache lines for head and tail to prevent false sharing
    alignas(64) std::atomic<size_t> head_{0};
    alignas(64) std::atomic<size_t> tail_{0};

public:
    SPSCQueue() = default;

    // Producer side - push (lock-free)
    [[nodiscard]] bool push(const T& item) noexcept {
        size_t current_tail = tail_.load(std::memory_order_relaxed);
        size_t next_tail = (current_tail + 1) & (Capacity - 1);

        if (next_tail == head_.load(std::memory_order_acquire)) {
            return false;  // Queue full
        }

        data_.buffer[current_tail] = item;
        tail_.store(next_tail, std::memory_order_release);
        return true;
    }

    // Move version
    [[nodiscard]] bool push(T&& item) noexcept {
        size_t current_tail = tail_.load(std::memory_order_relaxed);
        size_t next_tail = (current_tail + 1) & (Capacity - 1);

        if (next_tail == head_.load(std::memory_order_acquire)) {
            return false;
        }

        data_.buffer[current_tail] = std::move(item);
        tail_.store(next_tail, std::memory_order_release);
        return true;
    }

    // Consumer side - pop (lock-free)
    [[nodiscard]] bool pop(T& item) noexcept {
        size_t current_head = head_.load(std::memory_order_relaxed);

        if (current_head == tail_.load(std::memory_order_acquire)) {
            return false;  // Queue empty
        }

        item = std::move(data_.buffer[current_head]);
        head_.store((current_head + 1) & (Capacity - 1), std::memory_order_release);
        return true;
    }

    [[nodiscard]] bool isEmpty() const noexcept {
        return head_.load(std::memory_order_acquire) ==
               tail_.load(std::memory_order_acquire);
    }

    [[nodiscard]] size_t size() const noexcept {
        size_t h = head_.load(std::memory_order_acquire);
        size_t t = tail_.load(std::memory_order_acquire);
        return (t - h) & (Capacity - 1);
    }

    [[nodiscard]] static constexpr size_t capacity() noexcept {
        return Capacity - 1;  // One slot reserved for full detection
    }
};

// Multi-producer, single-consumer lock-free queue
template<typename T, size_t Capacity>
class MPSCQueue {
    static_assert((Capacity & (Capacity - 1)) == 0, "Capacity must be power of 2");

private:
    struct Node {
        std::atomic<T*> data{nullptr};
    };

    std::array<Node, Capacity> buffer_;
    alignas(64) std::atomic<size_t> write_index_{0};
    alignas(64) std::atomic<size_t> read_index_{0};

public:
    MPSCQueue() = default;

    // Multiple producers can call this
    [[nodiscard]] bool push(T* item) noexcept {
        size_t write_idx = write_index_.fetch_add(1, std::memory_order_relaxed);
        write_idx &= (Capacity - 1);

        // Wait if slot is occupied
        T* expected = nullptr;
        while (!buffer_[write_idx].data.compare_exchange_weak(
            expected, item, std::memory_order_release, std::memory_order_relaxed)) {
            expected = nullptr;
            __builtin_ia32_pause();  // x86 PAUSE instruction
        }

        return true;
    }

    // Single consumer
    [[nodiscard]] T* pop() noexcept {
        size_t read_idx = read_index_.load(std::memory_order_relaxed);
        read_idx &= (Capacity - 1);

        T* item = buffer_[read_idx].data.exchange(nullptr, std::memory_order_acquire);
        if (item) {
            read_index_.fetch_add(1, std::memory_order_release);
        }

        return item;
    }
};

} // namespace hft::lockfree

================================================================================
3. CACHE-FRIENDLY DESIGN
================================================================================

THEORY:
-------
Modern CPUs have multiple cache levels:
- L1: ~1ns (32-64KB per core)
- L2: ~3-5ns (256KB-1MB per core)
- L3: ~10-20ns (8-32MB shared)
- RAM: ~100ns

Cache misses are the primary source of latency in HFT systems.

PRINCIPLES:
-----------
1. Data locality: Keep related data together
2. Sequential access: Prefer arrays over linked structures
3. Cache line size: Align to 64 bytes
4. False sharing: Separate frequently-written variables
5. Prefetching: Help CPU predict access patterns

COMPLETE HFT EXAMPLE - Cache-Friendly Order Book:
--------------------------------------------------

namespace hft::cache {

// Cache-line aligned price level
struct alignas(64) PriceLevel {
    uint64_t price;           // 8 bytes
    uint64_t total_quantity;  // 8 bytes
    uint32_t num_orders;      // 4 bytes
    uint32_t padding_;        // 4 bytes (align to 24)

    // Remaining 40 bytes available for other data
    uint64_t last_update_ns;  // 8 bytes
    char reserved[32];        // 32 bytes reserved

    void reset() noexcept {
        price = 0;
        total_quantity = 0;
        num_orders = 0;
        last_update_ns = 0;
    }
};

static_assert(sizeof(PriceLevel) == 64, "PriceLevel must be exactly 64 bytes");

// Cache-friendly order book (entire book fits in L2 cache)
template<size_t MaxLevels = 10>
class OrderBook {
    static_assert(MaxLevels * sizeof(PriceLevel) * 2 < 256 * 1024,
                  "Order book must fit in L2 cache");

private:
    // Separate bid and ask sides on different cache lines
    alignas(64) std::array<PriceLevel, MaxLevels> bids_;
    alignas(64) std::array<PriceLevel, MaxLevels> asks_;
    alignas(64) uint32_t bid_count_;
    alignas(64) uint32_t ask_count_;
    alignas(64) uint64_t sequence_number_;

public:
    OrderBook() : bid_count_(0), ask_count_(0), sequence_number_(0) {
        for (auto& level : bids_) level.reset();
        for (auto& level : asks_) level.reset();
    }

    // Hot path: Update bid level (cache-friendly)
    void updateBid(uint64_t price, uint64_t quantity, uint64_t timestamp) noexcept {
        // Binary search for price level (cache-friendly sequential access)
        size_t pos = findBidPosition(price);

        if (pos < bid_count_ && bids_[pos].price == price) {
            // Update existing level (single cache line access)
            bids_[pos].total_quantity = quantity;
            bids_[pos].last_update_ns = timestamp;
            if (quantity == 0) {
                removeBidLevel(pos);
            }
        } else if (quantity > 0) {
            // Insert new level
            insertBidLevel(pos, price, quantity, timestamp);
        }

        ++sequence_number_;
    }

    // Hot path: Get best bid (single cache line read)
    [[nodiscard]] const PriceLevel* getBestBid() const noexcept {
        return bid_count_ > 0 ? &bids_[0] : nullptr;
    }

    [[nodiscard]] const PriceLevel* getBestAsk() const noexcept {
        return ask_count_ > 0 ? &asks_[0] : nullptr;
    }

    // Prefetch next levels for iteration
    void prefetchLevels(size_t start_level = 0) const noexcept {
        if (start_level + 1 < bid_count_) {
            __builtin_prefetch(&bids_[start_level + 1], 0, 3);  // Prefetch for read
        }
        if (start_level + 1 < ask_count_) {
            __builtin_prefetch(&asks_[start_level + 1], 0, 3);
        }
    }

private:
    size_t findBidPosition(uint64_t price) const noexcept {
        // Binary search optimized for cache locality
        size_t left = 0, right = bid_count_;

        while (left < right) {
            size_t mid = (left + right) / 2;
            // Prefetch next iteration's data
            if (right - left > 8) {
                __builtin_prefetch(&bids_[(left + mid) / 2], 0, 0);
                __builtin_prefetch(&bids_[(mid + right) / 2], 0, 0);
            }

            if (bids_[mid].price > price) {
                left = mid + 1;
            } else {
                right = mid;
            }
        }
        return left;
    }

    void insertBidLevel(size_t pos, uint64_t price,
                       uint64_t quantity, uint64_t timestamp) noexcept {
        if (bid_count_ >= MaxLevels) return;

        // Shift elements (sequential memory access, cache-friendly)
        for (size_t i = bid_count_; i > pos; --i) {
            bids_[i] = bids_[i - 1];
        }

        bids_[pos].price = price;
        bids_[pos].total_quantity = quantity;
        bids_[pos].num_orders = 1;
        bids_[pos].last_update_ns = timestamp;
        ++bid_count_;
    }

    void removeBidLevel(size_t pos) noexcept {
        if (pos >= bid_count_) return;

        // Shift elements left
        for (size_t i = pos; i < bid_count_ - 1; ++i) {
            bids_[i] = bids_[i + 1];
        }
        bids_[bid_count_ - 1].reset();
        --bid_count_;
    }
};

} // namespace hft::cache

================================================================================
4. NUMA-AWARE PROGRAMMING
================================================================================

THEORY:
-------
NUMA (Non-Uniform Memory Access) means memory access time depends on memory
location relative to processor. On multi-socket systems:
- Local memory: ~100ns
- Remote memory: ~200-300ns (2-3x slower)

For HFT, bind threads and memory to same NUMA node.

COMPLETE EXAMPLE:

namespace hft::numa {

#include <numa.h>
#include <sched.h>

class NumaAllocator {
    int node_id_;

public:
    explicit NumaAllocator(int node_id) : node_id_(node_id) {
        if (numa_available() < 0) {
            throw std::runtime_error("NUMA not available");
        }
    }

    template<typename T>
    T* allocate(size_t count) {
        void* ptr = numa_alloc_onnode(count * sizeof(T), node_id_);
        if (!ptr) {
            throw std::bad_alloc();
        }
        return static_cast<T*>(ptr);
    }

    template<typename T>
    void deallocate(T* ptr, size_t count) {
        numa_free(ptr, count * sizeof(T));
    }

    static void bindThreadToNode(int node_id, int cpu_id) {
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        CPU_SET(cpu_id, &cpuset);

        if (sched_setaffinity(0, sizeof(cpu_set_t), &cpuset) != 0) {
            throw std::runtime_error("Failed to set CPU affinity");
        }

        numa_run_on_node(node_id);
        numa_set_preferred(node_id);
    }
};

// Trading thread with NUMA affinity
class TradingThread {
    int numa_node_;
    int cpu_core_;
    std::thread thread_;

public:
    TradingThread(int numa_node, int cpu_core)
        : numa_node_(numa_node), cpu_core_(cpu_core) {

        thread_ = std::thread([this]() {
            // Bind to NUMA node and CPU
            NumaAllocator::bindThreadToNode(numa_node_, cpu_core_);

            // Run trading loop
            this->tradingLoop();
        });
    }

    ~TradingThread() {
        if (thread_.joinable()) {
            thread_.join();
        }
    }

private:
    void tradingLoop() {
        // All memory allocated here will be on local NUMA node
        // ... trading logic
    }
};

} // namespace hft::numa

================================================================================
5. BRANCH PREDICTION OPTIMIZATION
================================================================================

THEORY:
-------
Modern CPUs predict branch outcomes. Mispredictions cost 10-20 cycles.
For HFT, we must:
- Make branches predictable
- Use [[likely]]/[[unlikely]] attributes (C++20)
- Eliminate branches via branchless code
- Profile and optimize hot branches

COMPLETE EXAMPLE:

namespace hft::branch {

// Traditional branching version
inline int64_t calculatePnL_branching(int64_t position, double entry_price,
                                      double current_price) noexcept {
    if (position > 0) {
        return position * (current_price - entry_price);
    } else if (position < 0) {
        return position * (current_price - entry_price);
    } else {
        return 0;
    }
}

// Branchless version (faster for unpredictable positions)
inline int64_t calculatePnL_branchless(int64_t position, double entry_price,
                                       double current_price) noexcept {
    return position * (current_price - entry_price);
}

// Using C++20 attributes for hints
inline bool checkRiskLimit(int64_t position, int64_t limit) noexcept {
    if (std::abs(position) <= limit) [[likely]] {
        return true;
    } else [[unlikely]] {
        return false;
    }
}

// Branchless min/max (single instruction on modern CPUs)
inline int64_t branchless_min(int64_t a, int64_t b) noexcept {
    return b + ((a - b) & ((a - b) >> 63));
}

inline int64_t branchless_max(int64_t a, int64_t b) noexcept {
    return a - ((a - b) & ((a - b) >> 63));
}

// Branchless abs
inline int64_t branchless_abs(int64_t x) noexcept {
    int64_t mask = x >> 63;
    return (x + mask) ^ mask;
}

} // namespace hft::branch

PERFORMANCE MEASUREMENTS:
-------------------------
Branching version (unpredictable): ~15-20 cycles
Branchless version: ~3-5 cycles
Speedup: 3-4x for unpredictable branches

COMMON MISTAKES:
----------------
1. Not profiling branch misprediction rates
2. Overusing [[likely]]/[[unlikely]] without measurement
3. Not considering CPU branch predictor capabilities
4. Ignoring branch cost in tight loops
5. Not using branchless alternatives when appropriate

This completes the first 5 sections. Remaining sections follow similar detail.
