================================================================================
COMPLIANCE TESTING PROCEDURES
Systematic Validation and Control Testing Framework
High-Frequency Trading System
================================================================================

OVERVIEW
--------
Comprehensive testing procedures for validating compliance controls, regulatory
requirements, and risk management systems. Includes pre-production testing,
ongoing monitoring, and periodic validation protocols.

================================================================================
SECTION 1: COMPLIANCE TESTING FRAMEWORK
================================================================================

1.1 TESTING HIERARCHY
----------------------

Level 1: Unit Testing
- Individual compliance functions
- Rule logic validation
- Data integrity checks
- Performance benchmarks

Level 2: Integration Testing
- End-to-end workflow validation
- System interaction testing
- Data flow verification
- Cross-component validation

Level 3: Regression Testing
- Verify existing controls after changes
- Backward compatibility
- No degradation in compliance coverage

Level 4: User Acceptance Testing (UAT)
- Compliance team validation
- Real-world scenario testing
- Business process verification

Level 5: Production Monitoring
- Continuous compliance validation
- Real-time alerting
- Performance tracking


1.2 TESTING SCOPE MATRIX
-------------------------

Testing Area                | Frequency     | Owner         | Documentation
----------------------------|---------------|---------------|---------------
Pre-Trade Risk Controls     | Daily         | Risk Team     | Test Results DB
Order Validation            | Daily         | Tech Team     | Test Results DB
Trade Logging               | Daily         | Compliance    | Audit Logs
MiFID II Reporting          | Weekly        | Compliance    | Report Archive
Clock Synchronization       | Continuous    | Operations    | NTP Logs
Market Manipulation Det.    | Daily         | Compliance    | Alert Logs
Best Execution              | Weekly        | Compliance    | Quality Reports
Data Retention              | Monthly       | IT/Compliance | Retention Audit
Disaster Recovery           | Quarterly     | IT            | DR Test Report
Algorithm Testing           | Pre-deployment| Quant Team    | Test Certificates
Regulatory Reporting        | Monthly       | Compliance    | Submission Logs


================================================================================
SECTION 2: PRE-TRADE RISK CONTROL TESTING
================================================================================

2.1 AUTOMATED TEST SUITE
-------------------------

class PreTradeRiskControlTests {
public:
    struct TestCase {
        char test_id[64];
        char test_name[128];
        char description[512];
        std::vector<std::string> test_steps;
        std::string expected_result;
        std::string actual_result;
        bool passed;
        uint64_t execution_time_ns;
    };

    struct TestSuite {
        char suite_id[64];
        char suite_name[128];
        std::vector<TestCase> test_cases;
        uint32_t passed_count;
        uint32_t failed_count;
        uint64_t total_execution_time_ns;
    };

    TestSuite RunPreTradeRiskTests() {
        TestSuite suite{};
        strcpy(suite.suite_id, "PRE_TRADE_RISK_001");
        strcpy(suite.suite_name, "Pre-Trade Risk Control Validation");

        // Test 1: Order Size Limit
        suite.test_cases.push_back(TestOrderSizeLimit());

        // Test 2: Position Limit
        suite.test_cases.push_back(TestPositionLimit());

        // Test 3: Credit Limit
        suite.test_cases.push_back(TestCreditLimit());

        // Test 4: Price Collar
        suite.test_cases.push_back(TestPriceCollar());

        // Test 5: Fat Finger Check
        suite.test_cases.push_back(TestFatFingerCheck());

        // Test 6: Duplicate Order Detection
        suite.test_cases.push_back(TestDuplicateOrderDetection());

        // Test 7: Trading Halt Check
        suite.test_cases.push_back(TestTradingHaltCheck());

        // Test 8: Restricted Security Check
        suite.test_cases.push_back(TestRestrictedSecurityCheck());

        // Count results
        for (const auto& test : suite.test_cases) {
            if (test.passed) {
                suite.passed_count++;
            } else {
                suite.failed_count++;
            }
            suite.total_execution_time_ns += test.execution_time_ns;
        }

        // Generate report
        GenerateTestReport(suite);

        return suite;
    }

private:
    TestCase TestOrderSizeLimit() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_001");
        strcpy(test.test_name, "Order Size Limit Check");
        strcpy(test.description,
               "Verify that orders exceeding maximum size are rejected");

        auto start_time = GetNanosecondTimestamp();

        // Create test order exceeding limit
        Order test_order;
        test_order.symbol = "AAPL";
        test_order.side = 'B';
        test_order.quantity = 1000000;  // 1M shares (exceeds limit)
        test_order.limit_price = 150.0;

        // Expected: Order should be rejected
        test.expected_result = "Order rejected: Order size exceeds limit";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckOrderSize(test_order);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("Order size exceeds") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestPositionLimit() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_002");
        strcpy(test.test_name, "Position Limit Check");
        strcpy(test.description,
               "Verify that orders causing position limit breach are rejected");

        auto start_time = GetNanosecondTimestamp();

        // Setup: Set current position near limit
        SetTestPosition("AAPL", 90000);  // Near 100k limit

        // Create test order that would breach limit
        Order test_order;
        test_order.symbol = "AAPL";
        test_order.side = 'B';
        test_order.quantity = 15000;  // Would breach limit
        test_order.limit_price = 150.0;

        test.expected_result = "Order rejected: Position limit would be exceeded";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckPositionLimit(test_order);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("Position limit") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        // Cleanup
        ResetTestPosition("AAPL");

        return test;
    }

    TestCase TestCreditLimit() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_003");
        strcpy(test.test_name, "Credit Limit Check");
        strcpy(test.description,
               "Verify that orders exceeding credit limit are rejected");

        auto start_time = GetNanosecondTimestamp();

        // Setup: Set credit utilization near limit
        SetTestCreditUtilization("ACCT001", 9500000);  // Near 10M limit

        // Create large test order
        Order test_order;
        test_order.account_id = "ACCT001";
        test_order.symbol = "AAPL";
        test_order.side = 'B';
        test_order.quantity = 10000;
        test_order.limit_price = 150.0;  // Total: $1.5M (would exceed limit)

        test.expected_result = "Order rejected: Credit limit exceeded";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckCreditLimit(test_order);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("Credit limit") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        // Cleanup
        ResetTestCreditUtilization("ACCT001");

        return test;
    }

    TestCase TestPriceCollar() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_004");
        strcpy(test.test_name, "Price Collar Check");
        strcpy(test.description,
               "Verify that orders with prices outside collar are rejected");

        auto start_time = GetNanosecondTimestamp();

        // Setup: Set current market price
        SetTestMarketPrice("AAPL", 150.0, 150.05);  // Bid 150.0, Ask 150.05

        // Create test order with price outside collar (>10% from mid)
        Order test_order;
        test_order.symbol = "AAPL";
        test_order.side = 'B';
        test_order.quantity = 100;
        test_order.limit_price = 170.0;  // 13% above mid (outside 10% collar)

        test.expected_result = "Order rejected: Price outside reasonable range";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckPriceCollar(test_order);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("Price outside") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestFatFingerCheck() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_005");
        strcpy(test.test_name, "Fat Finger Detection");
        strcpy(test.description,
               "Verify detection of erroneous orders (fat finger errors)");

        auto start_time = GetNanosecondTimestamp();

        // Create obviously erroneous order
        Order test_order;
        test_order.symbol = "AAPL";
        test_order.side = 'B';
        test_order.quantity = 10000000;  // 10M shares (clearly erroneous)
        test_order.limit_price = 1500.0; // 10x market price

        test.expected_result = "Order rejected: Potential fat finger error";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckFatFinger(test_order);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("fat finger") !=
                          std::string::npos ||
                          result.rejection_reason.find("erroneous") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestDuplicateOrderDetection() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_006");
        strcpy(test.test_name, "Duplicate Order Detection");
        strcpy(test.description,
               "Verify detection of duplicate orders");

        auto start_time = GetNanosecondTimestamp();

        // Create and submit first order
        Order order1;
        order1.symbol = "AAPL";
        order1.side = 'B';
        order1.quantity = 100;
        order1.limit_price = 150.0;
        order1.client_order_id = "TEST_DUP_001";

        // Submit first order
        SubmitTestOrder(order1);

        // Create identical order immediately after
        Order order2 = order1;
        order2.client_order_id = "TEST_DUP_001";  // Same client order ID

        test.expected_result = "Order rejected: Duplicate order detected";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckDuplicateOrder(order2);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("Duplicate") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        // Cleanup
        CancelTestOrder(order1.client_order_id);

        return test;
    }

    TestCase TestTradingHaltCheck() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_007");
        strcpy(test.test_name, "Trading Halt Check");
        strcpy(test.description,
               "Verify that orders for halted securities are rejected");

        auto start_time = GetNanosecondTimestamp();

        // Setup: Mark symbol as halted
        SetTestTradingHalt("AAPL", true);

        // Create test order for halted symbol
        Order test_order;
        test_order.symbol = "AAPL";
        test_order.side = 'B';
        test_order.quantity = 100;
        test_order.limit_price = 150.0;

        test.expected_result = "Order rejected: Trading halted";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckTradingHalt(test_order);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("Trading halted") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        // Cleanup
        SetTestTradingHalt("AAPL", false);

        return test;
    }

    TestCase TestRestrictedSecurityCheck() {
        TestCase test{};
        strcpy(test.test_id, "PT_RISK_008");
        strcpy(test.test_name, "Restricted Security Check");
        strcpy(test.description,
               "Verify that orders for restricted securities are rejected");

        auto start_time = GetNanosecondTimestamp();

        // Setup: Add symbol to restricted list
        AddToRestrictedList("ACME");

        // Create test order for restricted symbol
        Order test_order;
        test_order.symbol = "ACME";
        test_order.side = 'B';
        test_order.quantity = 100;
        test_order.limit_price = 50.0;

        test.expected_result = "Order rejected: Restricted security";

        // Execute test
        RiskCheckResult result = risk_engine_.CheckRestrictedSecurity(test_order);

        if (!result.passed) {
            test.actual_result = result.rejection_reason;
            test.passed = (result.rejection_reason.find("Restricted") !=
                          std::string::npos);
        } else {
            test.actual_result = "Order accepted (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        // Cleanup
        RemoveFromRestrictedList("ACME");

        return test;
    }

    void GenerateTestReport(const TestSuite& suite) {
        std::ofstream report("test_reports/" + std::string(suite.suite_id) +
                           "_" + GetTimestampString() + ".html");

        report << "<html><head><title>Test Report: " << suite.suite_name
              << "</title></head><body>\n";
        report << "<h1>" << suite.suite_name << "</h1>\n";
        report << "<p>Suite ID: " << suite.suite_id << "</p>\n";
        report << "<p>Passed: " << suite.passed_count << " / "
              << (suite.passed_count + suite.failed_count) << "</p>\n";
        report << "<p>Pass Rate: "
              << (suite.passed_count * 100.0 / (suite.passed_count + suite.failed_count))
              << "%</p>\n";
        report << "<p>Total Execution Time: "
              << (suite.total_execution_time_ns / 1000000.0) << " ms</p>\n";

        report << "<h2>Test Cases</h2>\n";
        report << "<table border='1'>\n";
        report << "<tr><th>Test ID</th><th>Test Name</th><th>Result</th>"
              << "<th>Expected</th><th>Actual</th><th>Time (μs)</th></tr>\n";

        for (const auto& test : suite.test_cases) {
            std::string status = test.passed ? "PASS" : "FAIL";
            std::string color = test.passed ? "green" : "red";

            report << "<tr><td>" << test.test_id << "</td>"
                  << "<td>" << test.test_name << "</td>"
                  << "<td style='color:" << color << "'><b>" << status << "</b></td>"
                  << "<td>" << test.expected_result << "</td>"
                  << "<td>" << test.actual_result << "</td>"
                  << "<td>" << (test.execution_time_ns / 1000) << "</td></tr>\n";
        }

        report << "</table>\n";
        report << "</body></html>\n";
        report.close();

        LOG_INFO("Test report generated: " << suite.suite_id);
    }

    RiskEngine risk_engine_;
};


================================================================================
SECTION 3: COMPLIANCE RULE TESTING
================================================================================

3.1 REGULATORY RULE VALIDATION
-------------------------------

class ComplianceRuleTests {
public:
    TestSuite RunMiFIDIIComplianceTests() {
        TestSuite suite{};
        strcpy(suite.suite_id, "MIFID_COMPLIANCE_001");
        strcpy(suite.suite_name, "MiFID II Compliance Validation");

        // Test clock synchronization
        suite.test_cases.push_back(TestClockSynchronization());

        // Test transaction reporting
        suite.test_cases.push_back(TestTransactionReporting());

        // Test order record keeping
        suite.test_cases.push_back(TestOrderRecordKeeping());

        // Test best execution monitoring
        suite.test_cases.push_back(TestBestExecutionMonitoring());

        // Calculate results
        CalculateTestResults(suite);

        return suite;
    }

private:
    TestCase TestClockSynchronization() {
        TestCase test{};
        strcpy(test.test_id, "MIFID_001");
        strcpy(test.test_name, "Clock Synchronization (RTS 25)");
        strcpy(test.description,
               "Verify clock synchronization within 100 microseconds of UTC");

        auto start_time = GetNanosecondTimestamp();

        // Query multiple NTP servers
        std::vector<int64_t> offsets;
        std::vector<std::string> ntp_servers = {
            "time.google.com",
            "time.cloudflare.com",
            "ntp1.mifid.eu"
        };

        for (const auto& server : ntp_servers) {
            int64_t offset_us = QueryNTPOffset(server);
            offsets.push_back(offset_us);
        }

        // Calculate median offset
        std::sort(offsets.begin(), offsets.end());
        int64_t median_offset = offsets[offsets.size() / 2];

        test.expected_result = "Clock offset < 100 microseconds";

        if (std::abs(median_offset) < 100) {
            test.actual_result = "Clock offset: " + std::to_string(median_offset) +
                                " μs (PASS)";
            test.passed = true;
        } else {
            test.actual_result = "Clock offset: " + std::to_string(median_offset) +
                                " μs (FAIL - exceeds 100μs threshold)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestTransactionReporting() {
        TestCase test{};
        strcpy(test.test_id, "MIFID_002");
        strcpy(test.test_name, "Transaction Reporting (RTS 22)");
        strcpy(test.description,
               "Verify all required fields are captured for transaction reports");

        auto start_time = GetNanosecondTimestamp();

        // Create test execution
        Execution test_exec = CreateTestExecution();

        // Generate transaction report
        MiFIDIITransactionReport report = GenerateTransactionReport(test_exec);

        // Verify all required fields
        std::vector<std::string> missing_fields;

        if (strlen(report.trading_venue) == 0) missing_fields.push_back("trading_venue");
        if (strlen(report.isin) == 0) missing_fields.push_back("isin");
        if (strlen(report.buyer_id) == 0) missing_fields.push_back("buyer_id");
        if (strlen(report.seller_id) == 0) missing_fields.push_back("seller_id");
        if (strlen(report.trading_date_time) == 0) missing_fields.push_back("trading_date_time");
        if (report.quantity == 0) missing_fields.push_back("quantity");
        if (report.price == 0.0) missing_fields.push_back("price");

        test.expected_result = "All required fields populated";

        if (missing_fields.empty()) {
            test.actual_result = "All fields present (PASS)";
            test.passed = true;
        } else {
            test.actual_result = "Missing fields: " +
                                JoinStrings(missing_fields, ", ") + " (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestOrderRecordKeeping() {
        TestCase test{};
        strcpy(test.test_id, "MIFID_003");
        strcpy(test.test_name, "Order Record Keeping (RTS 24)");
        strcpy(test.description,
               "Verify order records contain all required fields and are retained");

        auto start_time = GetNanosecondTimestamp();

        // Create test order
        Order test_order = CreateTestOrder();

        // Record order
        RecordOrder(test_order);

        // Retrieve order record
        OrderRecord record = RetrieveOrderRecord(test_order.id);

        // Verify all required fields
        std::vector<std::string> missing_fields;

        if (record.client_id.empty()) missing_fields.push_back("client_id");
        if (record.instrument_isin.empty()) missing_fields.push_back("instrument_isin");
        if (record.order_id.empty()) missing_fields.push_back("order_id");
        if (record.order_receipt_time.empty()) missing_fields.push_back("order_receipt_time");
        if (record.investment_decision_maker.empty()) missing_fields.push_back("investment_decision_maker");
        if (record.execution_decision_maker.empty()) missing_fields.push_back("execution_decision_maker");

        test.expected_result = "All required order fields captured and stored";

        if (missing_fields.empty()) {
            test.actual_result = "All fields present and stored (PASS)";
            test.passed = true;
        } else {
            test.actual_result = "Missing fields: " +
                                JoinStrings(missing_fields, ", ") + " (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestBestExecutionMonitoring() {
        TestCase test{};
        strcpy(test.test_id, "MIFID_004");
        strcpy(test.test_name, "Best Execution Monitoring");
        strcpy(test.description,
               "Verify best execution monitoring is operational");

        auto start_time = GetNanosecondTimestamp();

        // Create test execution
        Execution test_exec = CreateTestExecution();
        Order test_order = CreateTestOrder();

        // Analyze execution quality
        ExecutionQualityMetrics metrics = AnalyzeExecutionQuality(test_exec, test_order);

        test.expected_result = "Execution quality calculated and monitored";

        // Verify metrics were calculated
        bool has_effective_spread = (metrics.factors.effective_spread != 0.0);
        bool has_quality_score = (metrics.execution_quality_score > 0.0);
        bool has_venue_comparison = !metrics.venue_alternatives.empty();

        if (has_effective_spread && has_quality_score && has_venue_comparison) {
            test.actual_result = "Execution quality monitored, score: " +
                                std::to_string(metrics.execution_quality_score) +
                                " (PASS)";
            test.passed = true;
        } else {
            test.actual_result = "Execution quality monitoring incomplete (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }
};


================================================================================
SECTION 4: PERFORMANCE AND STRESS TESTING
================================================================================

4.1 SYSTEM PERFORMANCE TESTING
-------------------------------

class PerformanceTests {
public:
    struct PerformanceMetrics {
        uint64_t throughput_orders_per_sec;
        uint64_t avg_latency_ns;
        uint64_t p50_latency_ns;
        uint64_t p95_latency_ns;
        uint64_t p99_latency_ns;
        uint64_t max_latency_ns;
        double cpu_utilization_pct;
        size_t memory_usage_mb;
    };

    TestCase TestOrderProcessingThroughput() {
        TestCase test{};
        strcpy(test.test_id, "PERF_001");
        strcpy(test.test_name, "Order Processing Throughput");
        strcpy(test.description,
               "Measure maximum sustainable order processing throughput");

        auto start_time = GetNanosecondTimestamp();

        // Generate test orders
        const uint32_t TEST_ORDER_COUNT = 100000;
        std::vector<Order> test_orders = GenerateTestOrders(TEST_ORDER_COUNT);

        // Process orders and measure time
        auto process_start = GetNanosecondTimestamp();

        for (const auto& order : test_orders) {
            ProcessOrder(order);
        }

        auto process_end = GetNanosecondTimestamp();
        uint64_t duration_ns = process_end - process_start;

        // Calculate throughput
        double duration_sec = duration_ns / 1000000000.0;
        uint64_t throughput = static_cast<uint64_t>(TEST_ORDER_COUNT / duration_sec);

        test.expected_result = "Throughput > 50,000 orders/second";

        if (throughput > 50000) {
            test.actual_result = "Throughput: " + std::to_string(throughput) +
                                " orders/sec (PASS)";
            test.passed = true;
        } else {
            test.actual_result = "Throughput: " + std::to_string(throughput) +
                                " orders/sec (FAIL - below 50k threshold)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestLatencyPercentiles() {
        TestCase test{};
        strcpy(test.test_id, "PERF_002");
        strcpy(test.test_name, "Order Processing Latency");
        strcpy(test.description,
               "Measure order processing latency percentiles");

        auto start_time = GetNanosecondTimestamp();

        // Generate test orders
        const uint32_t TEST_ORDER_COUNT = 10000;
        std::vector<Order> test_orders = GenerateTestOrders(TEST_ORDER_COUNT);

        // Measure latency for each order
        std::vector<uint64_t> latencies;
        latencies.reserve(TEST_ORDER_COUNT);

        for (const auto& order : test_orders) {
            auto order_start = GetNanosecondTimestamp();
            ProcessOrder(order);
            auto order_end = GetNanosecondTimestamp();

            latencies.push_back(order_end - order_start);
        }

        // Calculate percentiles
        std::sort(latencies.begin(), latencies.end());

        uint64_t p50 = latencies[latencies.size() * 50 / 100];
        uint64_t p95 = latencies[latencies.size() * 95 / 100];
        uint64_t p99 = latencies[latencies.size() * 99 / 100];

        test.expected_result = "P50 < 100μs, P95 < 500μs, P99 < 1ms";

        bool p50_ok = (p50 < 100000);    // 100μs
        bool p95_ok = (p95 < 500000);    // 500μs
        bool p99_ok = (p99 < 1000000);   // 1ms

        if (p50_ok && p95_ok && p99_ok) {
            test.actual_result = "P50: " + std::to_string(p50/1000) +
                                "μs, P95: " + std::to_string(p95/1000) +
                                "μs, P99: " + std::to_string(p99/1000) +
                                "μs (PASS)";
            test.passed = true;
        } else {
            test.actual_result = "P50: " + std::to_string(p50/1000) +
                                "μs, P95: " + std::to_string(p95/1000) +
                                "μs, P99: " + std::to_string(p99/1000) +
                                "μs (FAIL - exceeds thresholds)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }

    TestCase TestStressScenario() {
        TestCase test{};
        strcpy(test.test_id, "PERF_003");
        strcpy(test.test_name, "Stress Test - Peak Load");
        strcpy(test.description,
               "Test system behavior under peak load conditions");

        auto start_time = GetNanosecondTimestamp();

        // Simulate peak load (200k orders/sec for 10 seconds)
        const uint32_t ORDERS_PER_SECOND = 200000;
        const uint32_t DURATION_SECONDS = 10;

        bool system_stable = true;
        uint32_t failed_orders = 0;

        for (uint32_t sec = 0; sec < DURATION_SECONDS; sec++) {
            auto sec_start = GetNanosecondTimestamp();

            // Generate orders for this second
            std::vector<Order> orders = GenerateTestOrders(ORDERS_PER_SECOND);

            // Process orders
            for (const auto& order : orders) {
                if (!ProcessOrder(order)) {
                    failed_orders++;
                }
            }

            // Check system health
            if (!IsSystemHealthy()) {
                system_stable = false;
                break;
            }

            // Wait for next second
            auto sec_end = GetNanosecondTimestamp();
            uint64_t elapsed_ns = sec_end - sec_start;
            if (elapsed_ns < 1000000000ULL) {
                std::this_thread::sleep_for(
                    std::chrono::nanoseconds(1000000000ULL - elapsed_ns));
            }
        }

        test.expected_result = "System stable, < 0.1% failed orders";

        double failure_rate = (static_cast<double>(failed_orders) /
                              (ORDERS_PER_SECOND * DURATION_SECONDS)) * 100.0;

        if (system_stable && failure_rate < 0.1) {
            test.actual_result = "System stable, " +
                                std::to_string(failure_rate) +
                                "% failure rate (PASS)";
            test.passed = true;
        } else {
            test.actual_result = "System " +
                                std::string(system_stable ? "stable" : "UNSTABLE") +
                                ", " + std::to_string(failure_rate) +
                                "% failure rate (FAIL)";
            test.passed = false;
        }

        test.execution_time_ns = GetNanosecondTimestamp() - start_time;

        return test;
    }
};


================================================================================
SECTION 5: CONTINUOUS TESTING FRAMEWORK
================================================================================

5.1 AUTOMATED DAILY TESTING
----------------------------

class ContinuousComplianceTesting {
public:
    ContinuousComplianceTesting() {
        testing_thread_ = std::thread(
            &ContinuousComplianceTesting::TestingLoop, this);
    }

    void TestingLoop() {
        while (running_) {
            auto now = std::chrono::system_clock::now();
            auto now_time_t = std::chrono::system_clock::to_time_t(now);
            struct tm tm_info;
            localtime_r(&now_time_t, &tm_info);

            // Run tests at 6:00 AM daily
            if (tm_info.tm_hour == 6 && tm_info.tm_min == 0) {
                RunDailyTests();

                // Sleep for rest of hour
                std::this_thread::sleep_for(std::chrono::hours(1));
            }

            std::this_thread::sleep_for(std::chrono::minutes(1));
        }
    }

    void RunDailyTests() {
        LOG_INFO("Starting daily compliance tests");

        // Run all test suites
        auto pre_trade_results = PreTradeRiskControlTests().RunPreTradeRiskTests();
        auto mifid_results = ComplianceRuleTests().RunMiFIDIIComplianceTests();
        auto perf_results = RunPerformanceTests();

        // Generate consolidated report
        GenerateConsolidatedReport({
            pre_trade_results,
            mifid_results,
            perf_results
        });

        // Alert on failures
        if (pre_trade_results.failed_count > 0 ||
            mifid_results.failed_count > 0 ||
            perf_results.failed_count > 0) {

            AlertComplianceTeam("Daily compliance tests failed");
        }

        LOG_INFO("Daily compliance tests completed");
    }

private:
    std::thread testing_thread_;
    std::atomic<bool> running_{true};
};

================================================================================
END OF DOCUMENT
================================================================================
