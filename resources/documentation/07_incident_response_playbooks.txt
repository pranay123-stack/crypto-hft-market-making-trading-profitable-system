================================================================================
HFT SYSTEM INCIDENT RESPONSE PLAYBOOKS
================================================================================

VERSION: 1.0
LAST UPDATED: 2025-11-25
OWNER: Operations & Engineering Team
CLASSIFICATION: Internal Use Only

================================================================================
TABLE OF CONTENTS
================================================================================

1. INCIDENT RESPONSE FRAMEWORK
2. SEVERITY CLASSIFICATION
3. EXCHANGE OUTAGE PLAYBOOK
4. SYSTEM CRASH PLAYBOOK
5. PERFORMANCE DEGRADATION PLAYBOOK
6. SECURITY BREACH PLAYBOOK
7. DATA CORRUPTION PLAYBOOK
8. NETWORK FAILURE PLAYBOOK
9. TRADING ANOMALY PLAYBOOK
10. DATABASE FAILURE PLAYBOOK
11. COMMUNICATION PROTOCOLS
12. POST-INCIDENT REVIEW

================================================================================
1. INCIDENT RESPONSE FRAMEWORK
================================================================================

1.1 INCIDENT LIFECYCLE
----------------------

Phase 1: DETECTION (0-5 minutes)
- Alert fires or manual detection
- Initial assessment
- Determine if incident or false alarm

Phase 2: RESPONSE (5-30 minutes)
- Classify severity
- Engage appropriate response team
- Begin containment
- Initiate communication

Phase 3: MITIGATION (30 minutes - 4 hours)
- Implement fixes
- Restore service
- Verify resolution
- Monitor stability

Phase 4: RECOVERY (4-24 hours)
- Full system validation
- Return to normal operations
- Document incident
- Initial lessons learned

Phase 5: POST-INCIDENT (1-7 days)
- Detailed post-mortem
- Root cause analysis
- Action items
- Documentation updates

1.2 INCIDENT COMMANDER ROLE
----------------------------

Responsibilities:
- Overall incident coordination
- Decision making authority
- Communication hub
- Resource allocation
- Timeline management

Current Incident Commander: On-call lead engineer

Backup: VP Engineering

1.3 WAR ROOM PROCEDURES
------------------------

Physical War Room: Conference Room A (if on-site)
Virtual War Room: Zoom link + Slack channel #incident-response

Establish War Room when:
- Severity 1 incident
- Multiple teams needed
- Complexity requires coordination
- Extended duration expected

War Room Setup:
1. Create dedicated Slack channel: #incident-YYYYMMDD-HHMM
2. Start Zoom meeting
3. Assign scribe to document actions
4. Update status page
5. Begin coordinated response

================================================================================
2. SEVERITY CLASSIFICATION
================================================================================

SEV 1 - CRITICAL
-----------------
Definition: Complete service outage or critical security breach
Impact: Trading stopped, major financial loss, or data breach
Response Time: Immediate (< 5 minutes)
Escalation: Page all on-call + management
Examples:
- Complete system crash
- All exchanges disconnected
- Security breach detected
- Killswitch activated
- Database completely down

SEV 2 - HIGH
------------
Definition: Significant degradation or partial outage
Impact: Reduced trading capacity or performance
Response Time: 15 minutes
Escalation: Page on-call engineer
Examples:
- Single exchange disconnected
- High latency (P99 > 10ms)
- Strategy malfunction
- Database slow queries
- Partial data loss

SEV 3 - MEDIUM
--------------
Definition: Minor degradation or isolated issue
Impact: Limited impact, workaround available
Response Time: 1 hour
Escalation: Notify on-call (no page)
Examples:
- Single strategy underperforming
- Non-critical alert firing
- Disk space warning
- Elevated error rate

SEV 4 - LOW
-----------
Definition: Cosmetic issue or future concern
Impact: No immediate impact
Response Time: Next business day
Escalation: Create ticket
Examples:
- Documentation needs update
- Monitoring improvement needed
- Code cleanup required

================================================================================
3. EXCHANGE OUTAGE PLAYBOOK
================================================================================

SCENARIO: Exchange connection lost or exchange experiencing outage

DETECTION:
- Alert: "Exchange Disconnected" fires
- Dashboard shows red status for exchange
- No market data received for 30+ seconds

IMMEDIATE ACTIONS (0-5 minutes):
```bash
# 1. Verify exchange is really down
./scripts/check_exchange_status.sh <EXCHANGE_NAME>
curl https://status.<exchange>.com  # Check status page

# 2. Check if it's our side or exchange side
ping -c 5 stream.<exchange>.com
curl -I https://api.<exchange>.com

# 3. Stop strategies using affected exchange
./scripts/stop_strategies_by_exchange.sh <EXCHANGE_NAME>

# 4. Cancel open orders on exchange (if possible)
curl -X POST http://localhost:8080/api/v1/exchanges/<EXCHANGE_NAME>/cancel_all_orders
```

ASSESSMENT (5-15 minutes):
□ Determine scope: single exchange or multiple
□ Check official exchange status page
□ Estimate downtime duration
□ Assess trading impact
□ Determine if arbitrage/hedging affected

MITIGATION (15-60 minutes):
If Exchange Down (confirmed):
```bash
# 1. Disable exchange in configuration
curl -X PUT http://localhost:8080/api/v1/config \
  -d '{"exchanges": {"<EXCHANGE>": {"enabled": false}}}'

# 2. Redirect strategies to other exchanges (if applicable)
# Edit strategy configs to use alternative exchanges

# 3. Monitor for exchange recovery
watch -n 10 './scripts/check_exchange_status.sh <EXCHANGE_NAME>'
```

If Our Connection Issue:
```bash
# 1. Check network connectivity
traceroute stream.<exchange>.com
mtr --report --report-cycles=10 stream.<exchange>.com

# 2. Verify API keys
./scripts/verify_api_key.sh <EXCHANGE_NAME>

# 3. Attempt reconnection
curl -X POST http://localhost:8080/api/v1/exchanges/<EXCHANGE_NAME>/reconnect

# 4. Check firewall/security groups
sudo iptables -L -n | grep <EXCHANGE_IP>
```

RECOVERY (When exchange back):
```bash
# 1. Verify exchange operational
./scripts/check_exchange_status.sh <EXCHANGE_NAME>

# 2. Reconnect
curl -X POST http://localhost:8080/api/v1/exchanges/<EXCHANGE_NAME>/reconnect

# 3. Reconcile positions
./scripts/reconcile_positions.sh <EXCHANGE_NAME>

# 4. Restart strategies
./scripts/start_strategies_by_exchange.sh <EXCHANGE_NAME>

# 5. Monitor for 30 minutes
./scripts/monitor_exchange.sh <EXCHANGE_NAME>
```

COMMUNICATION:
- T+0min: Alert on-call, trading desk
- T+15min: Update status page
- T+30min: Email stakeholders if still down
- Resolution: Notify all parties

POST-INCIDENT:
- Document downtime duration
- Calculate trading impact (missed opportunities, losses)
- Review exchange reliability
- Consider backup exchanges

================================================================================
4. SYSTEM CRASH PLAYBOOK
================================================================================

SCENARIO: HFT system process crashed or won't start

DETECTION:
- Alert: "System Down" fires
- API not responding
- Process not running (systemctl shows dead)

IMMEDIATE ACTIONS (0-2 minutes):
```bash
# 1. CRITICAL: Activate killswitch if system not responding
# (prevents orphaned orders)
./scripts/emergency_killswitch.sh

# 2. Check process status
systemctl status hft_system
ps aux | grep hft_system

# 3. Check for core dump
ls -lh /tmp/core.* /var/crash/

# 4. Save logs before they rotate
cp /var/log/hft_system/system.log /var/log/hft_system/crash_$(date +%Y%m%d_%H%M%S).log
```

ASSESSMENT (2-5 minutes):
```bash
# Check last 100 log lines for crash reason
tail -100 /var/log/hft_system/crash_*.log

# Common crash reasons:
# - Segmentation fault
# - Out of memory (OOM killer)
# - Assertion failure
# - Unhandled exception
# - Database connection lost

# Check system resources
free -h
df -h
dmesg | tail -50
```

RECOVERY (5-15 minutes):
```bash
# 1. Fix underlying issue (if known)
# Example: If OOM, increase memory or fix leak

# 2. Verify database is running
systemctl status timescaledb
psql -h localhost -U hft_user -d hft_db -c "SELECT 1;"

# 3. Verify configuration is valid
./bin/hft_system --validate-config --config=config/production.json

# 4. Start system
systemctl start hft_system

# 5. Monitor startup
journalctl -u hft_system -f
```

VALIDATION (15-30 minutes):
```bash
# 1. Verify system healthy
./scripts/health_check.sh

# 2. Reconcile positions
./scripts/reconcile_positions.sh

# 3. Verify P&L matches expectations
curl http://localhost:8080/api/v1/positions | jq '.total_unrealized_pnl_usd'

# 4. Restart strategies gradually
# Start one strategy at a time, verify each

# 5. Monitor for 30 minutes for stability
watch -n 5 './scripts/health_check.sh'
```

ESCALATION:
If crash repeats within 1 hour:
- Page Level 2 support
- Consider failover to standby
- Initiate code review
- Check for recent deployments

COMMUNICATION:
- T+0min: Page on-call + trading desk
- T+5min: Status page update
- T+15min: Email management if not recovered
- T+30min: Conference call if still down

POST-INCIDENT:
□ Analyze core dump (if available)
□ Review logs for root cause
□ Create bug ticket
□ Add monitoring to prevent recurrence
□ Update documentation

================================================================================
5. PERFORMANCE DEGRADATION PLAYBOOK
================================================================================

SCENARIO: System latency exceeds thresholds or throughput reduced

DETECTION:
- Alert: "High Latency" fires (P99 > 1ms)
- Trading desk reports slow fills
- Dashboards show degraded performance

IMMEDIATE ACTIONS (0-5 minutes):
```bash
# 1. Check current latency
./scripts/latency_report.sh

# 2. Check system resources
top -bn1 | head -20
free -h
iostat -x 1 5

# 3. Check for long-running queries
psql -h localhost -U hft_user -d hft_db <<EOF
SELECT pid, now() - query_start AS duration, query
FROM pg_stat_activity
WHERE state = 'active' AND now() - query_start > interval '1 second'
ORDER BY duration DESC;
EOF
```

DIAGNOSIS (5-15 minutes):
```bash
# CPU bound?
if [ $(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | cut -d'%' -f1) -gt 80 ]; then
    echo "CPU bound - check for runaway process"
    top -H -p $(cat /var/run/hft_system.pid)
fi

# Memory bound?
if [ $(free | grep Mem | awk '{print ($3/$2)*100}') -gt 90 ]; then
    echo "Memory pressure - check for leak"
    ./scripts/check_memory_leak.sh
fi

# I/O bound?
if [ $(iostat -x 1 2 | tail -1 | awk '{print $NF}') -gt 80 ]; then
    echo "I/O bound - check database"
    iotop -b -n 1 | head -20
fi

# Network bound?
./scripts/check_network_latency.sh
```

MITIGATION:
CPU Bound:
```bash
# Reduce workload
./scripts/stop_low_priority_strategies.sh

# Check for hot threads
./scripts/profile_cpu.sh 10  # Profile for 10 seconds
```

Memory Bound:
```bash
# Clear caches
curl -X POST http://localhost:8080/api/v1/system/clear_caches

# Archive old orders
curl -X POST http://localhost:8080/api/v1/orders/archive

# If critical, restart system
./scripts/graceful_shutdown.sh && sleep 10 && systemctl start hft_system
```

Database Slow:
```bash
# Kill long-running queries
psql -h localhost -U postgres -d hft_db <<EOF
SELECT pg_terminate_backend(pid)
FROM pg_stat_activity
WHERE state = 'active' AND now() - query_start > interval '10 seconds';
EOF

# Vacuum if needed
psql -h localhost -U hft_user -d hft_db -c "VACUUM ANALYZE;"
```

RECOVERY:
```bash
# Monitor latency improvement
watch -n 1 './scripts/latency_report.sh'

# Gradually restore full load
./scripts/start_stopped_strategies.sh --gradual
```

================================================================================
6. SECURITY BREACH PLAYBOOK
================================================================================

SCENARIO: Unauthorized access, suspicious activity, or confirmed breach

DETECTION:
- Alert: "Unauthorized Access Detected"
- Unusual API activity
- Unexpected configuration changes
- Unknown orders placed

IMMEDIATE ACTIONS (0-1 minute):
```bash
# CRITICAL: Activate killswitch immediately
curl -X POST http://localhost:8080/api/v1/killswitch \
  -d '{"reason": "SECURITY BREACH DETECTED"}'

# Stop all trading
systemctl stop hft_system

# Isolate system from network
sudo iptables -P INPUT DROP
sudo iptables -P OUTPUT DROP
sudo iptables -A INPUT -p tcp --dport 22 -s TRUSTED_IP -j ACCEPT
```

ASSESSMENT (1-10 minutes):
```bash
# Preserve evidence
mkdir -p /var/evidence/breach_$(date +%Y%m%d_%H%M%S)
cp -r /var/log/hft_system /var/evidence/breach_*/logs/
cp -r /opt/hft_system/config /var/evidence/breach_*/config/

# Check for unauthorized access
last | head -20
who
sudo ausearch -m USER_LOGIN -ts recent

# Check for unauthorized API calls
grep "unauthorized\|failed auth" /var/log/hft_system/system.log | tail -100

# Check for configuration changes
git -C /opt/hft_system/config diff HEAD
```

CONTAINMENT (10-30 minutes):
```bash
# Rotate all API keys immediately
./scripts/rotate_all_api_keys.sh

# Change all passwords
# (Must be done manually on each exchange)

# Review and revoke any suspicious sessions
# Check exchange account activity logs

# Scan for malware
sudo clamscan -r /opt/hft_system
```

NOTIFICATION (Within 30 minutes):
- Page: Security team immediately
- Notify: Management (CTO, VP Eng, Legal)
- Contact: Exchange support if accounts compromised
- Prepare: Incident report for regulators (if required)

INVESTIGATION:
□ Forensic analysis of logs
□ Determine entry point
□ Identify what data accessed
□ Check for data exfiltration
□ Determine if keys/credentials compromised

RECOVERY:
Only after security team clears:
□ Patch vulnerability
□ Restore from clean backup (if needed)
□ New API keys on all exchanges
□ Enhanced monitoring
□ Resume trading (with approval)

POST-INCIDENT:
- Full security audit
- Penetration testing
- Update security procedures
- Staff training
- Regulatory reporting (if required)

================================================================================
(Additional playbooks 7-10 follow similar detailed format...)
================================================================================

11. COMMUNICATION PROTOCOLS
================================================================================

Internal Communication:
- Slack: #incident-response channel
- Email: incident-team@company.com
- Phone: Emergency hotline +1-555-0100

External Communication:
- Trading Desk: Always notify within 5 minutes
- Management: SEV1/SEV2 within 15 minutes
- Customers: If service impact (rare)

Status Page Updates:
- SEV1: Update every 15 minutes
- SEV2: Update every 30 minutes
- SEV3: Update when resolved

12. POST-INCIDENT REVIEW
================================================================================

Timeline: Within 48 hours of resolution

Attendees:
- Incident Commander
- All responders
- Management representative
- Affected teams

Agenda:
1. Timeline review
2. What went well?
3. What didn't go well?
4. Root cause analysis
5. Action items
6. Documentation updates

Deliverables:
- Written post-mortem document
- Action item list with owners
- Updated runbooks/procedures
- Prevention measures

Contact: operations@company.com / +1-555-0100
================================================================================
END OF DOCUMENT
================================================================================
