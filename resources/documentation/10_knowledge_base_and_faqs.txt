================================================================================
HFT SYSTEM - $(basename $file .txt | tr '_' ' ' | tr '[:lower:]' '[:upper:]')
================================================================================

VERSION: 1.0
LAST UPDATED: 2025-11-25
OWNER: Platform Engineering Team / Operations Team
CLASSIFICATION: Internal Use Only

================================================================================ 
TABLE OF CONTENTS
================================================================================

1. OVERVIEW AND PURPOSE
2. RESPONSIBILITIES AND DUTIES
3. ESCALATION PROCEDURES
4. COMMON SCENARIOS AND RESPONSES
5. TOOLS AND ACCESS REQUIRED
6. COMMUNICATION GUIDELINES
7. HANDOFF PROCEDURES
8. EMERGENCY CONTACTS
9. BEST PRACTICES
10. TROUBLESHOOTING QUICK REFERENCE
11. ON-CALL SCHEDULE AND ROTATION
12. FREQUENTLY ASKED QUESTIONS

================================================================================
1. OVERVIEW AND PURPOSE
================================================================================

This document provides comprehensive procedures and guidelines for the
on-call engineer role, change management processes, and common knowledge
base for the HFT cryptocurrency trading system.

Primary Goals:
- Ensure 24/7 system availability and support
- Quick response to incidents and alerts
- Proper escalation when needed
- Clear communication with stakeholders
- Systematic change management
- Knowledge sharing across team

Target Audience:
- On-call engineers
- Operations team
- Platform engineers
- DevOps team
- All technical staff

================================================================================
2. RESPONSIBILITIES AND DUTIES
================================================================================

On-Call Engineer Primary Responsibilities:
------------------------------------------
1. Monitor alerts 24/7 during on-call shift
2. Respond to incidents within SLA timeframes
3. Perform initial triage and diagnosis
4. Escalate when necessary
5. Communicate status to stakeholders
6. Document all incidents
7. Perform routine health checks
8. Execute scheduled maintenance (if any)

Response Time SLAs:
- SEV 1 (Critical): Acknowledge < 5 minutes, engage < 15 minutes
- SEV 2 (High): Acknowledge < 15 minutes, engage < 30 minutes
- SEV 3 (Medium): Acknowledge < 1 hour, engage < 4 hours
- SEV 4 (Low): Next business day

Daily Tasks:
□ Morning: Review overnight alerts and logs
□ Midday: Check system health and metrics
□ Evening: Review day's activities, prepare handoff
□ Continuous: Monitor alerts, respond as needed

Weekly Tasks:
□ Review incident trends
□ Update documentation as needed
□ Participate in on-call retrospective
□ Test emergency procedures

================================================================================
3. ESCALATION PROCEDURES
================================================================================

Escalation Matrix:
-----------------

Level 1: On-Call Engineer (You)
- Handle routine alerts
- Perform standard troubleshooting
- Execute documented procedures
- Escalate if needed

Level 2: Senior Platform Engineer
- Complex technical issues
- Issues requiring code changes
- Architectural decisions
- After 1 hour with no resolution

Level 3: Engineering Lead / VP Engineering
- Major outages (> 2 hours)
- Security incidents
- Regulatory issues
- High financial impact

Emergency Escalation:
- Trading Desk: Always notify within 5 minutes of SEV1
- Management: Notify within 15 minutes of SEV1
- Security Team: Immediate for security incidents
- Legal/Compliance: For regulatory issues

When to Escalate:
- Issue beyond your expertise
- Resolution time exceeds 1 hour
- Multiple systems affected
- Security concerns
- Regulatory implications
- High financial impact (> $10K)
- Unclear next steps

How to Escalate:
1. Document what you've tried
2. Gather relevant logs and metrics
3. Call escalation contact (don't just email)
4. Provide clear summary of situation
5. Stay engaged until handoff complete

Escalation Contacts:
- Level 2: senior-oncall@company.com (+1-555-0101)
- Level 3: management-oncall@company.com (+1-555-0104)
- Security: security@company.com (+1-555-0102)
- Trading Desk: trading@company.com (+1-555-0103)

================================================================================
4. COMMON SCENARIOS AND RESPONSES
================================================================================

SCENARIO 1: "Exchange Disconnected" Alert
------------------------------------------
Severity: SEV2 (High) - single exchange, SEV1 if multiple

Response:
```bash
# 1. Acknowledge alert
# 2. Check exchange status
./scripts/check_exchange_status.sh <EXCHANGE>

# 3. If exchange down (their issue):
# - Stop strategies using that exchange
# - Notify trading desk
# - Monitor for recovery
# - No further action needed

# 4. If our connectivity issue:
# - Check network connectivity
# - Attempt reconnection
# - Check API keys
# - Review firewall rules
```

Escalate if: Cannot reconnect after 30 minutes

SCENARIO 2: "High Latency" Alert
---------------------------------
Severity: SEV3 (Medium) unless sustained > 5 minutes (then SEV2)

Response:
```bash
# 1. Check current latency
./scripts/latency_report.sh

# 2. Check system resources
top -bn1 | head -20
free -h

# 3. Check for issues:
# - CPU > 80%: Check for runaway process
# - Memory > 90%: Check for memory leak
# - Disk I/O high: Check database

# 4. If transient spike: Monitor and document
# 5. If sustained: Escalate to Level 2
```

Escalate if: Latency remains high after initial checks

SCENARIO 3: "Daily Loss Limit Approaching" Alert
-------------------------------------------------
Severity: SEV1 (Critical)

Response:
```bash
# 1. IMMEDIATE: Review current P&L
curl http://localhost:8080/api/v1/risk/summary

# 2. Check which strategies losing money
curl http://localhost:8080/api/v1/strategies | jq '.strategies[] | select(.performance.daily_pnl_usd < 0)'

# 3. NOTIFY trading desk immediately

# 4. If at 90% of limit:
# - Prepare to activate killswitch
# - Stop losing strategies
# - Wait for trading desk decision

# 5. If limit hit:
# - Killswitch activates automatically
# - Notify management
```

Escalate immediately: This requires trading desk decision

SCENARIO 4: "System Crash / Not Responding"
--------------------------------------------
Severity: SEV1 (Critical)

Response:
```bash
# 1. IMMEDIATE: Page Level 2 + trading desk
# 2. Check if process running
systemctl status hft_system

# 3. Check logs for crash reason
tail -100 /var/log/hft_system/system.log

# 4. If safe to restart:
systemctl start hft_system

# 5. Monitor startup
journalctl -u hft_system -f

# 6. If doesn't start or crashes again:
# - Failover to standby
# - Escalate to Level 2 for investigation
```

Escalate immediately: Critical system failure

SCENARIO 5: "Disk Space Low" Alert
-----------------------------------
Severity: SEV3 (Medium) unless > 95% (then SEV2)

Response:
```bash
# 1. Check disk usage
df -h

# 2. Find large files
du -h /var/log/hft_system | sort -rh | head -20

# 3. Clean up:
# - Archive old logs
./scripts/archive_logs.sh --days=30

# - Drop old database partitions
./scripts/cleanup_old_partitions.sh

# 4. Monitor disk space
watch -n 60 'df -h'
```

Escalate if: Cannot free sufficient space

================================================================================
5. TOOLS AND ACCESS REQUIRED
================================================================================

Required Access:
□ SSH access to all production servers
□ sudo access (for system-level commands)
□ Database access (read/write)
□ Grafana/Prometheus access
□ PagerDuty account
□ Slack (all relevant channels)
□ VPN access (if remote)
□ Exchange web UIs (emergency manual trading)
□ API keys for system management

Required Tools:
□ Terminal emulator (iTerm2, Terminator, etc.)
□ SSH client configured
□ VPN client
□ PagerDuty mobile app
□ Slack mobile app
□ Web browser (for Grafana, exchange UIs)
□ Documentation access (offline copy recommended)

Verify Access Before Shift:
```bash
# Test SSH
ssh hft-prod-01 'hostname'

# Test API
curl http://hft-prod-01:8080/api/v1/health

# Test database
psql -h hft-prod-01 -U hft_user -d hft_db -c "SELECT 1;"

# Test sudo
ssh hft-prod-01 'sudo -n systemctl status hft_system'
```

================================================================================
6. COMMUNICATION GUIDELINES
================================================================================

Communication Channels:
- PagerDuty: Primary alerting
- Slack #incident-response: Active incidents
- Slack #hft-operations: Routine updates
- Email: Non-urgent communication
- Phone: Critical escalations only

Status Updates:
- SEV1: Update every 15 minutes
- SEV2: Update every 30 minutes
- SEV3: Update when resolved
- All: Final update when resolved

Status Page Updates:
URL: https://status.company.com

Update when:
- SEV1 or SEV2 incident
- Planned maintenance
- Service degradation

Tone and Style:
- Be factual and concise
- Avoid speculation
- Use plain language (not jargon)
- Include timeline and next steps

Example Good Update:
"11:30 AM: We are investigating connection issues with Binance exchange. 
Trading strategies using Binance have been stopped. Other exchanges operating 
normally. Next update in 15 minutes."

Example Bad Update:
"Something's wrong with Binance, looking into it."

================================================================================
7. HANDOFF PROCEDURES
================================================================================

End of Shift Handoff:
--------------------

30 Minutes Before Shift End:
1. Document all open incidents
2. Prepare handoff notes
3. Schedule handoff call (if complex situation)

At Shift End:
1. Join handoff call with incoming engineer
2. Review:
   - Open incidents and status
   - Recent alerts and resolutions
   - Any ongoing investigations
   - Scheduled maintenance
   - Anything unusual observed

3. Transfer PagerDuty primary on-call status
4. Confirm incoming engineer has access to all systems
5. Stay available for 30 minutes post-handoff

Handoff Template:
```
On-Call Handoff: [DATE] [TIME]
From: [YOUR NAME]
To: [INCOMING ENGINEER]

Open Incidents:
- INC-12345: Exchange connectivity issue (SEV2)
  Status: Monitoring for recovery
  Next steps: Reconnect when exchange up
  
Recent Activity:
- 14:30: High latency alert, resolved by restarting strategy
- 16:00: Routine database vacuum completed
- 18:00: Disk cleanup performed

Ongoing Monitoring:
- Binance connection flaky, watch for disconnects
- Memory usage trending up, may need restart in 24h

Scheduled Maintenance:
- None

Notes:
- Trading desk requested increased position limits (approved)
- New strategy deployed to staging, watch for issues

Contact me at: [PHONE] if questions
```

================================================================================
8. EMERGENCY CONTACTS
================================================================================

24/7 Emergency Contacts:
-----------------------
On-Call Engineer: via PagerDuty
Trading Desk: +1-555-0103
Operations Lead: +1-555-0100
Engineering Lead: +1-555-0101
Security Team: +1-555-0102
Management (Emergency): +1-555-0104

Exchange Support (24/7):
-----------------------
Binance: support@binance.com / Live chat
Coinbase: https://help.coinbase.com / +1-888-908-7930
Kraken: support@kraken.com / +1-833-KRAKEN-1
OKX: support@okx.com / Live chat
(See detailed exchange contact sheet for all 10+ exchanges)

Internal Teams:
--------------
Platform Engineering: platform-eng@company.com
Database Team: database@company.com
Security Team: security@company.com
Compliance: compliance@company.com

External Vendors:
----------------
Hosting Provider: support@provider.com (+1-800-XXX-XXXX)
Network Provider: noc@network.com (+1-800-YYY-YYYY)

================================================================================
9. BEST PRACTICES
================================================================================

DO:
- Acknowledge alerts promptly
- Document everything you do
- Ask for help early if unsure
- Communicate proactively
- Follow runbooks and procedures
- Test in staging when possible
- Take breaks (for long incidents)
- Review documentation after resolving
- Learn from each incident

DON'T:
- Ignore alerts (even false positives - acknowledge them)
- Make changes without understanding impact
- Restart services without checking state first
- Skip communication steps
- Escalate without providing context
- Panic or rush
- Work on production when tired
- Forget to document resolution

Tips for Success:
- Keep calm under pressure
- Communicate clearly and often
- Use checklists and runbooks
- Don't assume - verify everything
- When in doubt, ask
- Better to escalate early than late
- Document as you go, not after

Mental Health:
- On-call can be stressful
- Take breaks between incidents
- Ask for help if overwhelmed
- Debrief after major incidents
- It's okay to escalate
- Your health matters

================================================================================
10. TROUBLESHOOTING QUICK REFERENCE
================================================================================

Quick Commands:
```bash
# System status
curl http://localhost:8080/api/v1/status | jq .

# Health check
./scripts/health_check.sh

# Current positions
curl http://localhost:8080/api/v1/positions | jq .

# Recent errors
tail -100 /var/log/hft_system/errors.log

# System resources
top -bn1 | head -20 && free -h && df -h

# Exchange status
curl http://localhost:8080/api/v1/status | jq '.exchanges'

# Strategy status
curl http://localhost:8080/api/v1/strategies | jq '.strategies[] | {id: .strategy_id, status: .status, pnl: .performance.daily_pnl_usd}'

# Recent alerts
./scripts/recent_alerts.sh --hours=1

# Kill switch
curl -X POST http://localhost:8080/api/v1/killswitch -d '{"reason": "REASON"}'
```

Decision Tree - "Where to Start?":
```
Alert received
    ↓
Is it SEV1 (critical)?
    ↓ YES → Page Level 2 + Trading Desk immediately, then investigate
    ↓ NO
    ↓
Can you resolve in 15 minutes using runbooks?
    ↓ YES → Follow runbook, document, resolve
    ↓ NO
    ↓
Is it within your expertise?
    ↓ YES → Investigate, update stakeholders every 30min
    ↓ NO
    ↓
Escalate to Level 2 with full context
```

================================================================================
11. ON-CALL SCHEDULE AND ROTATION
================================================================================

Rotation Schedule:
- Duration: 1 week per rotation
- Handoff: Friday 5:00 PM local time
- Primary + Secondary on-call at all times

Current Rotation: (See PagerDuty for up-to-date schedule)

Primary On-Call Responsibilities:
- Respond to all alerts
- Lead incident response
- Perform routine checks
- Execute scheduled tasks

Secondary On-Call Responsibilities:
- Backup for primary
- Respond if primary doesn't acknowledge (15min)
- Assist with major incidents
- Cover for primary's breaks

Calendar:
- All on-call shifts in Google Calendar
- Block off time (no meetings during shift if possible)
- PagerDuty schedule is source of truth

Coverage for Time Off:
- Request swap in PagerDuty at least 1 week in advance
- Notify team in #hft-operations
- Manager approval for extended leave

Compensation:
- On-call stipend: Per company policy
- Overtime pay: If incidents during off-hours
- Comp time: Available for major incidents

================================================================================
12. FREQUENTLY ASKED QUESTIONS
================================================================================

Q: What if I get an alert I don't understand?
A: Acknowledge it, check runbooks. If still unclear after 15 minutes, escalate.

Q: Can I restart the system without approval?
A: For SEV1 outages during off-hours, yes. Otherwise, get approval.

Q: What if I'm in a no-service area?
A: Have secondary cover, or swap shifts. Always have connectivity.

Q: How do I know if I should activate the kill switch?
A: When in doubt, activate it. Better safe than sorry. Notify trading desk.

Q: What if multiple alerts fire at once?
A: Triage by severity. Address SEV1 first, escalate if overwhelmed.

Q: Should I fix the root cause or just the symptom?
A: Fix symptom to restore service, then investigate root cause.

Q: What if I make a mistake?
A: Document it, learn from it, share with team. Everyone makes mistakes.

Q: How much detail should I include in incident reports?
A: Enough for someone else to understand what happened and why.

Q: Can I ignore false positive alerts?
A: No - acknowledge them and document as false positive. Fix underlying issue.

Q: What if I need to take a break during a long incident?
A: Communicate with team, have secondary cover, set alarm to return.

================================================================================
CHANGE MANAGEMENT - KEY POINTS
================================================================================

All changes require:
□ Change request ticket
□ Risk assessment
□ Test plan
□ Rollback plan
□ Appropriate approvals
□ Communication to stakeholders

Change Categories:
- Standard: Pre-approved, low risk (config tweaks)
- Normal: Requires approval, tested (code deployments)
- Emergency: Post-approval, critical fixes

Emergency Changes:
- Can proceed with verbal approval
- Must document in ticket within 24 hours
- Post-incident review required

Change Windows:
- Preferred: Sunday 2:00-6:00 AM UTC (low trading volume)
- Avoid: Market open hours, high volatility periods
- Plan: Coordinate with trading desk

================================================================================
KNOWLEDGE BASE - COMMON ISSUES
================================================================================

Issue: Exchange connection keeps dropping
Solution: Check for rate limiting, verify API keys not expired,
          check for IP restrictions, review firewall rules

Issue: High latency after deployment
Solution: Check new code for blocking operations, verify thread
          pinning correct, check for increased logging, profile CPU

Issue: Strategy not placing orders
Solution: Check trading enabled, strategy running, risk limits not hit,
          exchange connected, sufficient balance

Issue: Database running out of space
Solution: Drop old partitions, archive old data, vacuum tables,
          check for table bloat

Issue: Memory usage growing over time
Solution: Likely memory leak - take heap dump, schedule restart,
          escalate to engineering for analysis

Issue: Orders stuck in pending state
Solution: Check exchange API status, verify network connectivity,
          query exchange directly for order status, reconcile

================================================================================
GETTING HELP
================================================================================

Documentation:
- See all 11 documentation files in this package
- Start with troubleshooting guide
- Reference operational runbooks

Team Support:
- Slack: #hft-operations (ask anytime)
- Senior engineers available for guidance
- Don't hesitate to escalate

Training:
- Shadow experienced on-call engineers
- Practice in test environment
- Review past incidents
- Monthly on-call training sessions

Resources:
- Runbook collection: /opt/hft_system/documentation/
- Internal wiki: https://wiki.company.com/hft
- Exchange docs: Linked in detailed documentation

Contact Information:
- Operations: operations@company.com
- Engineering: platform-eng@company.com
- On-Call Support: +1-555-0100

Remember: It's better to ask for help than to guess!

================================================================================
DOCUMENT REVISION HISTORY
================================================================================

Version    Date          Author              Changes
------------------------------------------------------------------------
1.0        2025-11-25    Operations Team     Initial procedures guide

================================================================================
END OF DOCUMENT
================================================================================
