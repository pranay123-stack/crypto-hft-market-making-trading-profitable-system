================================================================================
                    NETWORK INFRASTRUCTURE FOR HFT
                    Switches, NICs, Cabling, and Optimization
================================================================================

TABLE OF CONTENTS
================================================================================
1. Network Requirements for HFT
2. Network Topology Design
3. Switches and Routers
4. Network Interface Cards (NICs)
5. Cabling and Physical Layer
6. Network Optimization
7. Kernel Bypass Technologies
8. Monitoring and Troubleshooting

================================================================================
1. NETWORK REQUIREMENTS FOR HFT
================================================================================

LATENCY TARGETS:
- Exchange to Gateway: < 500 μs (one-way)
- Gateway to Strategy: < 10 μs
- Strategy to OMS: < 10 μs
- OMS to Execution: < 10 μs
- Total (end-to-end): < 50 μs (application)

NETWORK LATENCY BREAKDOWN:
┌──────────────────────┬──────────────┬──────────────┐
│ Component            │ Latency      │ Notes        │
├──────────────────────┼──────────────┼──────────────┤
│ NIC (standard)       │ 10-50 μs     │ Interrupt    │
│ NIC (optimized)      │ 1-5 μs       │ Poll mode    │
│ Switch (cut-through) │ 0.5-2 μs     │ Per hop      │
│ Switch (store-fwd)   │ 5-20 μs      │ Per hop      │
│ Cable (1m)          │ 0.005 μs     │ Per meter    │
│ Fiber (100m)        │ 0.5 μs       │ Per 100m     │
│ TCP/IP stack        │ 30-100 μs    │ Kernel       │
│ TCP/IP (tuned)      │ 10-30 μs     │ Optimized    │
│ Kernel bypass (DPDK)│ 2-10 μs      │ Userspace    │
└──────────────────────┴──────────────┴──────────────┘

KEY REQUIREMENTS:
✓ Lowest possible latency
✓ Deterministic performance (low jitter)
✓ High throughput (10/25/40/100 Gbps)
✓ Redundancy (no single point of failure)
✓ Monitoring and observability

================================================================================
2. NETWORK TOPOLOGY DESIGN
================================================================================

TRADITIONAL 3-TIER (NOT RECOMMENDED FOR HFT):

             ┌──────────┐
             │   Core   │
             │  Switch  │
             └────┬─────┘
          ┌───────┴───────┐
    ┌─────▼────┐    ┌────▼─────┐
    │Aggregation│    │Aggregation│
    │ Switch   │    │ Switch    │
    └────┬─────┘    └────┬──────┘
         │               │
    ┌────▼─────┬────────▼┬─────┐
    │  ToR 1   │  ToR 2  │ToR 3│
    └────┬─────┴─────┬───┴──┬──┘
         │           │      │
    [Servers]   [Servers] [Servers]

Latency: 3 hops = 6-60 μs (too high)

LEAF-SPINE (RECOMMENDED FOR HFT):

         ┌──────┐  ┌──────┐  ┌──────┐
         │Spine1│  │Spine2│  │Spine3│
         └──┬───┘  └───┬──┘  └───┬──┘
     ┌──────┼──────────┼─────────┼──────┐
     │      │          │         │      │
  ┌──▼──┐┌─▼───┐   ┌──▼──┐   ┌──▼──┐┌─▼───┐
  │Leaf1││Leaf2│   │Leaf3│   │Leaf4││Leaf5│
  └──┬──┘└──┬──┘   └──┬──┘   └──┬──┘└──┬──┘
     │      │         │         │      │
  [Srvs] [Srvs]   [Srvs]    [Srvs]  [Srvs]

Latency: 2 hops max = 1-4 μs
Benefits: Predictable latency, easy to scale

ULTRA-LOW-LATENCY (FLAT):

Exchange
   │
   ├──────────────────────┐
   │                      │
   ▼                      ▼
┌─────────┐          ┌─────────┐
│ Gateway │          │ Gateway │
│  (A)    │          │  (B)    │
└────┬────┘          └────┬────┘
     │                    │
     └──────┬─────────────┘
            │
     ┌──────▼──────┐
     │Top-of-Rack  │
     │   Switch    │
     └──────┬──────┘
            │
     ┌──────┼──────┬──────┬──────┐
     │      │      │      │      │
  ┌──▼─┐┌──▼─┐ ┌──▼─┐ ┌──▼─┐ ┌──▼─┐
  │MD  ││Strat││Risk││OMS ││Exec│
  └────┘└────┘ └────┘ └────┘ └────┘

Latency: 1 hop = 0.5-2 μs
Best for: Co-located trading

CO-LOCATION TOPOLOGY:

┌─────────────────────────────────────────┐
│         Exchange Data Center            │
│  ┌─────────────────────────────────┐    │
│  │   Exchange Matching Engine      │    │
│  └──────────────┬──────────────────┘    │
│                 │                        │
│  ┌──────────────▼──────────────────┐    │
│  │     Exchange Switch             │    │
│  └──────────────┬──────────────────┘    │
│                 │                        │
│  ┌──────────────▼──────────────────┐    │
│  │   Customer Cross-Connect        │    │
│  └──────────────┬──────────────────┘    │
└─────────────────┼───────────────────────┘
                  │ (< 10m fiber)
┌─────────────────▼───────────────────────┐
│      Customer Cage/Rack                 │
│  ┌──────────┐      ┌──────────┐        │
│  │ Gateway  │─────▶│ Trading  │        │
│  │  Server  │      │  Server  │        │
│  └──────────┘      └──────────┘        │
└─────────────────────────────────────────┘

Physical Distance: < 100 meters
Latency: < 50 μs (one-way to exchange)

================================================================================
3. SWITCHES AND ROUTERS
================================================================================

RECOMMENDED SWITCHES FOR HFT:

ARISTA 7050X3:
- Latency: 350 ns (cut-through)
- Ports: 32x100GbE or 128x25GbE
- Features: Low jitter, precision timing
- Price: $30K-50K

CISCO NEXUS 3172:
- Latency: 390 ns
- Ports: 48x10GbE + 6x40GbE
- Features: Low latency, high throughput
- Price: $25K-40K

MELLANOX SN2700:
- Latency: 300 ns
- Ports: 32x100GbE
- Features: RoCE, low latency
- Price: $35K-55K

SWITCH CONFIGURATION:

# Cisco NX-OS
! Enable cut-through switching
hardware profile tcam resource template cut-through

! Disable spanning tree (if no loops)
no spanning-tree vlan 1-4094

! Optimize buffers
hardware profile buffer info detail

! QoS for low latency
class-map type qos match-all TRADING
  match dscp ef

policy-map type qos PRIORITY
  class TRADING
    set qos-group 1
    priority level 1

# Arista EOS
! Cut-through switching
hardware forwarding-table low-latency

! Priority flow control
priority-flow-control mode on

! Queue configuration
qos map dscp 46 to traffic-class 7

SWITCH PLACEMENT:

Top-of-Rack (ToR):
- One switch per rack
- 10/25 GbE to servers
- 40/100 GbE uplinks
- < 1 μs latency

Leaf Switch:
- Aggregates multiple ToRs
- All servers 2 hops away
- 40/100 GbE links
- < 2 μs latency

================================================================================
4. NETWORK INTERFACE CARDS (NICs)
================================================================================

RECOMMENDED NICS:

INTEL X710 (10GbE):
- Latency: ~5 μs (kernel), ~1 μs (DPDK)
- Features: RSS, TSO, SR-IOV
- DPDK support: Excellent
- Price: $500-800

MELLANOX CONNECTX-6 (100GbE):
- Latency: ~3 μs (kernel), ~0.5 μs (DPDK)
- Features: RoCE, GPUDirect
- DPDK support: Excellent
- Price: $1500-2500

SOLARFLARE SFN8522 (10GbE):
- Latency: ~2 μs (kernel bypass)
- Features: Onload, low latency
- TCPDirect: Yes
- Price: $1000-1500

NIC CONFIGURATION:

# ethtool configuration
ethtool -G eth0 rx 4096 tx 4096  # Max ring buffers
ethtool -C eth0 rx-usecs 0       # Disable interrupt coalescing
ethtool -K eth0 tso off gso off  # Disable offloading for latency

# RSS (Receive Side Scaling)
ethtool -X eth0 equal 4  # Distribute across 4 queues

# IRQ affinity
echo 1 > /proc/irq/45/smp_affinity  # Pin IRQ to CPU 0

MULTI-QUEUE CONFIGURATION:

# 4 RX queues, pin to CPUs 0-3
for i in {0..3}; do
  echo $((1 << i)) > /proc/irq/$((45 + i))/smp_affinity
done

SR-IOV (Single Root I/O Virtualization):

# Enable SR-IOV
echo 4 > /sys/class/net/eth0/device/sriov_numvfs

# Assign VF to VM
virsh attach-interface vm1 hostdev <VF-PCI-address>

Benefits: Direct hardware access, lower latency

================================================================================
5. CABLING AND PHYSICAL LAYER
================================================================================

CABLE TYPES:

COPPER (DAC - Direct Attach Copper):
- Distance: Up to 7 meters
- Latency: ~5 ns per meter
- Use: Intra-rack connections
- Cost: $50-100 per cable

FIBER (MMF - Multi-Mode Fiber):
- Distance: Up to 300 meters
- Latency: ~5 ns per meter
- Use: Inter-rack connections
- Cost: $100-200 per cable

FIBER (SMF - Single-Mode Fiber):
- Distance: Up to 10 km
- Latency: ~5 ns per meter  
- Use: Inter-building
- Cost: $150-300 per cable

RECOMMENDATIONS:
- Intra-rack (< 7m): DAC (lowest latency)
- Inter-rack (< 100m): MMF
- Long distance (> 100m): SMF

CABLE MANAGEMENT:
✓ Shortest possible paths
✓ Avoid unnecessary patch panels
✓ Direct connections where possible
✓ Proper labeling
✓ Redundant paths for critical links

PHYSICAL PROXIMITY:

Co-location Benefits:
- Exchange to Gateway: < 10 meters fiber
- Latency savings: 200-500 μs vs remote
- Critical for HFT competitiveness

Rack Layout:
Row 1: Exchange cross-connects
Row 2: Gateway servers (closest to exchange)
Row 3: Trading servers
Row 4: Supporting infrastructure

================================================================================
6. NETWORK OPTIMIZATION
================================================================================

KERNEL NETWORK STACK TUNING:

# /etc/sysctl.conf

# TCP settings
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.ipv4.tcp_rmem = 4096 87380 67108864
net.ipv4.tcp_wmem = 4096 65536 67108864
net.ipv4.tcp_congestion_control = htcp
net.ipv4.tcp_mtu_probing = 1

# Disable TCP timestamps (reduces CPU)
net.ipv4.tcp_timestamps = 0

# Increase max backlog
net.core.netdev_max_backlog = 10000

# Reduce TCP keepalive (faster detection)
net.ipv4.tcp_keepalive_time = 60
net.ipv4.tcp_keepalive_intvl = 10
net.ipv4.tcp_keepalive_probes = 6

# Fast recycling
net.ipv4.tcp_tw_reuse = 1

Apply:
sysctl -p

CPU AFFINITY FOR NETWORK:

# Pin network IRQs to specific CPUs
for irq in $(cat /proc/interrupts | grep eth0 | cut -d: -f1); do
  echo 1 > /proc/irq/$irq/smp_affinity  # CPU 0
done

NUMA OPTIMIZATION:

# Check NIC NUMA node
cat /sys/class/net/eth0/device/numa_node

# Pin process to same NUMA node
numactl --cpunodebind=0 --membind=0 ./trading_app

JUMBO FRAMES:

# Enable jumbo frames (9000 MTU)
ip link set eth0 mtu 9000

Benefits: Fewer packets, lower CPU, higher throughput
Use when: All devices in path support it

================================================================================
7. KERNEL BYPASS TECHNOLOGIES
================================================================================

DPDK (Data Plane Development Kit):

Architecture:
┌─────────────────────────────────┐
│    Application (User Space)     │
│  ┌────────────────────────────┐ │
│  │      DPDK Libraries        │ │
│  │  (Poll Mode Drivers)       │ │
│  └────────┬───────────────────┘ │
└───────────┼─────────────────────┘
            │ (No kernel)
       ┌────▼────┐
       │   NIC   │
       └─────────┘

Benefits:
- 10x lower latency (2-10 μs vs 30-100 μs)
- 10x higher throughput
- Zero-copy
- Predictable performance

C++ DPDK Example:

#include <rte_eal.h>
#include <rte_ethdev.h>
#include <rte_mbuf.h>

class DPDKInterface {
    uint16_t port_id_;
    struct rte_mempool* mbuf_pool_;

public:
    DPDKInterface(int argc, char** argv) {
        // Initialize DPDK
        int ret = rte_eal_init(argc, argv);
        if (ret < 0) {
            throw std::runtime_error("EAL init failed");
        }

        // Get available ports
        uint16_t nb_ports = rte_eth_dev_count_avail();
        if (nb_ports == 0) {
            throw std::runtime_error("No Ethernet ports");
        }

        port_id_ = 0;

        // Create mbuf pool
        mbuf_pool_ = rte_pktmbuf_pool_create(
            "mbuf_pool",
            8192,               // Number of mbufs
            256,                // Cache size
            0,                  // Private data size
            RTE_MBUF_DEFAULT_BUF_SIZE,
            rte_socket_id()
        );

        // Configure port
        struct rte_eth_conf port_conf = {};
        rte_eth_dev_configure(port_id_, 1, 1, &port_conf);

        // Setup RX queue
        rte_eth_rx_queue_setup(port_id_, 0, 1024,
                              rte_eth_dev_socket_id(port_id_),
                              nullptr, mbuf_pool_);

        // Setup TX queue
        rte_eth_tx_queue_setup(port_id_, 0, 1024,
                              rte_eth_dev_socket_id(port_id_),
                              nullptr);

        // Start port
        rte_eth_dev_start(port_id_);
    }

    // Receive packets
    void receivePackets() {
        struct rte_mbuf* bufs[32];

        while (true) {
            uint16_t nb_rx = rte_eth_rx_burst(port_id_, 0, bufs, 32);

            for (uint16_t i = 0; i < nb_rx; ++i) {
                processPacket(bufs[i]);
                rte_pktmbuf_free(bufs[i]);
            }
        }
    }

    // Send packet
    void sendPacket(const void* data, size_t len) {
        struct rte_mbuf* mbuf = rte_pktmbuf_alloc(mbuf_pool_);
        if (!mbuf) return;

        char* pkt_data = rte_pktmbuf_mtod(mbuf, char*);
        memcpy(pkt_data, data, len);
        mbuf->data_len = len;
        mbuf->pkt_len = len;

        rte_eth_tx_burst(port_id_, 0, &mbuf, 1);
    }
};

PF_RING (Alternative to DPDK):

Features:
- Kernel module + userspace library
- Lower complexity than DPDK
- Good performance (5-15 μs)
- Easier to integrate

Usage:
- Market data capture
- Packet analysis
- Load balancing

================================================================================
8. MONITORING AND TROUBLESHOOTING
================================================================================

NETWORK MONITORING TOOLS:

ethtool:
ethtool -S eth0  # Statistics
ethtool -g eth0  # Ring buffer sizes
ethtool -k eth0  # Offload features

netstat/ss:
ss -s            # Summary
ss -tan          # TCP connections
netstat -i       # Interface stats

tcpdump:
tcpdump -i eth0 -c 100 -nn  # Capture 100 packets

iperf3:
# Server
iperf3 -s

# Client
iperf3 -c server_ip -t 10

LATENCY MEASUREMENT:

# Ping (ICMP)
ping -c 100 -i 0.2 192.168.1.10

# TCP latency
qperf server_ip tcp_lat

# Application-level (custom)
timestamp_t1 = get_timestamp();
send_packet();
receive_response();
timestamp_t2 = get_timestamp();
latency = t2 - t1;

PACKET LOSS DETECTION:

# Check interface drops
ethtool -S eth0 | grep drop

# Check kernel drops
netstat -s | grep dropped

# Check ring buffer full
ethtool -S eth0 | grep fifo

COMMON ISSUES:

Issue: High latency spikes
Cause: Interrupt coalescing enabled
Fix: ethtool -C eth0 rx-usecs 0

Issue: Packet drops
Cause: Small ring buffers
Fix: ethtool -G eth0 rx 4096

Issue: High CPU for network
Cause: No RSS, single queue
Fix: Enable multi-queue RSS

================================================================================
END OF NETWORK INFRASTRUCTURE
================================================================================
