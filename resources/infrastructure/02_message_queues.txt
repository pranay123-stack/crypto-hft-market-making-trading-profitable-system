================================================================================
                    MESSAGE QUEUE COMPARISON FOR HFT
                    ZeroMQ, RabbitMQ, Kafka - Performance & Use Cases
================================================================================

TABLE OF CONTENTS
================================================================================
1. Introduction to Message Queues in HFT
2. ZeroMQ - Lightweight Messaging Library
3. RabbitMQ - Enterprise Message Broker
4. Apache Kafka - Distributed Streaming Platform
5. Performance Comparison
6. Use Case Analysis
7. Latency Benchmarks
8. Throughput Benchmarks
9. Implementation Examples
10. Hybrid Architecture Design
11. Selection Criteria
12. Best Practices

================================================================================
1. INTRODUCTION TO MESSAGE QUEUES IN HFT
================================================================================

Message queues provide asynchronous communication between distributed
components. For HFT, the choice of message queue significantly impacts
latency and throughput.

KEY REQUIREMENTS FOR HFT:

Performance:
- Ultra-low latency (< 10 microseconds preferred)
- High throughput (millions of messages/second)
- Low jitter (consistent latency)

Reliability:
- Message delivery guarantees
- No message loss for orders
- Durability options

Scalability:
- Horizontal scaling
- Load distribution
- Fan-out capabilities

Operational:
- Simple deployment
- Low resource overhead
- Easy monitoring

================================================================================
2. ZEROMQ - LIGHTWEIGHT MESSAGING LIBRARY
================================================================================

OVERVIEW:

ZeroMQ is a high-performance asynchronous messaging library that provides
socket-style API with message-oriented semantics.

Key Characteristics:
- Library, not a broker (no central server)
- Multiple transport: IPC, TCP, inproc, PGM
- Multiple patterns: REQ-REP, PUB-SUB, PUSH-PULL, PAIR
- Zero-copy message passing
- Low latency (5-20 microseconds for IPC)

ARCHITECTURE:

Traditional Message Broker:
┌─────────┐      ┌────────┐      ┌─────────┐
│Publisher│─────>│ Broker │─────>│Subscriber│
└─────────┘      └────────┘      └─────────┘
                   (Central)

ZeroMQ (Brokerless):
┌─────────┐                      ┌─────────┐
│Publisher│─────────────────────>│Subscriber│
└─────────┘   (Direct Connection) └─────────┘

MESSAGING PATTERNS:

1. REQUEST-REPLY (REQ-REP)
   Client ─[REQ]─> Server
   Client <─[REP]─ Server

   Use Case: Risk checks, configuration queries

2. PUBLISH-SUBSCRIBE (PUB-SUB)
   Publisher ─[PUB]─> Subscriber 1
                   └─> Subscriber 2
                   └─> Subscriber N

   Use Case: Market data distribution, order updates

3. PUSH-PULL (Pipeline)
   Producer ─[PUSH]─> Worker 1
                   └─> Worker 2
                   └─> Worker N

   Use Case: Load distribution, parallel processing

4. PAIR (Exclusive Pair)
   Peer A <─[PAIR]─> Peer B

   Use Case: Inter-thread communication, gateway connections

C++ IMPLEMENTATION:

// PUB-SUB Example
#include <zmq.hpp>
#include <string>
#include <chrono>

// Publisher
class MarketDataPublisher {
    zmq::context_t ctx_;
    zmq::socket_t socket_;

public:
    MarketDataPublisher() : ctx_(1), socket_(ctx_, zmq::socket_type::pub) {
        socket_.bind("ipc:///tmp/market_data.ipc");

        // Configure socket options
        int sndhwm = 10000;  // High water mark
        socket_.set(zmq::sockopt::sndhwm, sndhwm);

        int linger = 0;  // Don't wait on close
        socket_.set(zmq::sockopt::linger, linger);
    }

    void publish(const std::string& symbol, const MarketData& data) {
        auto start = std::chrono::high_resolution_clock::now();

        // Topic: MARKET.<SYMBOL>
        std::string topic = "MARKET." + symbol;

        // Send topic (multi-part message)
        zmq::message_t topic_msg(topic.data(), topic.size());
        socket_.send(topic_msg, zmq::send_flags::sndmore);

        // Send data
        std::string serialized = data.SerializeAsString();
        zmq::message_t data_msg(serialized.data(), serialized.size());
        socket_.send(data_msg, zmq::send_flags::none);

        auto end = std::chrono::high_resolution_clock::now();
        auto latency = std::chrono::duration_cast<std::chrono::microseconds>(
            end - start).count();

        // Typical latency: 3-8 microseconds
    }
};

// Subscriber
class MarketDataSubscriber {
    zmq::context_t ctx_;
    zmq::socket_t socket_;

public:
    MarketDataSubscriber(const std::vector<std::string>& symbols)
        : ctx_(1), socket_(ctx_, zmq::socket_type::sub) {

        socket_.connect("ipc:///tmp/market_data.ipc");

        // Subscribe to topics
        for (const auto& symbol : symbols) {
            std::string topic = "MARKET." + symbol;
            socket_.set(zmq::sockopt::subscribe, topic);
        }

        // Receive buffer
        int rcvhwm = 10000;
        socket_.set(zmq::sockopt::rcvhwm, rcvhwm);
    }

    MarketData receive() {
        // Receive topic
        zmq::message_t topic;
        auto res = socket_.recv(topic, zmq::recv_flags::none);

        // Receive data
        zmq::message_t data;
        socket_.recv(data, zmq::recv_flags::none);

        // Deserialize
        MarketData md;
        md.ParseFromArray(data.data(), data.size());

        return md;
    }

    // Non-blocking receive
    std::optional<MarketData> tryReceive() {
        zmq::message_t topic;
        auto res = socket_.recv(topic, zmq::recv_flags::dontwait);

        if (!res) {
            return std::nullopt;
        }

        zmq::message_t data;
        socket_.recv(data, zmq::recv_flags::none);

        MarketData md;
        md.ParseFromArray(data.data(), data.size());

        return md;
    }
};

TRANSPORT OPTIONS:

IPC (Inter-Process Communication):
- Endpoint: ipc:///tmp/socket_name.ipc
- Latency: 5-15 microseconds
- Same-host only
- Best for HFT

TCP:
- Endpoint: tcp://localhost:5555
- Latency: 30-100 microseconds (localhost)
- Cross-host capable
- Good for distributed systems

Inproc (In-Process):
- Endpoint: inproc://socket_name
- Latency: 1-3 microseconds
- Same-process only
- Best for thread communication

PGM/EPGM (Multicast):
- Endpoint: epgm://eth0;239.192.1.1:5555
- Latency: 10-50 microseconds
- One-to-many efficient
- Requires multicast support

PROS:
+ Ultra-low latency (microseconds)
+ Zero-copy message passing
+ No broker overhead
+ Simple deployment (library)
+ Multiple patterns
+ Flexible transport options
+ Good documentation

CONS:
- No built-in persistence
- No message replay
- Manual reliability handling
- No central management
- Limited monitoring tools
- No guaranteed delivery (PUB-SUB)

IDEAL FOR:
✓ Low-latency trading operations
✓ Market data distribution
✓ Inter-service communication (same host)
✓ Real-time event streams

NOT IDEAL FOR:
✗ Audit logging (needs persistence)
✗ Complex routing logic
✗ Message replay requirements

================================================================================
3. RABBITMQ - ENTERPRISE MESSAGE BROKER
================================================================================

OVERVIEW:

RabbitMQ is a robust, enterprise-grade message broker implementing AMQP
(Advanced Message Queuing Protocol). It provides sophisticated routing,
delivery guarantees, and management features.

Key Characteristics:
- Full-featured message broker
- AMQP 0.9.1 protocol
- Persistent and durable queues
- Complex routing via exchanges
- Management UI and monitoring
- Latency: 0.5-5 milliseconds

ARCHITECTURE:

┌─────────┐         ┌──────────────────┐         ┌──────────┐
│Producer │────────>│   RabbitMQ       │────────>│ Consumer │
└─────────┘         │                  │         └──────────┘
                    │  ┌───────────┐   │
                    │  │ Exchange  │   │
                    │  └─────┬─────┘   │
                    │        │         │
                    │  ┌─────▼─────┐   │
                    │  │   Queue   │   │
                    │  └───────────┘   │
                    └──────────────────┘

EXCHANGE TYPES:

1. Direct Exchange
   Routing key must match exactly
   ┌────────┐  key=orders  ┌──────────┐
   │Producer│─────────────>│Queue:orders│
   └────────┘              └──────────┘

2. Topic Exchange
   Pattern matching with wildcards
   ┌────────┐  MARKET.AAPL.*  ┌─────────────┐
   │Producer│─────────────────>│Queue:AAPL   │
   └────────┘                  └─────────────┘

3. Fanout Exchange
   Broadcast to all queues
   ┌────────┐              ┌──────────┐
   │Producer│─────────────>│Queue 1   │
   └────────┘       └──────>│Queue 2   │
                            └──────────┘

4. Headers Exchange
   Route based on message headers

C++ IMPLEMENTATION:

#include <amqpcpp.h>
#include <amqpcpp/libevent.h>

// Publisher
class RabbitMQPublisher {
    event_base* evbase_;
    AMQP::LibEventHandler handler_;
    AMQP::TcpConnection connection_;
    AMQP::TcpChannel channel_;

public:
    RabbitMQPublisher(const std::string& address)
        : evbase_(event_base_new())
        , handler_(evbase_)
        , connection_(&handler_, AMQP::Address(address))
        , channel_(&connection_) {

        // Declare exchange
        channel_.declareExchange("market_data", AMQP::topic, AMQP::durable);

        // Set confirm mode for reliability
        channel_.confirmSelect()
            .onAck([](uint64_t deliveryTag, bool multiple) {
                // Message confirmed
            })
            .onNack([](uint64_t deliveryTag, bool multiple) {
                // Message rejected - handle error
            });
    }

    void publish(const std::string& routingKey, const MarketData& data) {
        std::string serialized = data.SerializeAsString();

        // Publish with properties
        AMQP::Envelope envelope(serialized.data(), serialized.size());
        envelope.setContentType("application/protobuf");
        envelope.setDeliveryMode(2);  // Persistent
        envelope.setTimestamp(getCurrentTimestamp());

        channel_.publish("market_data", routingKey, envelope);
    }
};

// Consumer
class RabbitMQConsumer {
    event_base* evbase_;
    AMQP::LibEventHandler handler_;
    AMQP::TcpConnection connection_;
    AMQP::TcpChannel channel_;

public:
    RabbitMQConsumer(const std::string& address,
                    const std::string& queue_name)
        : evbase_(event_base_new())
        , handler_(evbase_)
        , connection_(&handler_, AMQP::Address(address))
        , channel_(&connection_) {

        // Declare queue
        channel_.declareQueue(queue_name, AMQP::durable);

        // Bind to exchange
        channel_.bindQueue("market_data", queue_name, "MARKET.#");

        // Set QoS (prefetch)
        channel_.setQos(100);  // Prefetch 100 messages
    }

    void consume(std::function<void(const MarketData&)> callback) {
        channel_.consume(queue_name)
            .onReceived([callback](const AMQP::Message& message,
                                  uint64_t deliveryTag,
                                  bool redelivered) {

                // Deserialize
                MarketData md;
                md.ParseFromArray(message.body(), message.bodySize());

                // Process
                callback(md);

                // Acknowledge
                channel_.ack(deliveryTag);
            });

        // Start event loop
        event_base_dispatch(evbase_);
    }
};

CONFIGURATION:

# rabbitmq.conf
# Optimize for low latency

# TCP configuration
tcp_listen_options.backlog = 4096
tcp_listen_options.nodelay = true
tcp_listen_options.sndbuf = 196608
tcp_listen_options.recbuf = 196608

# Memory
vm_memory_high_watermark.relative = 0.6
vm_memory_high_watermark_paging_ratio = 0.75

# Disk
disk_free_limit.absolute = 50GB

# Performance
channel_max = 2048
frame_max = 131072
heartbeat = 60

# Clustering (for HA)
cluster_partition_handling = autoheal

PROS:
+ Message persistence and durability
+ Complex routing capabilities
+ Delivery guarantees (at-least-once, exactly-once)
+ Dead letter queues
+ Management UI
+ Good monitoring tools
+ Clustering for HA
+ Rich client libraries

CONS:
- Higher latency (milliseconds)
- Broker overhead
- More complex deployment
- Resource intensive
- Not suitable for microsecond latency

IDEAL FOR:
✓ Audit trails and compliance logging
✓ Order confirmations (non-critical path)
✓ System events and alerts
✓ Workflow orchestration
✓ Cross-datacenter communication

NOT IDEAL FOR:
✗ Market data distribution (too slow)
✗ Critical path trading operations
✗ Ultra-low latency requirements

================================================================================
4. APACHE KAFKA - DISTRIBUTED STREAMING PLATFORM
================================================================================

OVERVIEW:

Apache Kafka is a distributed streaming platform designed for high-throughput,
fault-tolerant, persistent messaging. It excels at handling large volumes of
data with replay capabilities.

Key Characteristics:
- Distributed commit log
- Persistent storage (disk-based)
- High throughput (millions msg/sec)
- Horizontal scalability
- Message replay capability
- Latency: 1-10 milliseconds

ARCHITECTURE:

┌──────────┐         ┌────────────────────────────────┐
│ Producer │────────>│         Kafka Cluster          │
└──────────┘         │  ┌──────────┐  ┌──────────┐   │
                     │  │ Broker 1 │  │ Broker 2 │   │
                     │  └──────────┘  └──────────┘   │
                     │                                │
                     │  Topic: market_data            │
                     │  ┌───────┬───────┬───────┐    │
                     │  │Part 0 │Part 1 │Part 2 │    │
                     │  └───────┴───────┴───────┘    │
                     └────────────────┬───────────────┘
                                      │
                               ┌──────┴──────┐
                               │             │
                         ┌─────▼────┐  ┌─────▼────┐
                         │Consumer 1│  │Consumer 2│
                         └──────────┘  └──────────┘

TOPICS AND PARTITIONS:

Topic: market_data
├── Partition 0: AAPL messages
├── Partition 1: GOOGL messages
└── Partition 2: MSFT messages

Benefits:
- Parallel consumption
- Ordered within partition
- Scalable throughput

C++ IMPLEMENTATION:

#include <librdkafka/rdkafkacpp.h>

// Producer
class KafkaProducer {
    std::unique_ptr<RdKafka::Conf> conf_;
    std::unique_ptr<RdKafka::Producer> producer_;

public:
    KafkaProducer(const std::string& brokers) {
        conf_ = std::unique_ptr<RdKafka::Conf>(
            RdKafka::Conf::create(RdKafka::Conf::CONF_GLOBAL));

        // Configuration
        std::string errstr;
        conf_->set("bootstrap.servers", brokers, errstr);
        conf_->set("compression.codec", "lz4", errstr);
        conf_->set("batch.num.messages", "1000", errstr);
        conf_->set("linger.ms", "5", errstr);  // Batch for 5ms
        conf_->set("acks", "1", errstr);  // Leader acknowledgment

        // Create producer
        producer_ = std::unique_ptr<RdKafka::Producer>(
            RdKafka::Producer::create(conf_.get(), errstr));
    }

    void produce(const std::string& topic,
                const std::string& key,
                const MarketData& data) {

        std::string serialized = data.SerializeAsString();

        RdKafka::ErrorCode err = producer_->produce(
            topic,
            RdKafka::Topic::PARTITION_UA,  // Automatic partitioning
            RdKafka::Producer::RK_MSG_COPY,
            const_cast<char*>(serialized.data()), serialized.size(),
            &key, nullptr);

        if (err != RdKafka::ERR_NO_ERROR) {
            std::cerr << "Failed to produce: "
                     << RdKafka::err2str(err) << std::endl;
        }

        // Poll to handle delivery reports
        producer_->poll(0);
    }

    void flush() {
        producer_->flush(10000);  // 10 second timeout
    }
};

// Consumer
class KafkaConsumer {
    std::unique_ptr<RdKafka::Conf> conf_;
    std::unique_ptr<RdKafka::KafkaConsumer> consumer_;

public:
    KafkaConsumer(const std::string& brokers,
                 const std::string& group_id,
                 const std::vector<std::string>& topics) {

        conf_ = std::unique_ptr<RdKafka::Conf>(
            RdKafka::Conf::create(RdKafka::Conf::CONF_GLOBAL));

        std::string errstr;
        conf_->set("bootstrap.servers", brokers, errstr);
        conf_->set("group.id", group_id, errstr);
        conf_->set("auto.offset.reset", "earliest", errstr);
        conf_->set("enable.auto.commit", "false", errstr);

        // Create consumer
        consumer_ = std::unique_ptr<RdKafka::KafkaConsumer>(
            RdKafka::KafkaConsumer::create(conf_.get(), errstr));

        // Subscribe to topics
        consumer_->subscribe(topics);
    }

    void consume(std::function<void(const MarketData&)> callback) {
        while (true) {
            RdKafka::Message* msg = consumer_->consume(1000);  // 1s timeout

            if (msg->err() == RdKafka::ERR_NO_ERROR) {
                // Deserialize
                MarketData md;
                md.ParseFromArray(msg->payload(), msg->len());

                // Process
                callback(md);

                // Commit offset
                consumer_->commitSync(msg);
            }

            delete msg;
        }
    }

    // Consume from specific offset (replay capability)
    void consumeFrom(int64_t offset) {
        RdKafka::TopicPartition* tp = RdKafka::TopicPartition::create(
            "market_data", 0, offset);

        std::vector<RdKafka::TopicPartition*> partitions = {tp};
        consumer_->assign(partitions);
    }
};

CONFIGURATION:

# server.properties (Kafka Broker)

# Network
num.network.threads=8
num.io.threads=16
socket.send.buffer.bytes=102400
socket.receive.buffer.bytes=102400
socket.request.max.bytes=104857600

# Log (storage)
log.dirs=/var/kafka-logs
num.partitions=10
log.retention.hours=168
log.segment.bytes=1073741824

# Replication
default.replication.factor=3
min.insync.replicas=2

# Performance
compression.type=lz4
log.flush.interval.messages=10000
log.flush.interval.ms=1000

PROS:
+ High throughput (millions msg/sec)
+ Message replay capability
+ Persistent storage
+ Horizontal scalability
+ Strong durability guarantees
+ Time-based retention
+ Stream processing integration
+ Good monitoring tools

CONS:
- Higher latency (milliseconds)
- Complex deployment
- Resource intensive (disk, memory)
- Operational overhead
- Not suitable for microsecond latency
- Requires Zookeeper (until KRaft)

IDEAL FOR:
✓ Historical data storage
✓ Audit trails
✓ Event sourcing
✓ Stream processing pipelines
✓ Analytics and reporting
✓ Cross-datacenter replication
✓ Order history and replay

NOT IDEAL FOR:
✗ Ultra-low latency trading
✗ Critical path operations
✗ Simple pub-sub requirements

================================================================================
5. PERFORMANCE COMPARISON
================================================================================

LATENCY COMPARISON (50th Percentile):
┌──────────────┬──────────┬──────────┬──────────┬──────────┐
│ Message Size │ ZeroMQ   │ RabbitMQ │  Kafka   │  Redis   │
│              │  (IPC)   │  (TCP)   │  (TCP)   │ (Pub/Sub)│
├──────────────┼──────────┼──────────┼──────────┼──────────┤
│ 100 bytes    │   8 μs   │  800 μs  │  2 ms    │  120 μs  │
│ 1 KB         │  12 μs   │  1.2 ms  │  3 ms    │  180 μs  │
│ 10 KB        │  35 μs   │  2.5 ms  │  5 ms    │  450 μs  │
│ 100 KB       │ 180 μs   │   12 ms  │  15 ms   │  2.5 ms  │
└──────────────┴──────────┴──────────┴──────────┴──────────┘

LATENCY COMPARISON (99th Percentile):
┌──────────────┬──────────┬──────────┬──────────┬──────────┐
│ Message Size │ ZeroMQ   │ RabbitMQ │  Kafka   │  Redis   │
├──────────────┼──────────┼──────────┼──────────┼──────────┤
│ 100 bytes    │  25 μs   │  2 ms    │  8 ms    │  500 μs  │
│ 1 KB         │  40 μs   │  3 ms    │  12 ms   │  800 μs  │
│ 10 KB        │ 120 μs   │  8 ms    │  25 ms   │  2 ms    │
│ 100 KB       │ 650 μs   │  40 ms   │  60 ms   │  10 ms   │
└──────────────┴──────────┴──────────┴──────────┴──────────┘

THROUGHPUT COMPARISON:
┌──────────────┬─────────────┬─────────────┬─────────────┐
│ Technology   │ Single Pub  │ Single Sub  │  Total      │
├──────────────┼─────────────┼─────────────┼─────────────┤
│ ZeroMQ (IPC) │ 3M msg/s    │ 2M msg/s    │ 2M msg/s    │
│ RabbitMQ     │ 20K msg/s   │ 20K msg/s   │ 20K msg/s   │
│ Kafka        │ 1M msg/s    │ 500K msg/s  │ 500K msg/s  │
│ Redis        │ 200K msg/s  │ 150K msg/s  │ 150K msg/s  │
└──────────────┴─────────────┴─────────────┴─────────────┘

RESOURCE USAGE (Idle):
┌──────────────┬─────────┬─────────┬─────────────┐
│ Technology   │  CPU    │ Memory  │ Disk I/O    │
├──────────────┼─────────┼─────────┼─────────────┤
│ ZeroMQ       │  < 1%   │  10 MB  │ None        │
│ RabbitMQ     │  5-10%  │ 200 MB  │ Low         │
│ Kafka        │ 10-20%  │ 1-2 GB  │ Medium      │
│ Redis        │  2-5%   │ 100 MB  │ Low (AOF)   │
└──────────────┴─────────┴─────────┴─────────────┘

FEATURE COMPARISON:
┌─────────────────────┬─────────┬──────────┬────────┬────────┐
│ Feature             │ ZeroMQ  │ RabbitMQ │ Kafka  │ Redis  │
├─────────────────────┼─────────┼──────────┼────────┼────────┤
│ Latency (μs)        │   8     │   800    │ 2000   │  120   │
│ Throughput (msg/s)  │  2M     │   20K    │  1M    │  200K  │
│ Persistence         │   No    │   Yes    │  Yes   │  Opt   │
│ Replay              │   No    │   No     │  Yes   │  No    │
│ Delivery Guarantee  │  Best   │   Yes    │  Yes   │  Best  │
│ Clustering          │   No    │   Yes    │  Yes   │  Yes   │
│ Operational Complex │  Low    │ Medium   │  High  │  Low   │
│ Learning Curve      │  Low    │ Medium   │  High  │  Low   │
└─────────────────────┴─────────┴──────────┴────────┴────────┘

================================================================================
6. USE CASE ANALYSIS
================================================================================

USE CASE 1: REAL-TIME MARKET DATA DISTRIBUTION
──────────────────────────────────────────────

Requirement:
- Latency: < 20 microseconds
- Throughput: 1-5M updates/second
- Fan-out: 10-100 subscribers
- No persistence needed
- Same host deployment

RECOMMENDATION: ZeroMQ (IPC, PUB-SUB)

Architecture:
┌───────────────┐
│ Market Data   │
│   Gateway     │
└───────┬───────┘
        │ ZeroMQ PUB (IPC)
        │ ipc:///tmp/market_data.ipc
        │
        ├──────────────┬──────────────┬──────────────┐
        │              │              │              │
  ┌─────▼─────┐  ┌────▼─────┐  ┌────▼─────┐  ┌────▼─────┐
  │Strategy A │  │Strategy B│  │Risk Mgr  │  │Logger    │
  └───────────┘  └──────────┘  └──────────┘  └──────────┘

Implementation:
// Publisher in Market Data Gateway
zmq::context_t ctx(1);
zmq::socket_t publisher(ctx, zmq::socket_type::pub);
publisher.bind("ipc:///tmp/market_data.ipc");

// Subscriber in Strategy
zmq::socket_t subscriber(ctx, zmq::socket_type::sub);
subscriber.connect("ipc:///tmp/market_data.ipc");
subscriber.set(zmq::sockopt::subscribe, "MARKET.AAPL");

USE CASE 2: ORDER AUDIT TRAIL
──────────────────────────────

Requirement:
- Latency: < 10 milliseconds (acceptable)
- Throughput: 10K-100K orders/second
- Persistence: Required (regulatory)
- Replay: Required (forensics)
- Retention: 7 years

RECOMMENDATION: Apache Kafka

Architecture:
┌──────────┐         ┌─────────────────┐
│   OMS    │────────>│  Kafka Topic:   │
└──────────┘         │  order_audit    │
                     │  (Partitioned)  │
                     └────────┬────────┘
                              │
                     ┌────────┴─────────┐
                     │                  │
              ┌──────▼──────┐    ┌─────▼──────┐
              │ Compliance  │    │ Analytics  │
              │   Service   │    │  Service   │
              └─────────────┘    └────────────┘

Configuration:
# Long retention for compliance
log.retention.hours=61320  # 7 years
log.retention.bytes=-1     # Unlimited
min.insync.replicas=2      # Durability
acks=all                   # All replicas

USE CASE 3: ORDER LIFECYCLE EVENTS
───────────────────────────────────

Requirement:
- Latency: < 1 millisecond
- Throughput: 50K-500K events/second
- Routing: Complex (order status, fills, cancels)
- Delivery: At-least-once
- No replay needed

RECOMMENDATION: RabbitMQ with Topic Exchange

Architecture:
┌──────────┐         ┌──────────────────┐
│   OMS    │────────>│  RabbitMQ        │
└──────────┘         │  Topic Exchange  │
                     │  "order_events"  │
                     └────────┬─────────┘
                              │
         ┌────────────────────┼────────────────────┐
         │                    │                    │
    order.new.*          order.fill.*        order.cancel.*
         │                    │                    │
   ┌─────▼─────┐        ┌────▼─────┐        ┌────▼──────┐
   │ Position  │        │   PnL    │        │ Reporting │
   │  Service  │        │ Service  │        │  Service  │
   └───────────┘        └──────────┘        └───────────┘

Routing Keys:
- order.new.equity.AAPL
- order.fill.equity.AAPL
- order.cancel.equity.AAPL

USE CASE 4: SYSTEM ALERTS & NOTIFICATIONS
──────────────────────────────────────────

Requirement:
- Latency: < 100 milliseconds (acceptable)
- Throughput: 1K-10K events/second
- Fan-out: Many subscribers
- Persistence: Optional
- Simple pub-sub

RECOMMENDATION: Redis Pub/Sub

Architecture:
┌────────────┐
│  Services  │
│ (Multiple) │
└──────┬─────┘
       │ Redis PUBLISH
       │
    ┌──▼────────────┐
    │  Redis        │
    │  Pub/Sub      │
    └──┬────────────┘
       │
       ├────────────┬────────────┬────────────┐
       │            │            │            │
   ┌───▼───┐   ┌───▼───┐   ┌───▼───┐   ┌───▼───┐
   │ Slack │   │ Email │   │ SMS   │   │ UI    │
   └───────┘   └───────┘   └───────┘   └───────┘

Implementation:
// Publisher
redis_client.publish("alerts:critical", alert_message);

// Subscriber
redis_client.subscribe("alerts:*", [](const std::string& msg) {
    handleAlert(msg);
});

USE CASE 5: CROSS-DATACENTER REPLICATION
─────────────────────────────────────────

Requirement:
- Latency: < 100 milliseconds (WAN)
- Throughput: 100K-1M events/second
- Durability: High
- Replay: Required
- Geographic distribution

RECOMMENDATION: Apache Kafka with MirrorMaker

Architecture:
Datacenter 1 (Primary):
┌──────────────┐
│ Kafka Cluster│
│  (Primary)   │
└──────┬───────┘
       │ MirrorMaker 2.0
       │ (Replication)
       │
Datacenter 2 (DR):
┌──────▼───────┐
│ Kafka Cluster│
│  (Replica)   │
└──────────────┘

Configuration:
# MirrorMaker 2.0
clusters=primary,dr
primary.bootstrap.servers=dc1-kafka:9092
dr.bootstrap.servers=dc2-kafka:9092
primary->dr.enabled=true
replication.factor=3

================================================================================
7. LATENCY BENCHMARKS
================================================================================

ZEROMQ LATENCY BENCHMARK:

Test Setup:
- Hardware: Intel Xeon E5-2680 v4
- OS: Ubuntu 22.04 RT kernel
- Transport: IPC
- Message Size: 100 bytes
- Pattern: REQ-REP, PUB-SUB
- Iterations: 1,000,000

Results (microseconds):
┌──────────┬────────┬────────┬────────┬────────┬────────┐
│ Pattern  │  Min   │  P50   │  P99   │ P99.9  │  Max   │
├──────────┼────────┼────────┼────────┼────────┼────────┤
│ REQ-REP  │  5.2   │  8.1   │  18.3  │  42.7  │ 128.5  │
│ PUB-SUB  │  2.8   │  5.6   │  12.4  │  28.9  │  86.3  │
│ PUSH-PULL│  2.1   │  4.3   │   9.8  │  21.7  │  67.2  │
└──────────┴────────┴────────┴────────┴────────┴────────┘

RABBITMQ LATENCY BENCHMARK:

Test Setup:
- Same hardware
- Transport: TCP localhost
- Message Size: 100 bytes
- Exchange: Direct
- Persistence: Off

Results (milliseconds):
┌──────────┬────────┬────────┬────────┬────────┬────────┐
│ Pattern  │  Min   │  P50   │  P99   │ P99.9  │  Max   │
├──────────┼────────┼────────┼────────┼────────┼────────┤
│ No Conf  │  0.4   │  0.8   │  2.1   │  5.3   │  28.7  │
│ With Conf│  0.6   │  1.2   │  3.4   │  8.9   │  45.2  │
│ Persist  │  1.8   │  3.5   │  9.2   │  24.1  │ 102.3  │
└──────────┴────────┴────────┴────────┴────────┴────────┘

KAFKA LATENCY BENCHMARK:

Test Setup:
- Same hardware
- Partitions: 10
- Replication Factor: 1 (local)
- Message Size: 100 bytes
- Batch Size: 16KB

Results (milliseconds):
┌──────────┬────────┬────────┬────────┬────────┬────────┐
│ Config   │  Min   │  P50   │  P99   │ P99.9  │  Max   │
├──────────┼────────┼────────┼────────┼────────┼────────┤
│ acks=0   │  0.8   │  1.5   │  4.2   │  12.8  │  67.3  │
│ acks=1   │  1.2   │  2.3   │  6.8   │  18.9  │  98.7  │
│ acks=all │  1.8   │  3.7   │  11.2  │  32.4  │ 156.2  │
└──────────┴────────┴────────┴────────┴────────┴────────┘

C++ Benchmarking Code:

#include <chrono>
#include <vector>
#include <algorithm>

class LatencyBenchmark {
    std::vector<int64_t> latencies_;

public:
    void recordLatency(int64_t latency_ns) {
        latencies_.push_back(latency_ns);
    }

    void printStats() {
        if (latencies_.empty()) return;

        std::sort(latencies_.begin(), latencies_.end());

        size_t count = latencies_.size();
        int64_t min = latencies_.front();
        int64_t max = latencies_.back();
        int64_t p50 = latencies_[count * 0.50];
        int64_t p99 = latencies_[count * 0.99];
        int64_t p999 = latencies_[count * 0.999];

        std::cout << "Latency Statistics (microseconds):\n";
        std::cout << "  Min:   " << min / 1000.0 << "\n";
        std::cout << "  P50:   " << p50 / 1000.0 << "\n";
        std::cout << "  P99:   " << p99 / 1000.0 << "\n";
        std::cout << "  P99.9: " << p999 / 1000.0 << "\n";
        std::cout << "  Max:   " << max / 1000.0 << "\n";
    }
};

// ZeroMQ Benchmark
void benchmarkZeroMQ() {
    zmq::context_t ctx(1);

    zmq::socket_t req(ctx, zmq::socket_type::req);
    req.connect("ipc:///tmp/bench.ipc");

    zmq::socket_t rep(ctx, zmq::socket_type::rep);
    rep.bind("ipc:///tmp/bench.ipc");

    LatencyBenchmark bench;
    std::vector<char> data(100);

    for (int i = 0; i < 1000000; ++i) {
        auto start = std::chrono::high_resolution_clock::now();

        // Send request
        zmq::message_t request(data.data(), data.size());
        req.send(request, zmq::send_flags::none);

        // Receive reply
        zmq::message_t reply;
        req.recv(reply, zmq::recv_flags::none);

        auto end = std::chrono::high_resolution_clock::now();
        auto latency = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();

        bench.recordLatency(latency);
    }

    bench.printStats();
}

================================================================================
8. THROUGHPUT BENCHMARKS
================================================================================

ZEROMQ THROUGHPUT BENCHMARK:

Test Setup:
- Pattern: PUB-SUB
- Transport: IPC
- Message Size: 1KB
- Publishers: 1
- Subscribers: 1, 10, 100

Results:
┌─────────────┬────────────────┬───────────────┐
│ Subscribers │ Throughput     │ CPU Usage     │
├─────────────┼────────────────┼───────────────┤
│      1      │ 2.8M msg/s     │ 180%          │
│     10      │ 2.5M msg/s     │ 320%          │
│    100      │ 1.2M msg/s     │ 850%          │
└─────────────┴────────────────┴───────────────┘

KAFKA THROUGHPUT BENCHMARK:

Test Setup:
- Partitions: 10
- Replication: 3
- Message Size: 1KB
- Batch Size: 100KB
- Compression: LZ4

Results:
┌─────────────┬────────────────┬───────────────┐
│ Config      │ Throughput     │ Disk I/O      │
├─────────────┼────────────────┼───────────────┤
│ acks=0      │ 1.8M msg/s     │ 1.5 GB/s      │
│ acks=1      │ 950K msg/s     │ 800 MB/s      │
│ acks=all    │ 420K msg/s     │ 350 MB/s      │
└─────────────┴────────────────┴───────────────┘

================================================================================
9. IMPLEMENTATION EXAMPLES
================================================================================

See previous sections for detailed C++ implementations of:
- ZeroMQ PUB-SUB, REQ-REP
- RabbitMQ Producer/Consumer
- Kafka Producer/Consumer

================================================================================
10. HYBRID ARCHITECTURE DESIGN
================================================================================

RECOMMENDED ARCHITECTURE FOR HFT:

Critical Path (Ultra-Low Latency):
┌─────────────┐
│ Market Data │
│   Gateway   │
└──────┬──────┘
       │ ZeroMQ IPC (5-15 μs)
       │
┌──────▼──────┐
│  Strategy   │
│   Engine    │
└──────┬──────┘
       │ ZeroMQ IPC (5-15 μs)
       │
┌──────▼──────┐
│   Order     │
│ Management  │
└──────┬──────┘
       │ ZeroMQ IPC (5-15 μs)
       │
┌──────▼──────┐
│ Execution   │
└─────────────┘

Non-Critical Path (Milliseconds OK):
┌─────────────┐
│   Order     │
│ Management  │
└──────┬──────┘
       │ RabbitMQ (1-5 ms)
       │
       ├──────────────┬──────────────┐
       │              │              │
┌──────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐
│   Position  │ │   PnL    │ │  Reporting  │
│   Service   │ │ Service  │ │   Service   │
└─────────────┘ └──────────┘ └─────────────┘

Audit Trail (Persistence):
┌─────────────┐
│   All       │
│  Services   │
└──────┬──────┘
       │ Kafka (2-10 ms)
       │
┌──────▼──────────────┐
│  Kafka Topic:       │
│  order_audit        │
│  (7-year retention) │
└─────────────────────┘

Alerts & Monitoring:
┌─────────────┐
│  Monitoring │
│   Service   │
└──────┬──────┘
       │ Redis Pub/Sub (100-500 μs)
       │
       ├──────────────┬──────────────┐
       │              │              │
┌──────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐
│  Dashboard  │ │   Slack  │ │  PagerDuty  │
└─────────────┘ └──────────┘ └─────────────┘

================================================================================
11. SELECTION CRITERIA
================================================================================

CHOOSE ZEROMQ IF:
✓ Ultra-low latency required (< 50 μs)
✓ Same-host communication
✓ No persistence needed
✓ Simple deployment preferred
✓ Direct control over messaging

CHOOSE RABBITMQ IF:
✓ Complex routing required
✓ Delivery guarantees needed
✓ Management UI desired
✓ Multiple protocols required
✓ Latency < 5 ms acceptable

CHOOSE KAFKA IF:
✓ Message replay required
✓ High throughput needed (> 100K msg/s)
✓ Long retention required
✓ Stream processing planned
✓ Latency < 10 ms acceptable

CHOOSE REDIS PUB/SUB IF:
✓ Simple pub/sub needed
✓ Redis already deployed
✓ Latency < 500 μs acceptable
✓ No persistence required
✓ Simple alerts/notifications

================================================================================
12. BEST PRACTICES
================================================================================

ZEROMQ:
✓ Use IPC for same-host, TCP for cross-host
✓ Set appropriate high water marks
✓ Handle partial messages correctly
✓ Use inproc for inter-thread communication
✓ Monitor socket states

RABBITMQ:
✓ Use publisher confirms for critical messages
✓ Set appropriate prefetch counts
✓ Use lazy queues for large backlogs
✓ Monitor queue depths
✓ Enable clustering for HA

KAFKA:
✓ Partition appropriately for parallelism
✓ Tune batch size and linger time
✓ Use appropriate compression
✓ Monitor consumer lag
✓ Plan retention policies

GENERAL:
✓ Measure latency percentiles, not averages
✓ Test under realistic load
✓ Plan for failure scenarios
✓ Monitor message rates
✓ Document message schemas

================================================================================
END OF MESSAGE QUEUE COMPARISON
================================================================================
