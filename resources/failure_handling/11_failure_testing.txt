================================================================================
                       FAILURE TESTING FRAMEWORK
               Chaos Engineering & Disaster Recovery Testing
================================================================================

DOCUMENT: Comprehensive Failure Testing
VERSION: 2.1.0
LAST UPDATED: 2025-11-26
SCOPE: Testing failure scenarios and recovery procedures

================================================================================
                          TABLE OF CONTENTS
================================================================================

1. Testing Philosophy
2. Chaos Engineering Principles
3. Fault Injection Framework
4. Automated Failure Testing
5. Disaster Recovery Drills
6. Production Testing
7. Test Scenarios
8. Metrics & Validation
9. Code Examples

================================================================================
                    1. TESTING PHILOSOPHY
================================================================================

CORE BELIEF:
------------
"If you haven't tested recovery from a failure,
 you haven't tested the system"

TESTING PRINCIPLES:
-------------------
1. TEST IN PRODUCTION
   - Staging doesn't replicate all conditions
   - Controlled production testing builds confidence
   - Start small, increase gradually

2. AUTOMATE EVERYTHING
   - Manual tests are inconsistent
   - Automation enables frequent testing
   - Regression testing for fixes

3. MEASURE AND VALIDATE
   - Define success criteria
   - Measure recovery time
   - Validate system state post-recovery

4. CONTINUOUS TESTING
   - Regular scheduled tests
   - Random chaos injection
   - Drift detection

TESTING SCHEDULE:
-----------------
Daily: Automated unit/integration tests
Weekly: Component failure tests
Monthly: Full system failover drill
Quarterly: Disaster recovery drill
Continuous: Random chaos injection (production)

================================================================================
                  2. CHAOS ENGINEERING PRINCIPLES
================================================================================

CHAOS ENGINEERING DEFINITION:
-----------------------------
"The discipline of experimenting on a system to build confidence
 in the system's capability to withstand turbulent conditions"

CHAOS METHODOLOGY:
------------------
1. Define steady state (normal metrics)
2. Hypothesize steady state continues
3. Introduce variables (failures)
4. Disprove hypothesis (detect failures)

IMPLEMENTATION:

namespace hft::testing {

class ChaosEngine {
public:
    struct ChaosExperiment {
        std::string name;
        std::string description;
        std::function<void()> inject_failure;
        std::function<bool()> validate_steady_state;
        std::chrono::seconds duration;
        double blast_radius;  // 0.0 to 1.0
    };
    
    void run_experiment(const ChaosExperiment& experiment) {
        logger_.info("Starting chaos experiment", {
            {"name", experiment.name},
            {"duration_s", experiment.duration.count()},
            {"blast_radius", experiment.blast_radius}
        });
        
        // 1. Establish baseline
        if (!experiment.validate_steady_state()) {
            logger_.error("System not in steady state - aborting experiment");
            return;
        }
        
        auto baseline_metrics = collect_metrics();
        
        // 2. Inject failure
        auto start_time = std::chrono::steady_clock::now();
        
        try {
            experiment.inject_failure();
        } catch (const std::exception& e) {
            logger_.error("Failure injection failed", {
                {"error", e.what()}
            });
            return;
        }
        
        // 3. Monitor system during failure
        auto results = monitor_during_failure(
            experiment.duration,
            experiment.validate_steady_state
        );
        
        // 4. Validate recovery
        auto end_time = std::chrono::steady_clock::now();
        auto recovery_time = end_time - start_time;
        
        bool recovered = experiment.validate_steady_state();
        
        // 5. Collect and analyze results
        auto final_metrics = collect_metrics();
        
        ChaosResults chaos_results{
            .experiment_name = experiment.name,
            .started_at = start_time,
            .duration = recovery_time,
            .recovered = recovered,
            .baseline_metrics = baseline_metrics,
            .final_metrics = final_metrics,
            .observations = results
        };
        
        save_chaos_results(chaos_results);
        
        logger_.info("Chaos experiment completed", {
            {"name", experiment.name},
            {"recovered", recovered},
            {"recovery_time_s", std::chrono::duration_cast<
                std::chrono::seconds>(recovery_time).count()}
        });
    }
};

} // namespace hft::testing

EXAMPLE CHAOS EXPERIMENTS:

// Experiment 1: Network latency injection
ChaosExperiment latency_experiment{
    .name = "network_latency_injection",
    .description = "Inject 100ms latency to exchange connection",
    .inject_failure = []() {
        network_simulator_.add_latency("exchange", 100ms);
    },
    .validate_steady_state = []() {
        return trading_engine_.is_healthy() &&
               order_latency_.get_p99() < 200ms;
    },
    .duration = std::chrono::seconds(60),
    .blast_radius = 0.5  // Affect 50% of connections
};

// Experiment 2: Random pod kills (Kubernetes)
ChaosExperiment pod_kill_experiment{
    .name = "random_pod_kill",
    .description = "Randomly kill trading pods",
    .inject_failure = []() {
        k8s_client_.kill_random_pod("trading-system");
    },
    .validate_steady_state = []() {
        return k8s_client_.get_ready_pods("trading-system") >= 2;
    },
    .duration = std::chrono::seconds(120),
    .blast_radius = 0.33  // Kill 1 of 3 pods
};

================================================================================
                    3. FAULT INJECTION FRAMEWORK
================================================================================

FAULT TYPES:
------------
1. Resource failures (CPU, memory, disk, network)
2. Dependency failures (exchanges, databases, services)
3. Timing failures (latency, timeout, clock skew)
4. Data failures (corruption, missing, invalid)
5. State failures (partial updates, inconsistency)

IMPLEMENTATION:

class FaultInjector {
public:
    // Inject network partition
    void inject_network_partition(const std::string& service_a,
                                  const std::string& service_b,
                                  std::chrono::seconds duration) {
        logger_.info("Injecting network partition", {
            {"service_a", service_a},
            {"service_b", service_b},
            {"duration_s", duration.count()}
        });
        
        // Use iptables to drop packets
        std::string cmd = fmt::format(
            "sudo iptables -A INPUT -s {} -j DROP",
            get_service_ip(service_b)
        );
        
        system(cmd.c_str());
        
        // Schedule restoration
        timer_.schedule([service_b]() {
            restore_network(service_b);
        }, duration);
    }
    
    // Inject CPU pressure
    void inject_cpu_pressure(double target_usage,
                            std::chrono::seconds duration) {
        logger_.info("Injecting CPU pressure", {
            {"target_usage", target_usage},
            {"duration_s", duration.count()}
        });
        
        // Spawn CPU-intensive threads
        int num_threads = std::thread::hardware_concurrency();
        std::vector<std::thread> threads;
        
        std::atomic<bool> stop{false};
        
        for (int i = 0; i < num_threads; ++i) {
            threads.emplace_back([&stop, target_usage]() {
                while (!stop) {
                    // Busy loop with controlled intensity
                    auto start = std::chrono::steady_clock::now();
                    while (std::chrono::steady_clock::now() - start <
                           std::chrono::milliseconds(10) * target_usage) {
                        // Burn CPU
                        volatile int x = 0;
                        for (int j = 0; j < 10000; ++j) x++;
                    }
                    std::this_thread::sleep_for(
                        std::chrono::milliseconds(10) * (1.0 - target_usage)
                    );
                }
            });
        }
        
        // Schedule stop
        timer_.schedule([&stop, &threads]() {
            stop = true;
            for (auto& t : threads) {
                if (t.joinable()) t.join();
            }
        }, duration);
    }
    
    // Inject memory pressure
    void inject_memory_pressure(size_t bytes,
                               std::chrono::seconds duration) {
        logger_.info("Injecting memory pressure", {
            {"bytes", bytes},
            {"duration_s", duration.count()}
        });
        
        // Allocate memory
        std::vector<char> memory_hog(bytes);
        
        // Keep memory allocated
        std::this_thread::sleep_for(duration);
        
        // Memory automatically freed when vector goes out of scope
    }
    
    // Inject packet loss
    void inject_packet_loss(const std::string& interface,
                           double loss_rate,
                           std::chrono::seconds duration) {
        logger_.info("Injecting packet loss", {
            {"interface", interface},
            {"loss_rate", loss_rate},
            {"duration_s", duration.count()}
        });
        
        // Use tc (traffic control) to inject packet loss
        std::string cmd = fmt::format(
            "sudo tc qdisc add dev {} root netem loss {}%",
            interface,
            loss_rate * 100
        );
        
        system(cmd.c_str());
        
        // Schedule restoration
        timer_.schedule([interface]() {
            std::string restore_cmd = fmt::format(
                "sudo tc qdisc del dev {} root",
                interface
            );
            system(restore_cmd.c_str());
        }, duration);
    }
};

================================================================================
                    7. TEST SCENARIOS
================================================================================

COMPREHENSIVE TEST SUITE:

1. NETWORK FAILURES
   - [ ] Exchange disconnect
   - [ ] High latency (>100ms)
   - [ ] Packet loss (1%, 5%, 10%)
   - [ ] Network partition
   - [ ] DNS failure
   - [ ] SSL handshake failure

2. EXCHANGE FAILURES
   - [ ] API timeout
   - [ ] Rate limit exceeded
   - [ ] Invalid response
   - [ ] Market data gap
   - [ ] Trading halt
   - [ ] Order rejection

3. SYSTEM FAILURES
   - [ ] Out of memory
   - [ ] CPU overload
   - [ ] Disk full
   - [ ] Process crash
   - [ ] Thread deadlock
   - [ ] Hardware failure

4. DATA FAILURES
   - [ ] Corrupt market data
   - [ ] Missing sequence numbers
   - [ ] Stale data
   - [ ] Database corruption
   - [ ] Checkpoint failure

5. RECOVERY SCENARIOS
   - [ ] Process restart
   - [ ] Failover to backup
   - [ ] State restoration
   - [ ] Position reconciliation
   - [ ] Full system recovery

TEST EXECUTION:

class FailureTestSuite {
public:
    void run_all_tests() {
        logger_.info("Starting comprehensive failure test suite");
        
        std::vector<TestResult> results;
        
        // Network failure tests
        results.push_back(test_exchange_disconnect());
        results.push_back(test_high_latency());
        results.push_back(test_packet_loss());
        
        // Exchange failure tests
        results.push_back(test_api_timeout());
        results.push_back(test_rate_limit());
        
        // System failure tests
        results.push_back(test_memory_exhaustion());
        results.push_back(test_process_crash());
        
        // Generate report
        generate_test_report(results);
    }
    
private:
    TestResult test_exchange_disconnect() {
        TestResult result{.name = "Exchange Disconnect"};
        auto start = std::chrono::steady_clock::now();
        
        try {
            // Inject failure
            fault_injector_.disconnect_exchange("NASDAQ");
            
            // Wait for detection
            wait_for_circuit_breaker_open("NASDAQ", 1s);
            
            // Wait for recovery
            auto recovery_start = std::chrono::steady_clock::now();
            wait_for_reconnection("NASDAQ", 10s);
            auto recovery_time = std::chrono::steady_clock::now() -
                               recovery_start;
            
            // Validate
            if (is_system_healthy()) {
                result.passed = true;
                result.recovery_time = recovery_time;
            } else {
                result.passed = false;
                result.error = "System unhealthy after recovery";
            }
            
        } catch (const std::exception& e) {
            result.passed = false;
            result.error = e.what();
        }
        
        result.duration = std::chrono::steady_clock::now() - start;
        return result;
    }
};

================================================================================
                              END OF DOCUMENT
================================================================================

================================================================================
                   4. PRODUCTION CHAOS TESTING
================================================================================

class ProductionChaosManager {
private:
    bool chaos_enabled_{false};
    double blast_radius_{0.1};  // Affect 10% of traffic
    std::chrono::hours chaos_window_start_{9};   // 9 AM
    std::chrono::hours chaos_window_end_{17};    // 5 PM
    
public:
    void enable_controlled_chaos() {
        if (!is_chaos_window()) {
            logger_.warn("Outside chaos testing window");
            return;
        }
        
        if (!get_manual_approval()) {
            logger_.warn("Manual approval required for production chaos");
            return;
        }
        
        chaos_enabled_ = true;
        
        logger_.info("Production chaos testing enabled", {
            {"blast_radius", blast_radius_},
            {"duration", "until_disabled"}
        });
        
        // Schedule random chaos events
        schedule_random_chaos_events();
    }
    
private:
    bool is_chaos_window() {
        auto now = std::chrono::system_clock::now();
        auto time_t_now = std::chrono::system_clock::to_time_t(now);
        auto* tm_now = std::localtime(&time_t_now);
        
        int hour = tm_now->tm_hour;
        
        return hour >= chaos_window_start_.count() &&
               hour < chaos_window_end_.count();
    }
    
    void schedule_random_chaos_events() {
        std::thread([this]() {
            while (chaos_enabled_) {
                // Wait random interval (5-15 minutes)
                auto wait_time = std::chrono::minutes(
                    5 + (rand() % 11)
                );
                std::this_thread::sleep_for(wait_time);
                
                if (!chaos_enabled_) break;
                
                // Execute random chaos experiment
                execute_random_experiment();
            }
        }).detach();
    }
    
    void execute_random_experiment() {
        std::vector<std::function<void()>> experiments = {
            [this]() { inject_latency(); },
            [this]() { inject_error_response(); },
            [this]() { inject_timeout(); },
            [this]() { inject_cpu_spike(); }
        };
        
        // Pick random experiment
        auto experiment = experiments[rand() % experiments.size()];
        
        logger_.info("Executing random chaos experiment");
        
        try {
            experiment();
        } catch (const std::exception& e) {
            logger_.error("Chaos experiment failed", {
                {"error", e.what()}
            });
        }
    }
    
    void inject_latency() {
        if (should_affect_request()) {
            auto latency = std::chrono::milliseconds(50 + (rand() % 100));
            std::this_thread::sleep_for(latency);
            
            logger_.debug("Injected latency", {
                {"latency_ms", latency.count()}
            });
        }
    }
    
    bool should_affect_request() {
        double random = static_cast<double>(rand()) / RAND_MAX;
        return random < blast_radius_;
    }
};

================================================================================
                   8. DISASTER RECOVERY TESTING
================================================================================

class DisasterRecoveryTest {
public:
    struct DRTestResult {
        std::string test_name;
        bool success;
        std::chrono::milliseconds recovery_time;
        std::vector<std::string> issues_found;
        std::map<std::string, std::string> metrics;
    };
    
    DRTestResult test_datacenter_failover() {
        DRTestResult result{.test_name = "Datacenter Failover"};
        auto start = std::chrono::steady_clock::now();
        
        logger_.info("Starting datacenter failover test");
        
        try {
            // Step 1: Verify primary DC is healthy
            if (!verify_primary_dc_healthy()) {
                result.success = false;
                result.issues_found.push_back("Primary DC unhealthy before test");
                return result;
            }
            
            // Step 2: Simulate primary DC failure
            simulate_dc_failure("primary");
            
            // Step 3: Wait for failover detection
            auto failover_detected = wait_for_failover_detection(
                std::chrono::seconds(30)
            );
            
            if (!failover_detected) {
                result.issues_found.push_back("Failover not detected within 30s");
            }
            
            // Step 4: Verify secondary DC takes over
            if (!verify_secondary_dc_active()) {
                result.success = false;
                result.issues_found.push_back("Secondary DC failed to activate");
                return result;
            }
            
            // Step 5: Verify trading continues
            if (!verify_trading_operational()) {
                result.success = false;
                result.issues_found.push_back("Trading not operational after failover");
                return result;
            }
            
            // Step 6: Restore primary DC
            restore_dc("primary");
            
            // Step 7: Verify failback
            if (!verify_failback_to_primary()) {
                result.issues_found.push_back("Failback to primary failed");
            }
            
            result.success = true;
            
        } catch (const std::exception& e) {
            result.success = false;
            result.issues_found.push_back(e.what());
        }
        
        result.recovery_time = std::chrono::duration_cast<
            std::chrono::milliseconds>(
            std::chrono::steady_clock::now() - start
        );
        
        // Collect metrics
        result.metrics["positions_lost"] = "0";
        result.metrics["orders_lost"] = "0";
        result.metrics["data_loss_bytes"] = "0";
        result.metrics["downtime_ms"] = std::to_string(
            result.recovery_time.count()
        );
        
        return result;
    }
    
    DRTestResult test_full_system_restore() {
        DRTestResult result{.test_name = "Full System Restore from Backup"};
        auto start = std::chrono::steady_clock::now();
        
        logger_.info("Starting full system restore test");
        
        try {
            // Step 1: Create snapshot of current state
            auto snapshot = create_system_snapshot();
            
            // Step 2: Simulate catastrophic failure
            simulate_catastrophic_failure();
            
            // Step 3: Restore from backup
            restore_from_backup(snapshot);
            
            // Step 4: Verify all data restored
            if (!verify_data_integrity(snapshot)) {
                result.success = false;
                result.issues_found.push_back("Data integrity check failed");
                return result;
            }
            
            // Step 5: Verify all services operational
            if (!verify_all_services_healthy()) {
                result.success = false;
                result.issues_found.push_back("Services not healthy after restore");
                return result;
            }
            
            result.success = true;
            
        } catch (const std::exception& e) {
            result.success = false;
            result.issues_found.push_back(e.what());
        }
        
        result.recovery_time = std::chrono::duration_cast<
            std::chrono::milliseconds>(
            std::chrono::steady_clock::now() - start
        );
        
        return result;
    }
};

================================================================================
                   9. CONTINUOUS TESTING FRAMEWORK
================================================================================

class ContinuousTestingFramework {
public:
    void schedule_automated_tests() {
        // Daily: Basic health tests
        scheduler_.schedule_daily(std::chrono::hours(2), [this]() {
            run_health_test_suite();
        });
        
        // Weekly: Component failure tests
        scheduler_.schedule_weekly(DayOfWeek::SATURDAY,
                                  std::chrono::hours(3), [this]() {
            run_component_failure_tests();
        });
        
        // Monthly: Full DR drill
        scheduler_.schedule_monthly(1, std::chrono::hours(4), [this]() {
            run_dr_drill();
        });
        
        // Continuous: Random chaos (production)
        if (config_.enable_production_chaos) {
            scheduler_.schedule_interval(std::chrono::minutes(30), [this]() {
                chaos_manager_.execute_random_experiment();
            });
        }
    }
    
private:
    void run_health_test_suite() {
        logger_.info("Running automated health test suite");
        
        std::vector<TestResult> results;
        
        results.push_back(test_all_services_responding());
        results.push_back(test_database_connectivity());
        results.push_back(test_exchange_connectivity());
        results.push_back(test_log_aggregation());
        results.push_back(test_metrics_collection());
        results.push_back(test_alert_delivery());
        
        generate_test_report(results);
        
        if (any_test_failed(results)) {
            alert_system_.send_alert(
                AlertSeverity::P2_HIGH,
                "Automated health tests failed",
                {{"failed_tests", count_failed(results)}}
            );
        }
    }
};

================================================================================
                              END OF DOCUMENT
================================================================================
