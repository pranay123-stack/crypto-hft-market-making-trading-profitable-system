================================================================================
           ALTERNATIVE DATA SOURCES FOR ALPHA GENERATION
                  COMPREHENSIVE RESEARCH DOCUMENTATION
================================================================================

LAST UPDATED: November 2025
STATUS: Data Pipeline Development
PRIORITY: MEDIUM
EXPECTED ALPHA: 0.5-2% annualized

CONTENTS:
1. Introduction to Alternative Data
2. Sentiment Analysis from Social Media
3. News Analytics
4. Satellite Imagery
5. Web Scraping and Price Data
6. Credit Card Transaction Data
7. Search Trends and Web Traffic
8. Data Processing Pipeline
9. Feature Engineering
10. Signal Generation
11. Integration with Trading Systems
12. Compliance and Ethics

================================================================================
1. INTRODUCTION TO ALTERNATIVE DATA
================================================================================

Alternative data refers to non-traditional data sources used for investment
decisions, beyond price, volume, and fundamental financial data.

TYPES OF ALTERNATIVE DATA:
- Social media sentiment (Twitter/X, Reddit, StockTwits)
- News analytics (Bloomberg, Reuters, earnings calls)
- Satellite imagery (retail parking lots, oil storage)
- Web scraping (e-commerce prices, product reviews)
- Credit card transactions (consumer spending patterns)
- Search trends (Google Trends, Bing)
- Mobile location data (foot traffic)
- Weather data (commodities trading)

WHY IT MATTERS:
- Information edge before it's priced in
- Non-correlated alpha source
- Predictive power for fundamentals
- Real-time market insights
- Complement traditional analysis

CHALLENGES:
- Data quality and reliability
- Signal-to-noise ratio
- Legal and ethical considerations
- Data processing complexity
- Integration with trading systems
- Cost vs benefit analysis

EXPECTED RETURNS:
- Alpha contribution: 0.5-2% annually
- Sharpe improvement: 0.3-0.8
- Information ratio: 0.4-1.0
- Hit rate: 52-58%

================================================================================
2. SENTIMENT ANALYSIS FROM SOCIAL MEDIA
================================================================================

2.1 TWITTER/X SENTIMENT ANALYSIS
---------------------------------

Monitor Twitter for stock mentions and sentiment.

```python
import tweepy
import re
from textblob import TextBlob
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import pandas as pd
from collections import defaultdict
import time

class TwitterSentimentMonitor:
    def __init__(self, api_key, api_secret, access_token, access_secret):
        # Twitter API authentication
        auth = tweepy.OAuthHandler(api_key, api_secret)
        auth.set_access_token(access_token, access_secret)
        self.api = tweepy.API(auth, wait_on_rate_limit=True)
        
        # Sentiment analyzers
        self.vader = SentimentIntensityAnalyzer()
        
        # Symbol tracking
        self.symbol_sentiment = defaultdict(list)
        self.symbol_volume = defaultdict(int)
        
    def clean_tweet(self, tweet):
        # Remove URLs, mentions, hashtags
        tweet = re.sub(r'http\S+', '', tweet)
        tweet = re.sub(r'@\w+', '', tweet)
        tweet = re.sub(r'#\w+', '', tweet)
        tweet = re.sub(r'[^A-Za-z0-9\s]', '', tweet)
        return tweet.strip()
    
    def analyze_sentiment(self, text):
        # Clean text
        cleaned = self.clean_tweet(text)
        
        # VADER sentiment (good for social media)
        vader_scores = self.vader.polarity_scores(cleaned)
        
        # TextBlob sentiment
        blob = TextBlob(cleaned)
        polarity = blob.sentiment.polarity
        subjectivity = blob.sentiment.subjectivity
        
        return {
            'vader_compound': vader_scores['compound'],
            'vader_pos': vader_scores['pos'],
            'vader_neg': vader_scores['neg'],
            'vader_neu': vader_scores['neu'],
            'textblob_polarity': polarity,
            'textblob_subjectivity': subjectivity,
            'text': cleaned
        }
    
    def extract_symbols(self, text):
        # Extract cashtags ($AAPL, $TSLA, etc.)
        cashtags = re.findall(r'\$([A-Z]{1,5})\b', text)
        
        # Common ticker patterns
        tickers = re.findall(r'\b([A-Z]{1,5})\b', text)
        
        # Filter valid tickers (simplified)
        valid_tickers = [t for t in tickers if len(t) <= 5 and t.isupper()]
        
        return list(set(cashtags + valid_tickers))
    
    def stream_tweets(self, symbols, duration_hours=24):
        """Stream tweets for specified symbols"""
        
        # Build search query
        query = ' OR '.join([f'${sym}' for sym in symbols])
        
        print(f"Monitoring tweets for: {query}")
        
        end_time = time.time() + duration_hours * 3600
        
        while time.time() < end_time:
            try:
                tweets = tweepy.Cursor(
                    self.api.search_tweets,
                    q=query,
                    lang='en',
                    tweet_mode='extended',
                    count=100
                ).items(100)
                
                for tweet in tweets:
                    # Extract full text
                    if hasattr(tweet, 'retweeted_status'):
                        text = tweet.retweeted_status.full_text
                    else:
                        text = tweet.full_text
                    
                    # Analyze sentiment
                    sentiment = self.analyze_sentiment(text)
                    
                    # Extract symbols from tweet
                    tweet_symbols = self.extract_symbols(text)
                    
                    # Record sentiment for each symbol
                    for symbol in tweet_symbols:
                        if symbol in symbols:
                            self.symbol_sentiment[symbol].append({
                                'timestamp': tweet.created_at,
                                'sentiment': sentiment,
                                'followers': tweet.user.followers_count,
                                'retweets': tweet.retweet_count,
                                'likes': tweet.favorite_count
                            })
                            self.symbol_volume[symbol] += 1
                
                # Rate limiting
                time.sleep(15)  # Twitter rate limits
                
            except Exception as e:
                print(f"Error streaming tweets: {e}")
                time.sleep(60)
    
    def get_aggregated_sentiment(self, symbol, window_minutes=60):
        """Get aggregated sentiment for symbol over time window"""
        
        if symbol not in self.symbol_sentiment:
            return None
        
        # Filter recent tweets
        cutoff_time = pd.Timestamp.now() - pd.Timedelta(minutes=window_minutes)
        recent_tweets = [
            t for t in self.symbol_sentiment[symbol]
            if pd.Timestamp(t['timestamp']) > cutoff_time
        ]
        
        if not recent_tweets:
            return None
        
        # Calculate weighted sentiment (weight by followers + engagement)
        total_sentiment = 0
        total_weight = 0
        
        for tweet in recent_tweets:
            engagement = (tweet['followers'] * 0.1 + 
                         tweet['retweets'] * 10 + 
                         tweet['likes'] * 1)
            weight = max(1, engagement)
            
            sentiment_score = tweet['sentiment']['vader_compound']
            total_sentiment += sentiment_score * weight
            total_weight += weight
        
        weighted_sentiment = total_sentiment / total_weight if total_weight > 0 else 0
        
        # Calculate momentum (change in sentiment)
        if len(recent_tweets) > 10:
            half = len(recent_tweets) // 2
            first_half = recent_tweets[:half]
            second_half = recent_tweets[half:]
            
            first_sentiment = sum(t['sentiment']['vader_compound'] for t in first_half) / len(first_half)
            second_sentiment = sum(t['sentiment']['vader_compound'] for t in second_half) / len(second_half)
            
            momentum = second_sentiment - first_sentiment
        else:
            momentum = 0
        
        return {
            'symbol': symbol,
            'sentiment': weighted_sentiment,
            'momentum': momentum,
            'volume': len(recent_tweets),
            'total_engagement': total_weight,
            'timestamp': pd.Timestamp.now()
        }

# Reddit Sentiment (WallStreetBets)
import praw

class RedditSentimentMonitor:
    def __init__(self, client_id, client_secret, user_agent):
        self.reddit = praw.Reddit(
            client_id=client_id,
            client_secret=client_secret,
            user_agent=user_agent
        )
        self.vader = SentimentIntensityAnalyzer()
        
    def monitor_wallstreetbets(self, limit=100):
        """Monitor r/wallstreetbets for stock mentions"""
        
        subreddit = self.reddit.subreddit('wallstreetbets')
        
        symbol_mentions = defaultdict(list)
        
        # Hot posts
        for post in subreddit.hot(limit=limit):
            # Extract symbols
            symbols = self.extract_symbols(post.title + ' ' + post.selftext)
            
            # Analyze sentiment
            sentiment = self.vader.polarity_scores(post.title + ' ' + post.selftext)
            
            for symbol in symbols:
                symbol_mentions[symbol].append({
                    'title': post.title,
                    'score': post.score,
                    'upvote_ratio': post.upvote_ratio,
                    'num_comments': post.num_comments,
                    'sentiment': sentiment['compound'],
                    'created': pd.Timestamp.fromtimestamp(post.created_utc)
                })
        
        return symbol_mentions
    
    def extract_symbols(self, text):
        # Similar to Twitter extraction
        cashtags = re.findall(r'\$([A-Z]{1,5})\b', text)
        tickers = re.findall(r'\b([A-Z]{1,5})\b', text)
        return list(set(cashtags + tickers))
    
    def get_meme_stock_score(self, symbol):
        """Calculate 'meme stock' score for symbol"""
        
        mentions = self.monitor_wallstreetbets()
        
        if symbol not in mentions:
            return 0
        
        posts = mentions[symbol]
        
        # Calculate score based on volume, sentiment, engagement
        volume_score = min(len(posts) / 10, 10)  # Max 10 points
        avg_sentiment = sum(p['sentiment'] for p in posts) / len(posts)
        sentiment_score = (avg_sentiment + 1) * 5  # 0-10 scale
        engagement_score = sum(p['score'] for p in posts) / len(posts) / 1000
        
        total_score = volume_score + sentiment_score + engagement_score
        
        return min(total_score, 10)  # Cap at 10

# FinBERT for financial sentiment
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

class FinBERTSentiment:
    def __init__(self):
        self.tokenizer = AutoTokenizer.from_pretrained(
            "ProsusAI/finbert"
        )
        self.model = AutoModelForSequenceClassification.from_pretrained(
            "ProsusAI/finbert"
        )
        self.model.eval()
        
    def analyze(self, text):
        """Analyze financial sentiment using FinBERT"""
        
        inputs = self.tokenizer(text, return_tensors="pt", 
                               truncation=True, max_length=512)
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            predictions = torch.softmax(outputs.logits, dim=1)
        
        # Classes: positive, negative, neutral
        scores = predictions[0].tolist()
        
        return {
            'positive': scores[0],
            'negative': scores[1],
            'neutral': scores[2],
            'sentiment': scores[0] - scores[1]  # Net sentiment
        }

# Usage example
if __name__ == "__main__":
    # Twitter monitoring
    twitter = TwitterSentimentMonitor(
        api_key="YOUR_KEY",
        api_secret="YOUR_SECRET",
        access_token="YOUR_TOKEN",
        access_secret="YOUR_TOKEN_SECRET"
    )
    
    symbols = ['AAPL', 'TSLA', 'NVDA', 'AMD']
    
    # Start streaming (in separate thread)
    import threading
    stream_thread = threading.Thread(
        target=twitter.stream_tweets,
        args=(symbols, 1)  # 1 hour
    )
    stream_thread.start()
    
    # Check sentiment periodically
    time.sleep(300)  # Wait 5 minutes
    
    for symbol in symbols:
        sentiment = twitter.get_aggregated_sentiment(symbol)
        if sentiment:
            print(f"{symbol}: Sentiment={sentiment['sentiment']:.3f}, "
                  f"Volume={sentiment['volume']}, "
                  f"Momentum={sentiment['momentum']:.3f}")
```

================================================================================
3. NEWS ANALYTICS
================================================================================

```python
import feedparser
from newspaper import Article
from datetime import datetime, timedelta
import requests
from bs4 import BeautifulSoup

class NewsAnalytics:
    def __init__(self):
        self.finbert = FinBERTSentiment()
        self.vader = SentimentIntensityAnalyzer()
        
        # News sources RSS feeds
        self.rss_feeds = {
            'bloomberg': 'https://www.bloomberg.com/feed/podcast/etf-report.xml',
            'reuters': 'https://www.reutersagency.com/feed/',
            'cnbc': 'https://www.cnbc.com/id/100003114/device/rss/rss.html',
            'marketwatch': 'http://feeds.marketwatch.com/marketwatch/topstories/',
        }
        
    def fetch_news(self, symbol, hours_back=24):
        """Fetch news articles for symbol"""
        
        articles = []
        cutoff = datetime.now() - timedelta(hours=hours_back)
        
        # Try RSS feeds
        for source, feed_url in self.rss_feeds.items():
            try:
                feed = feedparser.parse(feed_url)
                
                for entry in feed.entries:
                    # Check if symbol mentioned
                    if symbol.lower() in entry.title.lower() or \
                       symbol.lower() in entry.get('summary', '').lower():
                        
                        pub_date = datetime(*entry.published_parsed[:6])
                        
                        if pub_date > cutoff:
                            articles.append({
                                'source': source,
                                'title': entry.title,
                                'link': entry.link,
                                'published': pub_date,
                                'summary': entry.get('summary', '')
                            })
            except Exception as e:
                print(f"Error fetching from {source}: {e}")
        
        return articles
    
    def analyze_article(self, article_url):
        """Deep analysis of article"""
        
        try:
            article = Article(article_url)
            article.download()
            article.parse()
            article.nlp()
            
            # Extract key information
            title = article.title
            text = article.text
            summary = article.summary
            keywords = article.keywords
            
            # Sentiment analysis
            finbert_sentiment = self.finbert.analyze(text[:512])
            vader_sentiment = self.vader.polarity_scores(text)
            
            # Extract entities (companies, people, locations)
            # Would use spaCy NER here in production
            
            return {
                'title': title,
                'text': text,
                'summary': summary,
                'keywords': keywords,
                'finbert_sentiment': finbert_sentiment,
                'vader_sentiment': vader_sentiment,
                'publish_date': article.publish_date
            }
            
        except Exception as e:
            print(f"Error analyzing article: {e}")
            return None
    
    def calculate_news_score(self, symbol, hours_back=24):
        """Calculate aggregate news score for symbol"""
        
        articles = self.fetch_news(symbol, hours_back)
        
        if not articles:
            return None
        
        # Analyze each article
        sentiments = []
        for article in articles:
            analysis = self.analyze_article(article['link'])
            if analysis:
                sentiments.append(analysis['finbert_sentiment']['sentiment'])
        
        if not sentiments:
            return None
        
        # Aggregate
        avg_sentiment = sum(sentiments) / len(sentiments)
        sentiment_std = pd.Series(sentiments).std()
        
        # News velocity (how much coverage)
        news_velocity = len(articles) / hours_back
        
        return {
            'symbol': symbol,
            'avg_sentiment': avg_sentiment,
            'sentiment_volatility': sentiment_std,
            'news_velocity': news_velocity,
            'num_articles': len(articles),
            'timestamp': datetime.now()
        }

# Earnings call analysis
class EarningsCallAnalyzer:
    def __init__(self):
        self.finbert = FinBERTSentiment()
        
    def analyze_transcript(self, transcript_text):
        """Analyze earnings call transcript"""
        
        # Split into sections
        sections = {
            'prepared_remarks': [],
            'qa': []
        }
        
        # Sentiment by section
        prepared_sentiment = self.finbert.analyze(transcript_text[:2000])
        
        # Extract key phrases
        # Would use more sophisticated NLP here
        
        # Detect management confidence
        confidence_keywords = ['confident', 'strong', 'growth', 'positive',
                              'optimistic', 'bullish', 'success']
        uncertainty_keywords = ['uncertain', 'challenging', 'difficult',
                               'headwind', 'concern', 'risk']
        
        confidence_score = sum(transcript_text.lower().count(k) 
                              for k in confidence_keywords)
        uncertainty_score = sum(transcript_text.lower().count(k)
                               for k in uncertainty_keywords)
        
        net_confidence = confidence_score - uncertainty_score
        
        return {
            'sentiment': prepared_sentiment,
            'confidence_score': net_confidence,
            'num_positive_keywords': confidence_score,
            'num_negative_keywords': uncertainty_score
        }
```

================================================================================
4. SATELLITE IMAGERY ANALYSIS
================================================================================

```python
import cv2
import numpy as np
from sklearn.cluster import KMeans

class SatelliteImageryAnalyzer:
    """
    Analyze satellite images for trading signals
    - Parking lot occupancy for retail stocks
    - Oil storage tank levels
    - Shipping container counts
    - Agricultural crop health
    """
    
    def __init__(self):
        pass
    
    def analyze_parking_lot(self, image_path):
        """Count cars in parking lot"""
        
        # Load image
        img = cv2.imread(image_path)
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # Detect cars (simplified - would use YOLO/ResNet in production)
        # Using blob detection as proxy
        params = cv2.SimpleBlobDetector_Params()
        params.filterByArea = True
        params.minArea = 50
        params.maxArea = 500
        
        detector = cv2.SimpleBlobDetector_create(params)
        keypoints = detector.detect(gray)
        
        car_count = len(keypoints)
        
        # Calculate occupancy percentage
        # Assume known total spots
        total_spots = 500
        occupancy = min(car_count / total_spots, 1.0)
        
        return {
            'car_count': car_count,
            'occupancy_rate': occupancy,
            'timestamp': datetime.now()
        }
    
    def analyze_oil_storage(self, image_path):
        """Estimate oil tank fill levels"""
        
        img = cv2.imread(image_path)
        
        # Detect tank shadows (shadow length indicates fill level)
        # More shadow = more full
        # This is simplified - real implementation would use
        # more sophisticated computer vision
        
        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
        
        # Detect edges
        edges = cv2.Canny(gray, 50, 150)
        
        # Find contours (tanks)
        contours, _ = cv2.findContours(
            edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE
        )
        
        # Analyze each tank
        fill_levels = []
        for contour in contours:
            area = cv2.contourArea(contour)
            if 1000 < area < 50000:  # Filter tank-sized objects
                # Measure shadow (simplified)
                x, y, w, h = cv2.boundingRect(contour)
                roi = gray[y:y+h, x:x+w]
                
                # Darker = more shadow = more full
                darkness = 255 - np.mean(roi)
                fill_level = darkness / 255
                
                fill_levels.append(fill_level)
        
        if fill_levels:
            avg_fill = np.mean(fill_levels)
        else:
            avg_fill = None
        
        return {
            'num_tanks': len(fill_levels),
            'avg_fill_level': avg_fill,
            'fill_levels': fill_levels
        }
    
    def track_over_time(self, location, image_dates):
        """Track changes over time"""
        
        time_series = []
        
        for date, image_path in image_dates:
            # Analyze image
            if 'parking' in location.lower():
                result = self.analyze_parking_lot(image_path)
                metric = result['occupancy_rate']
            elif 'oil' in location.lower():
                result = self.analyze_oil_storage(image_path)
                metric = result['avg_fill_level']
            else:
                continue
            
            time_series.append({
                'date': date,
                'metric': metric
            })
        
        # Calculate trend
        df = pd.DataFrame(time_series)
        df['trend'] = df['metric'].pct_change()
        
        return df
```

================================================================================
5. WEB SCRAPING FOR PRICE DATA
================================================================================

```python
import requests
from bs4 import BeautifulSoup
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC

class WebScraper:
    def __init__(self):
        # Selenium for JavaScript-rendered pages
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        self.driver = webdriver.Chrome(options=options)
        
    def scrape_amazon_prices(self, product_asin):
        """Scrape Amazon product prices"""
        
        url = f"https://www.amazon.com/dp/{product_asin}"
        
        try:
            self.driver.get(url)
            
            # Wait for price to load
            price_element = WebDriverWait(self.driver, 10).until(
                EC.presence_of_element_located((By.ID, "priceblock_ourprice"))
            )
            
            price_text = price_element.text
            price = float(price_text.replace('$', '').replace(',', ''))
            
            # Get availability
            availability = self.driver.find_element(By.ID, "availability").text
            
            return {
                'asin': product_asin,
                'price': price,
                'availability': availability,
                'timestamp': datetime.now()
            }
            
        except Exception as e:
            print(f"Error scraping Amazon: {e}")
            return None
    
    def monitor_crypto_exchanges(self):
        """Monitor crypto prices across exchanges"""
        
        exchanges = {
            'binance': 'https://api.binance.com/api/v3/ticker/price',
            'coinbase': 'https://api.coinbase.com/v2/prices/BTC-USD/spot',
            'kraken': 'https://api.kraken.com/0/public/Ticker?pair=XBTUSD'
        }
        
        prices = {}
        
        for exchange, url in exchanges.items():
            try:
                response = requests.get(url)
                data = response.json()
                
                # Parse based on exchange API format
                if exchange == 'binance':
                    btc_data = [x for x in data if x['symbol'] == 'BTCUSDT'][0]
                    price = float(btc_data['price'])
                elif exchange == 'coinbase':
                    price = float(data['data']['amount'])
                elif exchange == 'kraken':
                    price = float(data['result']['XXBTZUSD']['c'][0])
                
                prices[exchange] = price
                
            except Exception as e:
                print(f"Error fetching from {exchange}: {e}")
        
        # Calculate arbitrage opportunities
        if len(prices) > 1:
            min_price = min(prices.values())
            max_price = max(prices.values())
            spread = (max_price - min_price) / min_price
            
            return {
                'prices': prices,
                'min_exchange': min(prices, key=prices.get),
                'max_exchange': max(prices, key=prices.get),
                'spread_pct': spread * 100,
                'timestamp': datetime.now()
            }
        
        return None
    
    def __del__(self):
        self.driver.quit()
```

================================================================================
6. DATA PROCESSING PIPELINE
================================================================================

```python
import apache_beam as beam
from apache_beam.options.pipeline_options import PipelineOptions
import logging

class AlternativeDataPipeline:
    """Real-time processing pipeline for alternative data"""
    
    def __init__(self):
        self.options = PipelineOptions()
        
    def run(self):
        with beam.Pipeline(options=self.options) as pipeline:
            # Twitter sentiment stream
            twitter_sentiment = (
                pipeline
                | 'Read Twitter' >> beam.io.ReadFromPubSub(
                    subscription='projects/myproject/subscriptions/twitter'
                )
                | 'Parse Twitter' >> beam.Map(self.parse_twitter_message)
                | 'Analyze Twitter Sentiment' >> beam.Map(self.analyze_sentiment)
                | 'Window Twitter' >> beam.WindowInto(
                    beam.window.FixedWindows(60)  # 1-minute windows
                )
                | 'Aggregate Twitter' >> beam.CombinePerKey(self.combine_sentiments)
            )
            
            # News analytics stream
            news_analytics = (
                pipeline
                | 'Read News' >> beam.io.ReadFromPubSub(
                    subscription='projects/myproject/subscriptions/news'
                )
                | 'Parse News' >> beam.Map(self.parse_news_message)
                | 'Analyze News' >> beam.Map(self.analyze_news)
                | 'Window News' >> beam.WindowInto(
                    beam.window.FixedWindows(300)  # 5-minute windows
                )
                | 'Aggregate News' >> beam.CombinePerKey(self.combine_news)
            )
            
            # Combine signals
            combined = (
                (twitter_sentiment, news_analytics)
                | 'Flatten' >> beam.Flatten()
                | 'Combine Signals' >> beam.CombinePerKey(self.combine_all_signals)
                | 'Generate Trading Signals' >> beam.Map(self.generate_trading_signal)
                | 'Write to Trading System' >> beam.io.WriteToKafka(
                    topic='trading-signals',
                    producer_config={'bootstrap.servers': 'localhost:9092'}
                )
            )
    
    def parse_twitter_message(self, message):
        data = json.loads(message)
        return (data['symbol'], data)
    
    def analyze_sentiment(self, kv):
        symbol, data = kv
        # Sentiment analysis logic
        return (symbol, {'sentiment': 0.5, 'volume': 1})
    
    def combine_sentiments(self, sentiments):
        # Aggregate sentiments
        return {
            'avg_sentiment': sum(s['sentiment'] for s in sentiments) / len(sentiments),
            'total_volume': sum(s['volume'] for s in sentiments)
        }
    
    def parse_news_message(self, message):
        data = json.loads(message)
        return (data['symbol'], data)
    
    def analyze_news(self, kv):
        symbol, data = kv
        # News analysis logic
        return (symbol, {'sentiment': 0.6, 'count': 1})
    
    def combine_news(self, news_items):
        return {
            'avg_sentiment': sum(n['sentiment'] for n in news_items) / len(news_items),
            'count': sum(n['count'] for n in news_items)
        }
    
    def combine_all_signals(self, signals):
        # Combine Twitter + News + other signals
        return {
            'combined_sentiment': sum(s.get('avg_sentiment', 0) for s in signals) / len(signals),
            'confidence': len(signals) / 5.0  # More sources = higher confidence
        }
    
    def generate_trading_signal(self, kv):
        symbol, data = kv
        
        sentiment = data['combined_sentiment']
        confidence = data['confidence']
        
        # Generate signal
        if sentiment > 0.6 and confidence > 0.7:
            signal = 'BUY'
        elif sentiment < 0.4 and confidence > 0.7:
            signal = 'SELL'
        else:
            signal = 'HOLD'
        
        return {
            'symbol': symbol,
            'signal': signal,
            'sentiment': sentiment,
            'confidence': confidence,
            'timestamp': datetime.now().isoformat()
        }
```

================================================================================
7. INTEGRATION WITH TRADING SYSTEM
================================================================================

```cpp
#include <iostream>
#include <string>
#include <map>
#include <chrono>
#include <kafka/KafkaConsumer.h>
#include <nlohmann/json.hpp>

using json = nlohmann::json;

class AlternativeDataIntegration {
private:
    struct AlternativeSignal {
        std::string symbol;
        double sentiment;
        double confidence;
        std::chrono::system_clock::time_point timestamp;
    };
    
    std::map<std::string, AlternativeSignal> latest_signals_;
    
public:
    AlternativeDataIntegration() {}
    
    void consume_signals() {
        // Kafka consumer for alternative data signals
        kafka::Properties props({
            {"bootstrap.servers", "localhost:9092"},
            {"group.id", "hft-trading-system"},
            {"auto.offset.reset", "latest"}
        });
        
        kafka::KafkaConsumer consumer(props);
        consumer.subscribe({"trading-signals"});
        
        while (true) {
            auto records = consumer.poll(std::chrono::milliseconds(100));
            
            for (const auto& record : records) {
                try {
                    // Parse JSON message
                    json signal = json::parse(record.value().toString());
                    
                    std::string symbol = signal["symbol"];
                    double sentiment = signal["sentiment"];
                    double confidence = signal["confidence"];
                    
                    // Store signal
                    latest_signals_[symbol] = AlternativeSignal{
                        symbol, sentiment, confidence,
                        std::chrono::system_clock::now()
                    };
                    
                    std::cout << "Received signal: " << symbol
                              << " sentiment=" << sentiment
                              << " confidence=" << confidence << std::endl;
                    
                } catch (const std::exception& e) {
                    std::cerr << "Error parsing signal: " << e.what() << std::endl;
                }
            }
        }
    }
    
    double get_alternative_alpha(const std::string& symbol) {
        auto it = latest_signals_.find(symbol);
        if (it == latest_signals_.end()) {
            return 0.0;  // No signal available
        }
        
        auto& signal = it->second;
        
        // Check if signal is stale (> 5 minutes old)
        auto now = std::chrono::system_clock::now();
        auto age = std::chrono::duration_cast<std::chrono::minutes>(
            now - signal.timestamp
        ).count();
        
        if (age > 5) {
            return 0.0;  // Stale signal
        }
        
        // Convert sentiment to alpha signal
        // Sentiment range [-1, 1] -> Alpha range [-0.02, 0.02] (2% max)
        double alpha = (signal.sentiment - 0.5) * 2.0 * 0.02 * signal.confidence;
        
        return alpha;
    }
    
    void adjust_strategy(const std::string& symbol, double& target_position) {
        double alpha = get_alternative_alpha(symbol);
        
        // Adjust target position based on alternative data
        // Positive alpha -> increase position
        // Negative alpha -> decrease position
        
        target_position *= (1.0 + alpha);
        
        std::cout << "Adjusted " << symbol << " position by "
                  << (alpha * 100) << "% based on alternative data" << std::endl;
    }
};
```

================================================================================
8. COMPLIANCE AND ETHICS
================================================================================

LEGAL CONSIDERATIONS:
- Insider trading laws: Ensure data is publicly available
- Material non-public information: Never use MNPI
- Data licensing: Respect terms of service
- Privacy laws: GDPR, CCPA compliance
- Securities regulations: SEC, FINRA rules

ETHICAL GUIDELINES:
- Transparency: Disclose data sources
- Fairness: Don't exploit information asymmetries unfairly
- Privacy: Respect individual privacy
- Accuracy: Verify data quality
- Responsibility: Consider market impact

BEST PRACTICES:
- Legal review of all data sources
- Data provenance tracking
- Regular compliance audits
- Clear documentation
- Ethical review board

================================================================================
9. PERFORMANCE EXPECTATIONS
================================================================================

ALPHA CONTRIBUTION BY SOURCE:
Twitter/Reddit sentiment: 0.3-0.8% annually
News analytics: 0.4-1.0% annually
Satellite imagery: 0.5-1.5% annually (sector-specific)
Web scraping: 0.2-0.6% annually
Combined: 0.5-2.0% annually

SHARPE IMPROVEMENT: 0.3-0.8
INFORMATION RATIO: 0.4-1.0
HIT RATE: 52-58%

COSTS:
- Data acquisition: $50K-500K/year
- Infrastructure: $20K-100K/year
- Personnel: $200K-500K/year
- Total: $270K-1.1M/year

ROI:
On $100M AUM with 1% alpha = $1M/year
Net profit: $0-730K/year
ROI: 0-70%

TIMELINE: 6-12 months from research to production

================================================================================
CONCLUSION
================================================================================

Alternative data provides meaningful alpha when:
1. High quality data sources
2. Robust processing pipeline
3. Proper integration with trading system
4. Legal and ethical compliance
5. Continuous validation and improvement

Expected contribution: 0.5-2% alpha with moderate costs.
Worth pursuing for funds with >$50M AUM.

================================================================================
END OF ALTERNATIVE DATA DOCUMENTATION
================================================================================
