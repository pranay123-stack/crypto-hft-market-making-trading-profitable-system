================================================================================
                      REPLAY BEST PRACTICES
                 Production Tips and Common Pitfalls
================================================================================

PURPOSE: Comprehensive guide to implementing and operating a replay engine
effectively, avoiding common mistakes, and achieving accurate results.

================================================================================
                         TABLE OF CONTENTS
================================================================================

1. Data Quality Requirements
2. Common Pitfalls to Avoid
3. Production Deployment Tips
4. Continuous Validation
5. Regulatory Considerations
6. Operational Procedures
7. Troubleshooting Guide

================================================================================
                   1. DATA QUALITY REQUIREMENTS
================================================================================

DATA CAPTURE CHECKLIST:
-----------------------

✓ TIMESTAMP PRECISION
  - Use nanosecond precision (rdtsc + calibration)
  - Synchronize all machines with PTP/GPS
  - Record both local receive time AND exchange time
  - Account for clock drift

✓ SEQUENCE INTEGRITY
  - Assign monotonic sequence numbers
  - Detect and record gaps
  - Store gap recovery data separately
  - Mark out-of-order messages

✓ COMPLETE CAPTURE
  - All market data messages (not sampled)
  - Full order book updates (not just BBO)
  - All trade events
  - Order lifecycle events (if trading)

✓ METADATA
  - Capture session boundaries
  - Record exchange status changes
  - Store trading halt/resume events
  - Preserve original message format

DATA VALIDATION RULES:
----------------------

```cpp
class DataQualityValidator {
public:
    struct ValidationResult {
        bool passed;
        std::vector<std::string> errors;
        std::vector<std::string> warnings;
        DataQualityMetrics metrics;
    };

    ValidationResult validate(const TickFile& file) {
        ValidationResult result;
        result.passed = true;

        // Check timestamp monotonicity
        if (!validate_timestamps(file)) {
            result.errors.push_back("Non-monotonic timestamps detected");
            result.passed = false;
        }

        // Check for gaps
        auto gaps = detect_gaps(file);
        if (!gaps.empty()) {
            result.warnings.push_back(
                "Detected " + std::to_string(gaps.size()) + " sequence gaps"
            );
        }

        // Check data completeness
        double completeness = calculate_completeness(file);
        if (completeness < 0.99) {
            result.warnings.push_back(
                "Data completeness: " + std::to_string(completeness * 100) + "%"
            );
        }

        // Check for suspicious patterns
        if (detect_duplicate_timestamps(file)) {
            result.warnings.push_back("Duplicate timestamps found");
        }

        result.metrics = calculate_metrics(file);
        return result;
    }

private:
    bool validate_timestamps(const TickFile& file) {
        int64_t prev_ts = 0;
        for (const auto& tick : file) {
            if (tick.timestamp < prev_ts) {
                return false;
            }
            prev_ts = tick.timestamp;
        }
        return true;
    }

    std::vector<Gap> detect_gaps(const TickFile& file) {
        std::vector<Gap> gaps;
        int64_t prev_seq = 0;
        for (const auto& tick : file) {
            if (tick.sequence != prev_seq + 1 && prev_seq != 0) {
                gaps.push_back({prev_seq, tick.sequence});
            }
            prev_seq = tick.sequence;
        }
        return gaps;
    }
};
```

MINIMUM DATA REQUIREMENTS:
--------------------------

| Data Type         | Min Frequency    | Max Gap Allowed | Notes              |
|-------------------|------------------|-----------------|---------------------|
| BBO Updates       | Per change       | 0               | Every price change  |
| Order Book L2     | 100ms snapshots  | 500ms           | Full depth          |
| Trades            | Every trade      | 0               | With trade ID       |
| Exchange Status   | On change        | 0               | Halts, sessions     |
| Timestamps        | Nanosecond       | N/A             | Monotonic           |

================================================================================
                   2. COMMON PITFALLS TO AVOID
================================================================================

PITFALL #1: LOOK-AHEAD BIAS
---------------------------

Problem: Using information that wasn't available at the time

```cpp
// WRONG: Using future data
void process_tick(const Tick& tick) {
    // This price wasn't known at tick.timestamp!
    double future_price = get_price(tick.timestamp + 1_sec);
    if (future_price > tick.price) {
        buy();  // Guaranteed profit - CHEATING!
    }
}

// CORRECT: Only use past/current data
void process_tick(const Tick& tick) {
    // Only use data available at this moment
    double current_price = tick.price;
    double historical_avg = calculate_avg(tick.timestamp - 1_min, tick.timestamp);
    if (current_price < historical_avg * 0.99) {
        buy();
    }
}
```

PITFALL #2: PERFECT FILL ASSUMPTION
-----------------------------------

Problem: Assuming orders fill instantly at quoted price

```cpp
// WRONG: Instant fill at mid
void backtest_fill(Order& order) {
    order.fill_price = (bid + ask) / 2;  // Never happens in reality
    order.filled = true;
}

// CORRECT: Simulate realistic fills
void replay_fill(Order& order, const OrderBook& book) {
    // Consider queue position
    int queue_position = estimate_queue_position(order);

    // Consider market impact
    double impact = calculate_impact(order.quantity, book.depth());

    // Simulate partial fills
    order.fill_quantity = simulate_partial_fill(order, book);
    order.fill_price = order.side == BUY ?
        book.ask + impact : book.bid - impact;
}
```

PITFALL #3: IGNORING LATENCY
----------------------------

Problem: Processing events faster than real-time

```cpp
// WRONG: Process all ticks instantly
void run_backtest() {
    for (const auto& tick : ticks) {
        process_tick(tick);  // No delay between ticks
    }
}

// CORRECT: Respect actual timing
void run_replay() {
    auto start_real = Clock::now();
    auto start_sim = ticks.front().timestamp;

    for (const auto& tick : ticks) {
        // Wait until proper time
        auto target_real = start_real + (tick.timestamp - start_sim);
        std::this_thread::sleep_until(target_real);

        // Add processing latency
        process_tick_with_latency(tick);
    }
}
```

PITFALL #4: DATA SNOOPING
-------------------------

Problem: Optimizing parameters on test data

```cpp
// WRONG: Optimize on full dataset
void optimize_strategy() {
    for (double param = 0.1; param <= 2.0; param += 0.1) {
        double sharpe = backtest_full_history(param);
        if (sharpe > best_sharpe) {
            best_param = param;  // Overfitting!
        }
    }
}

// CORRECT: Walk-forward optimization
void optimize_strategy_properly() {
    // Split data into train/test
    auto [train_data, test_data] = split_data(0.7);

    // Optimize on train
    double best_param = optimize(train_data);

    // Validate on unseen test data
    double test_sharpe = backtest(test_data, best_param);

    // Only use if test performance is acceptable
    if (test_sharpe > MIN_ACCEPTABLE_SHARPE) {
        use_parameter(best_param);
    }
}
```

PITFALL #5: SURVIVORSHIP BIAS
-----------------------------

Problem: Only testing on symbols that still exist

```cpp
// WRONG: Test on current universe
void backtest_universe() {
    auto symbols = get_current_symbols();  // Missing delisted symbols
    for (const auto& symbol : symbols) {
        test_strategy(symbol);
    }
}

// CORRECT: Include delisted symbols
void backtest_universe_correctly() {
    auto symbols = get_historical_universe(backtest_start_date);
    for (const auto& symbol : symbols) {
        test_strategy(symbol);  // Includes symbols that later delisted
    }
}
```

PITFALL #6: NOT ACCOUNTING FOR FEES
-----------------------------------

Problem: Ignoring transaction costs

```cpp
// WRONG: Ignore fees
double calculate_pnl(const std::vector<Trade>& trades) {
    double pnl = 0;
    for (const auto& trade : trades) {
        pnl += trade.side == BUY ?
            -trade.price * trade.quantity :
            trade.price * trade.quantity;
    }
    return pnl;  // Looks great, but fees will kill it
}

// CORRECT: Include all costs
double calculate_pnl_with_costs(const std::vector<Trade>& trades) {
    double pnl = 0;
    for (const auto& trade : trades) {
        double gross = trade.side == BUY ?
            -trade.price * trade.quantity :
            trade.price * trade.quantity;

        // Subtract fees
        double fee = trade.quantity * trade.price * FEE_RATE;

        // Add/subtract rebate
        double rebate = trade.is_maker ?
            trade.quantity * trade.price * MAKER_REBATE : 0;

        pnl += gross - fee + rebate;
    }
    return pnl;
}
```

================================================================================
                   3. PRODUCTION DEPLOYMENT TIPS
================================================================================

DEPLOYMENT CHECKLIST:
---------------------

□ Data pipeline validated
□ Storage capacity planned (expect 50-100GB/day for crypto)
□ Backup system configured
□ Monitoring dashboards deployed
□ Alerting configured
□ Documentation complete
□ Team trained on operation
□ Runbooks created
□ DR procedures tested

INFRASTRUCTURE RECOMMENDATIONS:
-------------------------------

STORAGE:
- Use NVMe SSDs for hot data (last 30 days)
- Use HDD arrays for warm data (30 days - 1 year)
- Use object storage (S3) for cold data (> 1 year)
- Implement tiered storage with automatic migration

COMPUTE:
- Dedicate cores for replay (isolcpus)
- Use NUMA-aware allocation
- Pre-allocate memory for large replays
- Consider GPU for parallel scenario testing

NETWORK:
- Low-latency storage network
- Separate replay traffic from production
- Monitor bandwidth utilization

RECOMMENDED SPECS:
------------------

| Component    | Minimum          | Recommended       | High Performance  |
|--------------|------------------|-------------------|-------------------|
| CPU          | 8 cores          | 16 cores          | 32+ cores         |
| RAM          | 64 GB            | 128 GB            | 256+ GB           |
| Storage      | 2 TB NVMe        | 8 TB NVMe         | 16+ TB NVMe       |
| Network      | 10 Gbps          | 25 Gbps           | 100 Gbps          |

CONFIGURATION BEST PRACTICES:
-----------------------------

```yaml
# replay_config.yaml
replay_engine:
  # Thread configuration
  io_threads: 4
  processing_threads: 8
  isolated_cpus: [16, 17, 18, 19, 20, 21, 22, 23]

  # Memory configuration
  preallocate_mb: 8192
  buffer_size_mb: 512
  use_huge_pages: true

  # Storage configuration
  data_directory: /data/replay
  temp_directory: /tmp/replay
  compression: lz4

  # Performance tuning
  batch_size: 10000
  prefetch_seconds: 60
  parallel_reads: 4

  # Validation
  validate_data_quality: true
  max_gap_tolerance_ms: 100
  timestamp_check: strict
```

================================================================================
                   4. CONTINUOUS VALIDATION
================================================================================

DAILY VALIDATION PROCESS:
-------------------------

```
06:00 - Capture overnight data quality metrics
06:30 - Run automated data validation
07:00 - Compare previous day replay vs live
07:30 - Generate discrepancy report
08:00 - Review by ops team
08:30 - Fix any issues before trading
```

AUTOMATED VALIDATION PIPELINE:
------------------------------

```cpp
class ContinuousValidator {
public:
    void run_daily_validation(const Date& date) {
        // 1. Validate data quality
        auto data_quality = validate_data(date);
        if (!data_quality.passed) {
            alert("Data quality issues detected", data_quality.errors);
        }

        // 2. Run replay
        auto replay_result = run_replay(date);

        // 3. Compare with live
        auto live_trades = load_live_trades(date);
        auto comparison = compare(replay_result, live_trades);

        // 4. Check correlation
        if (comparison.correlation < MIN_CORRELATION) {
            alert("Replay correlation below threshold",
                  "Correlation: " + std::to_string(comparison.correlation));
        }

        // 5. Store results
        store_validation_result(date, comparison);

        // 6. Update calibration if needed
        if (needs_recalibration(comparison)) {
            trigger_recalibration();
        }
    }

private:
    const double MIN_CORRELATION = 0.95;
};
```

VALIDATION METRICS TO TRACK:
----------------------------

| Metric                | Target    | Alert Threshold | Critical Threshold |
|-----------------------|-----------|-----------------|---------------------|
| Data Completeness     | >99.9%    | <99.5%          | <99.0%             |
| Timestamp Accuracy    | <1μs      | >5μs            | >10μs              |
| P&L Correlation       | >0.95     | <0.90           | <0.85              |
| Trade Match Rate      | >98%      | <95%            | <90%               |
| Fill Price Accuracy   | <1 bps    | >3 bps          | >5 bps             |

================================================================================
                   5. REGULATORY CONSIDERATIONS
================================================================================

RECORD KEEPING REQUIREMENTS:
----------------------------

SEC RULE 17a-4:
- Keep all communications and trading records
- 6 year retention minimum
- Searchable within 24 hours
- Write-once storage recommended

MiFID II:
- Nanosecond timestamps
- 5-year retention for order records
- 7-year retention for transaction reports
- Clock synchronization to UTC

FINRA:
- Maintain audit trail of all orders
- Record timing of receipt and execution
- Preserve market data used for decisions

AUDIT-READY REPLAY:
-------------------

```cpp
class AuditableReplayEngine {
public:
    void run_auditable_replay(const ReplayConfig& config) {
        // Create audit record
        AuditRecord audit;
        audit.start_time = Clock::now();
        audit.replay_id = generate_uuid();
        audit.config = config;

        // Run replay with full logging
        enable_audit_logging();
        auto result = run_replay(config);

        // Finalize audit record
        audit.end_time = Clock::now();
        audit.result_hash = hash(result);
        audit.config_hash = hash(config);

        // Sign audit record
        audit.signature = sign(audit);

        // Store in tamper-evident storage
        store_audit_record(audit);
    }

private:
    void enable_audit_logging() {
        // Log every decision point
        log_strategy_signals_ = true;
        log_order_generation_ = true;
        log_fill_simulation_ = true;
        log_pnl_calculation_ = true;
    }
};
```

================================================================================
                   6. OPERATIONAL PROCEDURES
================================================================================

STANDARD OPERATING PROCEDURES:
------------------------------

SOP-001: DAILY REPLAY VALIDATION
1. Run at 06:00 UTC automatically
2. Review results by 08:00 UTC
3. Escalate issues immediately
4. Document any discrepancies
5. Sign off in ops log

SOP-002: STRATEGY CHANGE VALIDATION
1. Run replay on last 30 days data
2. Compare before/after metrics
3. Require 2 approvals for production
4. Document change rationale
5. Store baseline for regression

SOP-003: DATA RECOVERY
1. Detect gap via monitoring
2. Retrieve backup data
3. Validate recovered data
4. Merge into primary store
5. Re-run affected replays
6. Document incident

SOP-004: CALIBRATION UPDATE
1. Review calibration metrics weekly
2. Update if correlation drops below 0.95
3. Test calibration on held-out data
4. Deploy with proper approval
5. Monitor for 48 hours post-change

RUNBOOK TEMPLATE:
-----------------

```markdown
# Replay Engine Runbook

## Service Overview
- Service: replay-engine
- Owner: Trading Technology
- On-Call: trading-tech@company.com

## Health Checks
- Dashboard: https://grafana.internal/replay
- Alerts: PagerDuty #replay-engine

## Common Issues

### Issue: Replay hangs
Symptoms: Progress stops, no errors
Steps:
1. Check disk space: `df -h /data/replay`
2. Check memory: `free -h`
3. Check logs: `tail -f /var/log/replay/engine.log`
4. Restart if needed: `systemctl restart replay-engine`

### Issue: Low correlation
Symptoms: Correlation drops below 0.90
Steps:
1. Check data quality report
2. Review recent calibration changes
3. Check for exchange API changes
4. Run manual comparison on specific trades
5. Escalate if not resolved in 2 hours
```

================================================================================
                   7. TROUBLESHOOTING GUIDE
================================================================================

COMMON ISSUES AND SOLUTIONS:
----------------------------

ISSUE: "Replay P&L doesn't match live"

Diagnosis Steps:
1. Check trade count - do they match?
2. Compare individual trade prices
3. Check for missed fills
4. Verify fee calculations
5. Check for position reconciliation errors

Solutions:
- If trade count differs: Check signal generation
- If prices differ: Calibrate fill simulation
- If fees differ: Update fee schedule
- If positions differ: Check for missed fills

ISSUE: "Replay is too slow"

Diagnosis Steps:
1. Check disk I/O: `iostat -x 1`
2. Check CPU usage: `htop`
3. Check memory: `free -h`
4. Profile code: `perf record ./replay`

Solutions:
- If I/O bound: Use faster storage, increase prefetch
- If CPU bound: Use more threads, optimize hot paths
- If memory bound: Reduce buffer sizes, use mmap

ISSUE: "Data quality validation fails"

Diagnosis Steps:
1. Check capture system logs
2. Verify network connectivity during capture
3. Check exchange status (maintenance?)
4. Compare with alternative data source

Solutions:
- If gaps detected: Recover from backup
- If timestamps wrong: Check clock sync
- If duplicates found: Deduplicate data
- If format wrong: Check parser version

DEBUGGING TOOLS:
----------------

```cpp
// Replay debugging utilities
class ReplayDebugger {
public:
    // Step through tick by tick
    void step_replay(int num_ticks = 1) {
        for (int i = 0; i < num_ticks; i++) {
            auto tick = next_tick();
            print_tick_detail(tick);
            wait_for_keypress();
        }
    }

    // Breakpoint on specific condition
    void break_on_condition(std::function<bool(const Tick&)> condition) {
        while (true) {
            auto tick = next_tick();
            if (condition(tick)) {
                print_tick_detail(tick);
                enter_debug_mode();
                break;
            }
        }
    }

    // Dump state at specific time
    void dump_state_at(const Timestamp& time) {
        seek_to(time);
        dump_order_book();
        dump_positions();
        dump_pending_orders();
        dump_strategy_state();
    }
};
```

================================================================================
                         END OF DOCUMENT
================================================================================
