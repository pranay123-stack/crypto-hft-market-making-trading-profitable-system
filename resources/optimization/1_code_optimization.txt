================================================================================
HFT CODE OPTIMIZATION TECHNIQUES
================================================================================

TABLE OF CONTENTS
-----------------
1. Theory and Fundamental Concepts
2. Algorithmic Optimization
3. Data Structure Optimization
4. Hot Path Optimization
5. Branch Optimization
6. Loop Optimization
7. Function Inlining
8. Template Metaprogramming
9. Benchmarking Methods
10. Measurement Techniques
11. Expected Improvements

================================================================================
1. THEORY AND FUNDAMENTAL CONCEPTS
================================================================================

Core Principles for HFT Code Optimization:
-------------------------------------------
1. Minimize latency at every level (nanosecond matters)
2. Maximize CPU instruction throughput (IPC - Instructions Per Cycle)
3. Reduce cache misses (L1, L2, L3)
4. Eliminate branch mispredictions
5. Minimize memory allocations
6. Maximize data locality
7. Leverage compiler optimizations

Latency Budget for HFT Systems:
--------------------------------
- L1 cache access: ~1-2 ns
- L2 cache access: ~3-10 ns
- L3 cache access: ~10-40 ns
- Main memory access: ~60-100 ns
- Context switch: ~1-10 μs
- Network round trip: ~10-500 μs

Target: Sub-microsecond order processing latency
Critical path: Order validation → Market data update → Order submission

================================================================================
2. ALGORITHMIC OPTIMIZATION
================================================================================

2.1 Constant Time Operations
-----------------------------

BEFORE (Inefficient - O(n) search):
```cpp
// Linear search in order book
struct OrderBook {
    std::vector<Order> orders;

    Order* findOrder(uint64_t orderId) {
        for (auto& order : orders) {
            if (order.id == orderId) {
                return &order;
            }
        }
        return nullptr;
    }
};
// Latency: ~50-500 ns depending on position
```

AFTER (Optimized - O(1) hash lookup):
```cpp
// Hash-based order lookup with pre-allocated memory
struct OrderBook {
    // Pre-sized hash map to avoid rehashing
    std::unordered_map<uint64_t, Order*> orderIndex;
    std::array<Order, MAX_ORDERS> orderPool;
    size_t orderCount = 0;

    OrderBook() {
        orderIndex.reserve(MAX_ORDERS);
    }

    Order* findOrder(uint64_t orderId) {
        auto it = orderIndex.find(orderId);
        return (it != orderIndex.end()) ? it->second : nullptr;
    }

    void addOrder(const Order& order) {
        if (orderCount < MAX_ORDERS) {
            orderPool[orderCount] = order;
            orderIndex[order.id] = &orderPool[orderCount];
            ++orderCount;
        }
    }
};
// Latency: ~5-15 ns (3-10x faster)
```

2.2 Lock-Free Algorithms
-------------------------

BEFORE (Mutex-based queue):
```cpp
template<typename T>
class ThreadSafeQueue {
    std::queue<T> queue;
    std::mutex mtx;

public:
    void push(const T& item) {
        std::lock_guard<std::mutex> lock(mtx);
        queue.push(item);
    }

    bool pop(T& item) {
        std::lock_guard<std::mutex> lock(mtx);
        if (queue.empty()) return false;
        item = queue.front();
        queue.pop();
        return true;
    }
};
// Latency: ~50-200 ns (mutex contention)
```

AFTER (Lock-free ring buffer):
```cpp
template<typename T, size_t Size>
class LockFreeRingBuffer {
    static_assert((Size & (Size - 1)) == 0, "Size must be power of 2");

    alignas(64) std::atomic<size_t> writePos{0};
    alignas(64) std::atomic<size_t> readPos{0};
    std::array<T, Size> buffer;

public:
    bool push(const T& item) {
        const size_t currentWrite = writePos.load(std::memory_order_relaxed);
        const size_t nextWrite = (currentWrite + 1) & (Size - 1);

        if (nextWrite == readPos.load(std::memory_order_acquire)) {
            return false; // Full
        }

        buffer[currentWrite] = item;
        writePos.store(nextWrite, std::memory_order_release);
        return true;
    }

    bool pop(T& item) {
        const size_t currentRead = readPos.load(std::memory_order_relaxed);

        if (currentRead == writePos.load(std::memory_order_acquire)) {
            return false; // Empty
        }

        item = buffer[currentRead];
        readPos.store((currentRead + 1) & (Size - 1), std::memory_order_release);
        return true;
    }
};
// Latency: ~5-20 ns (10-40x faster)
```

================================================================================
3. DATA STRUCTURE OPTIMIZATION
================================================================================

3.1 Cache-Friendly Data Structures
-----------------------------------

BEFORE (Pointer-heavy structure):
```cpp
struct PriceLevel {
    double price;
    Order* firstOrder;  // Pointer chasing
};

struct Order {
    uint64_t id;
    uint32_t quantity;
    uint32_t timestamp;
    Order* next;  // Linked list = cache misses
};
// Cache miss rate: ~30-50%
// Latency per level: ~100-200 ns
```

AFTER (Flat, cache-friendly structure):
```cpp
struct alignas(64) PriceLevel {
    double price;
    uint32_t orderStartIdx;
    uint32_t orderCount;
    uint32_t totalQuantity;
    uint32_t padding;  // Cache line alignment

    // Store orders contiguously
    static constexpr size_t MAX_ORDERS_PER_LEVEL = 64;
    std::array<uint32_t, MAX_ORDERS_PER_LEVEL> orderIndices;
};

struct alignas(32) Order {
    uint64_t id;
    uint32_t quantity;
    uint32_t timestamp;
    uint16_t symbolId;
    uint8_t side;
    uint8_t padding;
};

// Global order pool for spatial locality
alignas(64) std::array<Order, MAX_ORDERS> g_orderPool;
// Cache miss rate: ~5-10%
// Latency per level: ~20-40 ns (5x faster)
```

3.2 SOA (Structure of Arrays) vs AOS (Array of Structures)
-----------------------------------------------------------

BEFORE (AOS - poor SIMD utilization):
```cpp
struct MarketData {
    uint32_t symbolId;
    double bidPrice;
    double askPrice;
    uint32_t bidQty;
    uint32_t askQty;
};

std::vector<MarketData> marketData;

// Calculate mid-prices
void calculateMidPrices(std::vector<double>& midPrices) {
    for (size_t i = 0; i < marketData.size(); ++i) {
        midPrices[i] = (marketData[i].bidPrice + marketData[i].askPrice) / 2.0;
    }
}
// No SIMD auto-vectorization due to scattered memory access
// Latency for 1000 symbols: ~5-8 μs
```

AFTER (SOA - SIMD-friendly):
```cpp
struct MarketDataSOA {
    std::vector<uint32_t> symbolIds;
    std::vector<double> bidPrices;
    std::vector<double> askPrices;
    std::vector<uint32_t> bidQtys;
    std::vector<uint32_t> askQtys;

    void reserve(size_t n) {
        symbolIds.reserve(n);
        bidPrices.reserve(n);
        askPrices.reserve(n);
        bidQtys.reserve(n);
        askQtys.reserve(n);
    }
};

// Calculate mid-prices with SIMD
void calculateMidPrices(const MarketDataSOA& data, std::vector<double>& midPrices) {
    midPrices.resize(data.bidPrices.size());

    // Compiler auto-vectorizes this with AVX2/AVX512
    for (size_t i = 0; i < data.bidPrices.size(); ++i) {
        midPrices[i] = (data.bidPrices[i] + data.askPrices[i]) * 0.5;
    }
}
// AVX2: 4 doubles per cycle, AVX512: 8 doubles per cycle
// Latency for 1000 symbols: ~1-2 μs (4-8x faster)
```

================================================================================
4. HOT PATH OPTIMIZATION
================================================================================

4.1 Identifying Hot Paths
--------------------------

Use perf to identify hot functions:
```bash
perf record -g ./trading_engine
perf report --stdio
```

Typical HFT hot paths:
1. Order matching engine (30-40% CPU time)
2. Market data parsing (20-30% CPU time)
3. Risk checks (10-15% CPU time)
4. Order book updates (10-15% CPU time)

4.2 Hot Path Optimization Example
----------------------------------

BEFORE (Generic order validation):
```cpp
class OrderValidator {
public:
    bool validate(const Order& order) {
        // Multiple checks, each with branches
        if (order.price <= 0.0) return false;
        if (order.quantity == 0) return false;
        if (order.quantity > maxOrderSize) return false;
        if (order.symbolId >= symbolCount) return false;

        // Lookup symbol info (cache miss possible)
        const SymbolInfo& info = symbolTable[order.symbolId];
        if (order.price < info.minPrice || order.price > info.maxPrice) {
            return false;
        }

        // Price tick check (division - expensive)
        double remainder = std::fmod(order.price, info.tickSize);
        if (remainder > 1e-9) return false;

        return true;
    }
};
// Latency: ~80-150 ns
// Branch mispredictions: ~15-20%
```

AFTER (Optimized hot path validation):
```cpp
class FastOrderValidator {
    // Pre-computed lookup tables
    alignas(64) std::array<SymbolValidationInfo, MAX_SYMBOLS> symbolInfo;

    struct SymbolValidationInfo {
        double minPrice;
        double maxPrice;
        double invTickSize;  // Pre-computed 1/tickSize
        uint32_t maxQuantity;
    };

public:
    // Use [[likely]] attributes for branch hints
    __attribute__((hot, always_inline))
    bool validate(const Order& order) {
        // Combine multiple checks to reduce branches
        const bool basicValid =
            (order.price > 0.0) &
            (order.quantity > 0) &
            (order.quantity <= maxOrderSize) &
            (order.symbolId < symbolCount);

        if (!basicValid) [[unlikely]] return false;

        // Single cache-line access
        const auto& info = symbolInfo[order.symbolId];

        // Combine price range check
        const bool priceValid =
            (order.price >= info.minPrice) &
            (order.price <= info.maxPrice);

        if (!priceValid) [[unlikely]] return false;

        // Replace division with multiplication
        const double ticks = order.price * info.invTickSize;
        const double roundedTicks = std::round(ticks);
        const bool tickValid = (std::abs(ticks - roundedTicks) < 1e-9);

        return tickValid;
    }
};
// Latency: ~15-30 ns (5x faster)
// Branch mispredictions: ~2-5%
```

================================================================================
5. BRANCH OPTIMIZATION
================================================================================

5.1 Branch Prediction Hints
----------------------------

BEFORE (No branch hints):
```cpp
void processOrder(const Order& order) {
    if (order.type == OrderType::MARKET) {
        processMarketOrder(order);
    } else if (order.type == OrderType::LIMIT) {
        processLimitOrder(order);
    } else {
        processOtherOrder(order);
    }
}
// Branch misprediction rate: ~15-25%
```

AFTER (With branch hints):
```cpp
void processOrder(const Order& order) {
    // LIMIT orders are 90% of traffic in most HFT systems
    if (order.type == OrderType::LIMIT) [[likely]] {
        processLimitOrder(order);
    } else if (order.type == OrderType::MARKET) [[unlikely]] {
        processMarketOrder(order);
    } else [[unlikely]] {
        processOtherOrder(order);
    }
}
// Branch misprediction rate: ~2-5%
// Improvement: ~10-20 ns saved per order
```

5.2 Branch Elimination
-----------------------

BEFORE (Branchy code):
```cpp
uint32_t calculateFee(const Order& order, const User& user) {
    uint32_t baseFee = order.quantity * feePerShare;

    if (user.vipLevel == 1) {
        return baseFee * 90 / 100;  // 10% discount
    } else if (user.vipLevel == 2) {
        return baseFee * 80 / 100;  // 20% discount
    } else if (user.vipLevel == 3) {
        return baseFee * 70 / 100;  // 30% discount
    } else {
        return baseFee;
    }
}
// 3 unpredictable branches
// Latency: ~15-25 ns
```

AFTER (Branch-free with lookup table):
```cpp
class FeeCalculator {
    static constexpr std::array<uint32_t, 4> discountMultipliers = {
        100, 90, 80, 70  // VIP levels 0-3
    };

public:
    __attribute__((always_inline))
    uint32_t calculateFee(const Order& order, const User& user) {
        uint32_t baseFee = order.quantity * feePerShare;
        uint32_t vipLevel = std::min(user.vipLevel, 3u);
        return (baseFee * discountMultipliers[vipLevel]) / 100;
    }
};
// Zero branches
// Latency: ~3-8 ns (3-5x faster)
```

================================================================================
6. LOOP OPTIMIZATION
================================================================================

6.1 Loop Unrolling
------------------

BEFORE (Simple loop):
```cpp
void updatePrices(double* prices, size_t count, double adjustment) {
    for (size_t i = 0; i < count; ++i) {
        prices[i] *= adjustment;
    }
}
// Loop overhead: increment, compare, branch per iteration
```

AFTER (Manual unrolling + SIMD):
```cpp
void updatePrices(double* prices, size_t count, double adjustment) {
    const size_t unrollFactor = 8;
    const size_t unrolledCount = count - (count % unrollFactor);

    // Vectorized unrolled loop
    const __m256d adjVec = _mm256_set1_pd(adjustment);

    for (size_t i = 0; i < unrolledCount; i += unrollFactor) {
        __m256d p1 = _mm256_loadu_pd(&prices[i]);
        __m256d p2 = _mm256_loadu_pd(&prices[i + 4]);

        p1 = _mm256_mul_pd(p1, adjVec);
        p2 = _mm256_mul_pd(p2, adjVec);

        _mm256_storeu_pd(&prices[i], p1);
        _mm256_storeu_pd(&prices[i + 4], p2);
    }

    // Handle remainder
    for (size_t i = unrolledCount; i < count; ++i) {
        prices[i] *= adjustment;
    }
}
// Processes 8 doubles per iteration
// 5-8x faster for large arrays
```

6.2 Loop Invariant Code Motion
-------------------------------

BEFORE (Redundant calculations):
```cpp
void calculateSpread(const std::vector<Quote>& quotes,
                     std::vector<double>& spreads) {
    for (size_t i = 0; i < quotes.size(); ++i) {
        // tickSize is constant but fetched every iteration
        double tickSize = symbolTable[quotes[i].symbolId].tickSize;
        spreads[i] = (quotes[i].ask - quotes[i].bid) / tickSize;
    }
}
```

AFTER (Hoisted invariants):
```cpp
void calculateSpread(const std::vector<Quote>& quotes,
                     std::vector<double>& spreads,
                     uint32_t symbolId) {
    // Move invariant outside loop
    const double invTickSize = 1.0 / symbolTable[symbolId].tickSize;

    spreads.resize(quotes.size());

    for (size_t i = 0; i < quotes.size(); ++i) {
        spreads[i] = (quotes[i].ask - quotes[i].bid) * invTickSize;
    }
}
// Eliminates table lookup and division from loop
// 2-3x faster
```

================================================================================
7. FUNCTION INLINING
================================================================================

7.1 Force Inlining Critical Functions
--------------------------------------

BEFORE (Function call overhead):
```cpp
class PriceCalculator {
public:
    double calculateMidPrice(double bid, double ask) {
        return (bid + ask) * 0.5;
    }

    void processTick(const Tick& tick) {
        double mid = calculateMidPrice(tick.bid, tick.ask);
        // Process mid price...
    }
};
// Function call overhead: ~2-5 ns per call
```

AFTER (Forced inlining):
```cpp
class PriceCalculator {
public:
    __attribute__((always_inline))
    inline double calculateMidPrice(double bid, double ask) {
        return (bid + ask) * 0.5;
    }

    void processTick(const Tick& tick) {
        double mid = calculateMidPrice(tick.bid, tick.ask);
        // Mid price calculation inlined - zero call overhead
    }
};
// No function call overhead
// Latency saved: ~2-5 ns per call
```

7.2 Prevent Unwanted Inlining
------------------------------

```cpp
// Cold path functions should NOT be inlined
__attribute__((noinline, cold))
void handleError(const char* msg) {
    // Error handling code
    logError(msg);
    sendAlert(msg);
}

// Hot path stays compact and cache-friendly
void processOrder(const Order& order) {
    if (validateOrder(order)) [[likely]] {
        // Hot path - keep code compact
        matchOrder(order);
    } else [[unlikely]] {
        handleError("Invalid order");  // Not inlined
    }
}
```

================================================================================
8. TEMPLATE METAPROGRAMMING
================================================================================

8.1 Compile-Time Computation
-----------------------------

BEFORE (Runtime calculation):
```cpp
class OrderSizeValidator {
    std::array<uint32_t, 10> allowedSizes;

public:
    OrderSizeValidator() {
        // Calculated at runtime every time
        for (int i = 0; i < 10; ++i) {
            allowedSizes[i] = 100 * (1 << i);  // 100, 200, 400, 800...
        }
    }

    bool isValidSize(uint32_t size) {
        return std::find(allowedSizes.begin(), allowedSizes.end(), size)
               != allowedSizes.end();
    }
};
```

AFTER (Compile-time generation):
```cpp
template<size_t... Is>
constexpr auto generateSizes(std::index_sequence<Is...>) {
    return std::array<uint32_t, sizeof...(Is)>{(100 * (1 << Is))...};
}

class OrderSizeValidator {
    static constexpr auto allowedSizes =
        generateSizes(std::make_index_sequence<10>{});

public:
    constexpr bool isValidSize(uint32_t size) {
        for (auto allowed : allowedSizes) {
            if (size == allowed) return true;
        }
        return false;
    }
};
// Zero runtime initialization cost
// Lookup can be further optimized with binary search or hash
```

8.2 Template Specialization for Hot Paths
------------------------------------------

```cpp
// Generic template
template<typename T>
class Calculator {
public:
    T calculate(T a, T b) {
        return a + b;  // Generic addition
    }
};

// Specialized for double with FMA (Fused Multiply-Add)
template<>
class Calculator<double> {
public:
    __attribute__((always_inline))
    double calculate(double a, double b) {
        // Use FMA for better precision and performance
        return std::fma(a, 1.0, b);
    }
};

// Specialized for fixed-point arithmetic
template<>
class Calculator<int64_t> {
public:
    __attribute__((always_inline))
    int64_t calculate(int64_t a, int64_t b) {
        // Direct addition for integers
        return a + b;
    }
};
```

================================================================================
9. BENCHMARKING METHODS
================================================================================

9.1 Microbenchmarking Framework
--------------------------------

```cpp
#include <chrono>
#include <algorithm>
#include <numeric>

class Benchmark {
    using Clock = std::chrono::high_resolution_clock;
    using Duration = std::chrono::nanoseconds;

public:
    template<typename Func>
    static void measure(const char* name, Func&& func, size_t iterations = 1000000) {
        std::vector<double> latencies;
        latencies.reserve(iterations);

        // Warm-up
        for (size_t i = 0; i < 1000; ++i) {
            func();
        }

        // Actual measurement
        for (size_t i = 0; i < iterations; ++i) {
            auto start = Clock::now();
            func();
            auto end = Clock::now();

            auto duration = std::chrono::duration_cast<Duration>(end - start);
            latencies.push_back(duration.count());
        }

        // Calculate statistics
        std::sort(latencies.begin(), latencies.end());

        double mean = std::accumulate(latencies.begin(), latencies.end(), 0.0)
                      / latencies.size();
        double p50 = latencies[latencies.size() * 50 / 100];
        double p99 = latencies[latencies.size() * 99 / 100];
        double p999 = latencies[latencies.size() * 999 / 1000];
        double min = latencies.front();
        double max = latencies.back();

        printf("%s Results (ns):\n", name);
        printf("  Mean: %.2f\n", mean);
        printf("  P50:  %.2f\n", p50);
        printf("  P99:  %.2f\n", p99);
        printf("  P999: %.2f\n", p999);
        printf("  Min:  %.2f\n", min);
        printf("  Max:  %.2f\n", max);
    }
};

// Usage:
void benchmarkOrderValidation() {
    Order testOrder = {/* ... */};
    FastOrderValidator validator;

    Benchmark::measure("Order Validation", [&]() {
        volatile bool result = validator.validate(testOrder);
    });
}
```

9.2 RDTSC-based High-Precision Timing
--------------------------------------

```cpp
#include <x86intrin.h>

class TSCBenchmark {
    static uint64_t rdtsc() {
        unsigned int aux;
        return __rdtsc();
    }

    static double tscToNanoseconds(uint64_t cycles) {
        // Calibrate TSC frequency (assuming 2.4 GHz CPU)
        static constexpr double TSC_FREQ = 2.4e9;
        return cycles * (1e9 / TSC_FREQ);
    }

public:
    template<typename Func>
    static double measureCycles(Func&& func) {
        // Serialize instruction pipeline
        _mm_lfence();
        uint64_t start = rdtsc();
        _mm_lfence();

        func();

        _mm_lfence();
        uint64_t end = rdtsc();
        _mm_lfence();

        return tscToNanoseconds(end - start);
    }
};

// Usage:
double latency = TSCBenchmark::measureCycles([&]() {
    validator.validate(order);
});
printf("Latency: %.2f ns\n", latency);
```

================================================================================
10. MEASUREMENT TECHNIQUES
================================================================================

10.1 Performance Counter Monitoring
------------------------------------

```cpp
#include <linux/perf_event.h>
#include <sys/syscall.h>
#include <unistd.h>

class PerfCounter {
    int fd;

public:
    PerfCounter(uint32_t type, uint64_t config) {
        struct perf_event_attr pe = {};
        pe.type = type;
        pe.size = sizeof(struct perf_event_attr);
        pe.config = config;
        pe.disabled = 1;
        pe.exclude_kernel = 1;
        pe.exclude_hv = 1;

        fd = syscall(__NR_perf_event_open, &pe, 0, -1, -1, 0);
    }

    void start() {
        ioctl(fd, PERF_EVENT_IOC_RESET, 0);
        ioctl(fd, PERF_EVENT_IOC_ENABLE, 0);
    }

    uint64_t stop() {
        ioctl(fd, PERF_EVENT_IOC_DISABLE, 0);
        uint64_t count;
        read(fd, &count, sizeof(count));
        return count;
    }

    ~PerfCounter() { close(fd); }
};

// Measure cache misses
void measureCacheMisses() {
    PerfCounter l1Misses(PERF_TYPE_HW_CACHE,
                         (PERF_COUNT_HW_CACHE_L1D) |
                         (PERF_COUNT_HW_CACHE_OP_READ << 8) |
                         (PERF_COUNT_HW_CACHE_RESULT_MISS << 16));

    l1Misses.start();

    // Run your code here
    processOrders();

    uint64_t misses = l1Misses.stop();
    printf("L1 cache misses: %lu\n", misses);
}
```

10.2 IPC (Instructions Per Cycle) Measurement
----------------------------------------------

```bash
# Measure IPC using perf
perf stat -e cycles,instructions,cache-misses,branch-misses ./trading_engine

# Output example:
# Performance counter stats for './trading_engine':
#
#     2,345,678,901      cycles                    #    2.400 GHz
#     4,567,890,123      instructions              #    1.95  insn per cycle
#        12,345,678      cache-misses              #    0.53% of all cache refs
#         1,234,567      branch-misses             #    0.89% of all branches
```

Target IPC for HFT code: 2.5-3.5 (modern CPUs can achieve 4-6 with perfect code)

================================================================================
11. EXPECTED IMPROVEMENTS
================================================================================

Optimization Category          | Latency Reduction | Throughput Gain
-------------------------------|-------------------|------------------
Algorithmic (O(n) to O(1))     | 80-95%           | 10-50x
Lock-free data structures      | 70-90%           | 10-40x
Cache optimization             | 60-80%           | 3-8x
Branch optimization            | 30-50%           | 1.5-2x
SIMD/vectorization            | 50-75%           | 4-8x
Function inlining              | 10-20%           | 1.1-1.3x
Loop unrolling                 | 20-40%           | 1.5-3x
Template metaprogramming       | 15-30%           | 1.2-1.5x

Combined Effect (All techniques): 95-99% latency reduction, 50-200x throughput gain

Real-World HFT Example:
-----------------------
Order Processing Pipeline:

BEFORE optimization:
- Order validation: 150 ns
- Risk check: 200 ns
- Order matching: 500 ns
- Market data update: 300 ns
Total: ~1,150 ns (1.15 μs)

AFTER optimization:
- Order validation: 20 ns (7.5x faster)
- Risk check: 30 ns (6.7x faster)
- Order matching: 80 ns (6.25x faster)
- Market data update: 50 ns (6x faster)
Total: ~180 ns (6.4x faster overall)

Improvement: 970 ns saved per order
At 1M orders/day: 970 million ns = 0.97 seconds saved
Competitive advantage: Orders execute 6.4x faster than competitors

================================================================================
COMPILER FLAGS FOR CODE OPTIMIZATION
================================================================================

Recommended GCC/Clang flags:
```bash
-O3                          # Maximum optimization
-march=native                # Use all CPU features
-mtune=native                # Tune for current CPU
-flto                        # Link-time optimization
-ffast-math                  # Aggressive floating-point optimization
-funroll-loops               # Automatic loop unrolling
-finline-functions           # Aggressive inlining
-fomit-frame-pointer         # Remove frame pointer for more registers
-fno-exceptions              # Disable exceptions for performance
-fno-rtti                    # Disable RTTI
-fprofile-generate           # For PGO (first pass)
-fprofile-use                # For PGO (second pass)
```

Example compilation:
```bash
g++ -O3 -march=native -mtune=native -flto -ffast-math \
    -funroll-loops -finline-functions -fomit-frame-pointer \
    -fno-exceptions -fno-rtti -std=c++20 \
    -o trading_engine trading_engine.cpp
```

================================================================================
END OF CODE OPTIMIZATION GUIDE
================================================================================
