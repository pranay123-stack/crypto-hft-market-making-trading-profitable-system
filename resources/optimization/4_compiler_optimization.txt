================================================================================
HFT COMPILER OPTIMIZATION TECHNIQUES
================================================================================

TABLE OF CONTENTS
-----------------
1. Theory and Fundamental Concepts
2. Optimization Flags and Levels
3. Profile-Guided Optimization (PGO)
4. Link-Time Optimization (LTO)
5. Auto-Vectorization
6. Interprocedural Optimization
7. Compiler-Specific Features
8. Inline Assembly Optimization
9. Benchmarking Compiler Settings
10. Measurement Techniques
11. Expected Improvements

================================================================================
1. THEORY AND FUNDAMENTAL CONCEPTS
================================================================================

Compiler Optimization Principles:
----------------------------------
1. Dead Code Elimination: Remove unused code
2. Constant Folding: Evaluate constants at compile time
3. Loop Optimization: Unrolling, fusion, interchange
4. Function Inlining: Eliminate call overhead
5. Register Allocation: Minimize memory accesses
6. Instruction Scheduling: Maximize pipeline utilization
7. Vectorization: Use SIMD instructions

Optimization Levels (GCC/Clang):
---------------------------------
-O0: No optimization (debug builds)
     - Compile time: Fast
     - Runtime: Slow (2-10x slower than -O3)
     - Binary size: Large
     - Debuggability: Excellent

-O1: Basic optimization
     - Compile time: Fast
     - Runtime: Moderate
     - Binary size: Moderate
     - Focus: Quick compilation with some optimization

-O2: Recommended optimization
     - Compile time: Moderate
     - Runtime: Fast (1.3-1.8x faster than -O1)
     - Binary size: Moderate
     - Focus: Balance between compile time and performance

-O3: Aggressive optimization
     - Compile time: Slow
     - Runtime: Fastest (1.1-1.5x faster than -O2)
     - Binary size: Large (due to inlining)
     - Focus: Maximum performance

-Os: Optimize for size
     - Binary size: Small
     - Runtime: Moderate (slower than -O2)

-Ofast: -O3 + fast-math (non-standards compliant)
     - Runtime: Fastest for floating-point heavy code
     - Warning: May violate IEEE 754 standards

HFT Compiler Strategy:
----------------------
Use -O3 as baseline, then add:
- Profile-Guided Optimization (PGO)
- Link-Time Optimization (LTO)
- Architecture-specific flags (-march=native)
- Fast-math for non-critical paths
- Custom optimization flags

Expected Performance Gain:
-O0 to -O3: 5-10x faster
-O3 to -O3 + PGO + LTO: 1.5-3x faster
Total: 7.5-30x faster with full optimization

================================================================================
2. OPTIMIZATION FLAGS AND LEVELS
================================================================================

2.1 Complete GCC Optimization Flags
------------------------------------

```bash
# Basic optimization flags
GCC_FLAGS="-O3"                          # Aggressive optimization

# Architecture-specific
GCC_FLAGS+=" -march=native"              # Use all CPU features
GCC_FLAGS+=" -mtune=native"              # Tune for current CPU

# Function optimization
GCC_FLAGS+=" -finline-functions"         # Inline all suitable functions
GCC_FLAGS+=" -finline-limit=1000"        # Increase inline limit
GCC_FLAGS+=" -fomit-frame-pointer"       # Free up register

# Loop optimization
GCC_FLAGS+=" -funroll-loops"             # Unroll loops
GCC_FLAGS+=" -ftree-vectorize"           # Enable vectorization
GCC_FLAGS+=" -floop-parallelize-all"     # Parallelize loops

# Link-time optimization
GCC_FLAGS+=" -flto"                      # Enable LTO
GCC_FLAGS+=" -fuse-linker-plugin"        # Use LTO plugin

# Fast math (use with caution)
GCC_FLAGS+=" -ffast-math"                # Aggressive FP optimization
GCC_FLAGS+=" -funsafe-math-optimizations"

# Size vs speed trade-offs
GCC_FLAGS+=" -falign-functions=32"       # Align functions to 32 bytes
GCC_FLAGS+=" -falign-loops=32"           # Align loops

# Debugging and profiling
GCC_FLAGS+=" -g3"                        # Debug info (for perf)
GCC_FLAGS+=" -fno-omit-frame-pointer"    # Keep frame pointer for profiling

# C++ specific
GCC_FLAGS+=" -fno-exceptions"            # Disable exceptions
GCC_FLAGS+=" -fno-rtti"                  # Disable RTTI
GCC_FLAGS+=" -fno-threadsafe-statics"    # Disable thread-safe statics

# Security features (disable for max performance)
GCC_FLAGS+=" -fno-stack-protector"       # Disable stack canaries
GCC_FLAGS+=" -fno-plt"                   # Disable PLT indirection

# Compilation command
g++ $GCC_FLAGS -std=c++20 -o trading_engine trading_engine.cpp
```

2.2 Clang-Specific Optimization Flags
--------------------------------------

```bash
# Clang optimization flags
CLANG_FLAGS="-O3"

# Architecture
CLANG_FLAGS+=" -march=native"
CLANG_FLAGS+=" -mtune=native"

# Vectorization
CLANG_FLAGS+=" -fvectorize"
CLANG_FLAGS+=" -fslp-vectorize"          # Superword-level parallelization

# Polly (advanced loop optimizer)
CLANG_FLAGS+=" -mllvm -polly"
CLANG_FLAGS+=" -mllvm -polly-vectorizer=stripmine"

# Link-time optimization
CLANG_FLAGS+=" -flto=thin"               # Thin LTO (faster compilation)

# Profile-guided optimization
CLANG_FLAGS+=" -fprofile-generate"       # First pass
# CLANG_FLAGS+=" -fprofile-use=default.profdata"  # Second pass

# Math optimization
CLANG_FLAGS+=" -ffast-math"
CLANG_FLAGS+=" -ffp-contract=fast"       # Allow FMA contractions

# Compilation
clang++ $CLANG_FLAGS -std=c++20 -o trading_engine trading_engine.cpp
```

2.3 Optimization Flags Comparison
----------------------------------

BEFORE (No optimization):
```bash
g++ -O0 -o trading_engine trading_engine.cpp

# Characteristics:
# - Compile time: 5 seconds
# - Binary size: 2.5 MB
# - Order processing: 15,000 ns
# - IPC: 0.8
```

AFTER (Full optimization):
```bash
g++ -O3 -march=native -mtune=native -flto -ffast-math \
    -funroll-loops -finline-functions -fomit-frame-pointer \
    -fno-exceptions -fno-rtti \
    -o trading_engine trading_engine.cpp

# Characteristics:
# - Compile time: 45 seconds
# - Binary size: 3.8 MB
# - Order processing: 1,800 ns (8.3x faster)
# - IPC: 2.4 (3x better)
```

2.4 Architecture-Specific Flags
--------------------------------

```bash
# Intel Skylake-X
ARCH_FLAGS="-march=skylake-avx512"
ARCH_FLAGS+=" -mavx512f -mavx512cd -mavx512vl -mavx512dq -mavx512bw"

# AMD Zen 3
ARCH_FLAGS="-march=znver3"
ARCH_FLAGS+=" -mavx2 -mfma -mbmi2"

# Generic x86-64 with AVX2
ARCH_FLAGS="-march=haswell"
ARCH_FLAGS+=" -mavx2 -mfma"

# Determine architecture automatically
ARCH_FLAGS="-march=native -mtune=native"
```

================================================================================
3. PROFILE-GUIDED OPTIMIZATION (PGO)
================================================================================

3.1 PGO Workflow
----------------

PGO optimizes based on actual runtime behavior:
1. Instrument binary to collect profiling data
2. Run binary with representative workload
3. Recompile with profile data

Expected improvement: 10-30% over -O3

3.2 GCC PGO Implementation
---------------------------

Step 1: Compile with instrumentation
```bash
# Generate instrumented binary
g++ -O3 -march=native -fprofile-generate -fprofile-dir=./prof \
    -o trading_engine_instrumented trading_engine.cpp

# Creates instrumented binary that will generate .gcda files
```

Step 2: Run with representative workload
```bash
# Run typical trading workload
./trading_engine_instrumented --replay market_data.pcap

# This generates profiling data in ./prof/*.gcda
```

Step 3: Recompile with profile data
```bash
# Rebuild with profile-guided optimization
g++ -O3 -march=native -fprofile-use -fprofile-dir=./prof \
    -o trading_engine_optimized trading_engine.cpp

# Binary is now optimized based on actual runtime behavior
```

3.3 Clang PGO Implementation
-----------------------------

```bash
# Step 1: Generate profiling instrumentation
clang++ -O3 -march=native -fprofile-instr-generate \
        -o trading_engine_instrumented trading_engine.cpp

# Step 2: Run and collect profile
LLVM_PROFILE_FILE="trading_engine.profraw" \
    ./trading_engine_instrumented --replay market_data.pcap

# Step 3: Convert raw profile to indexed format
llvm-profdata merge -output=trading_engine.profdata trading_engine.profraw

# Step 4: Compile with profile data
clang++ -O3 -march=native -fprofile-instr-use=trading_engine.profdata \
        -o trading_engine_optimized trading_engine.cpp
```

3.4 PGO Code Example
--------------------

BEFORE PGO:
```cpp
// Compiler doesn't know which branch is more likely
void processOrder(const Order& order) {
    if (order.type == OrderType::LIMIT) {
        processLimitOrder(order);
    } else if (order.type == OrderType::MARKET) {
        processMarketOrder(order);
    } else {
        processOtherOrder(order);
    }
}

// Generated assembly: Equal treatment of all branches
// Branch prediction: Poor (50% miss rate)
```

AFTER PGO:
```cpp
// Same code, but compiler sees profile data:
// LIMIT orders: 85% of all orders
// MARKET orders: 12% of all orders
// Other orders: 3% of all orders

// Compiler optimizes for the common case:
// - LIMIT branch is predicted taken
// - Code layout optimized for LIMIT path
// - Other branches moved to "cold" section

// Generated assembly: LIMIT path is inline, others are out-of-line
// Branch prediction: Excellent (95% hit rate)
// Improvement: 20-30% faster order processing
```

3.5 AutoFDO (Automatic Feedback-Directed Optimization)
-------------------------------------------------------

AutoFDO uses sampling profiler (perf) instead of instrumentation:

```bash
# Step 1: Compile with debug info
g++ -O3 -march=native -g -o trading_engine trading_engine.cpp

# Step 2: Profile with perf (no instrumentation overhead)
perf record -b -e cycles:u ./trading_engine --replay market_data.pcap

# Step 3: Convert perf.data to AutoFDO format
create_gcov --binary=trading_engine --profile=perf.data \
            --gcov=trading_engine.afdo

# Step 4: Compile with AutoFDO profile
g++ -O3 -march=native -fauto-profile=trading_engine.afdo \
    -o trading_engine_optimized trading_engine.cpp
```

Advantage: No instrumentation overhead, can profile production binaries
Disadvantage: Less accurate than traditional PGO

================================================================================
4. LINK-TIME OPTIMIZATION (LTO)
================================================================================

4.1 LTO Theory
--------------

Traditional compilation:
```
source.cpp → compile → object.o → link → binary
                 ↓
           Per-file optimization only
```

LTO compilation:
```
source.cpp → compile → IR → link with optimization → binary
                              ↓
                    Whole-program optimization
```

Benefits:
- Cross-file function inlining
- Better dead code elimination
- More accurate escape analysis
- Improved register allocation

Expected improvement: 5-15% over -O3

4.2 GCC LTO
-----------

```bash
# Compile with LTO
g++ -O3 -march=native -flto -fuse-linker-plugin \
    -c order_processor.cpp -o order_processor.o

g++ -O3 -march=native -flto -fuse-linker-plugin \
    -c market_data.cpp -o market_data.o

# Link with LTO
g++ -O3 -march=native -flto -fuse-linker-plugin \
    order_processor.o market_data.o -o trading_engine

# Specify LTO jobs for faster compilation
g++ -O3 -march=native -flto=8 -fuse-linker-plugin \
    order_processor.o market_data.o -o trading_engine
```

4.3 Clang Thin LTO
------------------

Thin LTO is faster than regular LTO with similar benefits:

```bash
# Compile with Thin LTO
clang++ -O3 -march=native -flto=thin -c order_processor.cpp
clang++ -O3 -march=native -flto=thin -c market_data.cpp

# Link
clang++ -O3 -march=native -flto=thin \
        order_processor.o market_data.o -o trading_engine

# Advantages of Thin LTO:
# - Parallelizable (faster compilation)
# - Lower memory usage
# - Incremental compilation support
```

4.4 LTO Code Example
--------------------

BEFORE LTO (separate compilation units):

File: order_processor.cpp
```cpp
// Compiler sees only this file
inline void validatePrice(double price) {
    if (price <= 0.0) throw std::runtime_error("Invalid price");
}

void processOrder(const Order& order) {
    validatePrice(order.price);  // Not inlined across files
    // Process order...
}
```

File: main.cpp
```cpp
// Compiler sees only this file
extern void processOrder(const Order& order);

int main() {
    Order order = {/* ... */};
    processOrder(order);  // Call overhead
    return 0;
}

// Without LTO:
// - validatePrice not inlined in processOrder (header not visible)
// - processOrder not inlined in main (separate compilation unit)
// - Call overhead: ~5-10 ns per call
```

AFTER LTO:
```cpp
// Same source code, but with -flto:
// Compiler sees entire program

// After LTO optimization:
int main() {
    Order order = {/* ... */};

    // Both validatePrice and processOrder are inlined
    if (order.price <= 0.0) throw std::runtime_error("Invalid price");
    // Inlined order processing code...

    return 0;
}

// With LTO:
// - validatePrice inlined into processOrder
// - processOrder inlined into main
// - No call overhead
// - Improvement: 10-20% faster
```

================================================================================
5. AUTO-VECTORIZATION
================================================================================

5.1 Vectorization Fundamentals
-------------------------------

SIMD (Single Instruction Multiple Data):
- SSE: 128-bit (2 doubles, 4 floats, 4 ints)
- AVX: 256-bit (4 doubles, 8 floats, 8 ints)
- AVX-512: 512-bit (8 doubles, 16 floats, 16 ints)

Performance gain: 2-8x depending on SIMD width

5.2 Enabling Auto-Vectorization
--------------------------------

```bash
# GCC vectorization flags
GCC_VEC_FLAGS="-O3"
GCC_VEC_FLAGS+=" -ftree-vectorize"           # Enable vectorization
GCC_VEC_FLAGS+=" -fopt-info-vec-optimized"   # Report vectorized loops
GCC_VEC_FLAGS+=" -fopt-info-vec-missed"      # Report failed vectorization
GCC_VEC_FLAGS+=" -mavx2"                     # Enable AVX2
GCC_VEC_FLAGS+=" -mfma"                      # Enable FMA

# Clang vectorization flags
CLANG_VEC_FLAGS="-O3"
CLANG_VEC_FLAGS+=" -fvectorize"
CLANG_VEC_FLAGS+=" -Rpass=loop-vectorize"    # Report vectorized loops
CLANG_VEC_FLAGS+=" -Rpass-missed=loop-vectorize"  # Report failures
CLANG_VEC_FLAGS+=" -mavx2 -mfma"

# Compile and see vectorization report
g++ $GCC_VEC_FLAGS -c market_data.cpp
```

5.3 Vectorization Examples
---------------------------

Example 1: Simple Array Processing

BEFORE (Scalar code):
```cpp
void updatePrices(double* prices, size_t count, double factor) {
    for (size_t i = 0; i < count; ++i) {
        prices[i] *= factor;
    }
}

// Without vectorization:
// - Processes 1 double per iteration
// - Latency per element: ~3-5 cycles
// - Throughput for 1000 elements: 3000-5000 cycles
```

AFTER (Auto-vectorized with AVX2):
```cpp
// Same source code compiled with -O3 -mavx2

// Compiler generates:
// vmovupd ymm0, [prices + i]      ; Load 4 doubles
// vmulpd ymm0, ymm0, ymm1          ; Multiply 4 doubles
// vmovupd [prices + i], ymm0       ; Store 4 doubles
// add i, 32                        ; Increment by 4*8 bytes

// With AVX2 vectorization:
// - Processes 4 doubles per iteration
// - Latency per 4 elements: ~3-5 cycles
// - Throughput for 1000 elements: 750-1250 cycles (4x faster)
```

Example 2: Reduction Operations

```cpp
// Calculate total volume
double calculateTotalVolume(const std::vector<Order>& orders) {
    double total = 0.0;

    // Compiler can vectorize with reduction
    #pragma omp simd reduction(+:total)
    for (const auto& order : orders) {
        total += order.quantity * order.price;
    }

    return total;
}

// Auto-vectorized with AVX2:
// - Accumulates 4 sums in parallel
// - Horizontal add at the end
// - 3-4x faster than scalar
```

Example 3: Structure of Arrays (SOA) Vectorization

```cpp
struct MarketDataSOA {
    std::vector<double> bidPrices;
    std::vector<double> askPrices;
    std::vector<double> midPrices;
};

// Vectorization-friendly code
void calculateMidPrices(MarketDataSOA& data) {
    const size_t n = data.bidPrices.size();

    // Compiler auto-vectorizes this loop
    for (size_t i = 0; i < n; ++i) {
        data.midPrices[i] = (data.bidPrices[i] + data.askPrices[i]) * 0.5;
    }
}

// Assembly with AVX2:
// Loop processes 4 elements per iteration:
// vmovupd ymm0, [bidPrices + i]
// vmovupd ymm1, [askPrices + i]
// vaddpd ymm0, ymm0, ymm1
// vmulpd ymm0, ymm0, [0.5, 0.5, 0.5, 0.5]
// vmovupd [midPrices + i], ymm0
```

5.4 Vectorization Hints and Pragmas
------------------------------------

```cpp
// OpenMP SIMD hints
void processData(double* data, size_t n) {
    #pragma omp simd aligned(data:32) simdlen(4)
    for (size_t i = 0; i < n; ++i) {
        data[i] = data[i] * 2.0 + 1.0;
    }
}

// GCC vector extensions
typedef double v4df __attribute__((vector_size(32)));  // AVX2

void vectorAdd(const double* a, const double* b, double* c, size_t n) {
    const v4df* va = reinterpret_cast<const v4df*>(a);
    const v4df* vb = reinterpret_cast<const v4df*>(b);
    v4df* vc = reinterpret_cast<v4df*>(c);

    size_t vn = n / 4;
    for (size_t i = 0; i < vn; ++i) {
        vc[i] = va[i] + vb[i];
    }
}

// Alignment hints
void processAlignedData(double* data, size_t n) {
    // Tell compiler data is 32-byte aligned
    __builtin_assume_aligned(data, 32);

    for (size_t i = 0; i < n; ++i) {
        data[i] *= 2.0;
    }
}
```

5.5 Preventing Vectorization Issues
------------------------------------

Common vectorization blockers:

```cpp
// BAD: Data dependencies prevent vectorization
void badLoop(int* data, size_t n) {
    for (size_t i = 1; i < n; ++i) {
        data[i] = data[i-1] + 1;  // Loop-carried dependency
    }
    // Cannot vectorize: data[i] depends on data[i-1]
}

// GOOD: No dependencies
void goodLoop(int* data, size_t n) {
    for (size_t i = 0; i < n; ++i) {
        data[i] = data[i] * 2 + 1;  // Independent operations
    }
    // Can vectorize: each iteration is independent
}

// BAD: Function calls prevent vectorization
double externalFunction(double x);

void badLoop2(double* data, size_t n) {
    for (size_t i = 0; i < n; ++i) {
        data[i] = externalFunction(data[i]);  // Unknown side effects
    }
}

// GOOD: Inline or use vectorizable functions
inline double vectorizableFunction(double x) {
    return x * x + 2.0 * x + 1.0;
}

void goodLoop2(double* data, size_t n) {
    for (size_t i = 0; i < n; ++i) {
        data[i] = vectorizableFunction(data[i]);
    }
}
```

================================================================================
6. INTERPROCEDURAL OPTIMIZATION
================================================================================

6.1 Whole Program Optimization
-------------------------------

```bash
# GCC interprocedural optimization
g++ -O3 -march=native -fipa-pta -fipa-icf -fwhole-program \
    -o trading_engine *.cpp

# Flags explanation:
# -fipa-pta: Interprocedural pointer analysis
# -fipa-icf: Identical code folding
# -fwhole-program: Assume this is the whole program
```

6.2 Devirtualization
--------------------

```cpp
// Virtual function calls are expensive (indirect branch)
class OrderProcessor {
public:
    virtual void processOrder(const Order& order) = 0;
};

class LimitOrderProcessor : public OrderProcessor {
public:
    void processOrder(const Order& order) override {
        // Process limit order
    }
};

// Usage
void handleOrders(OrderProcessor* processor, const std::vector<Order>& orders) {
    for (const auto& order : orders) {
        processor->processOrder(order);  // Virtual call: ~5-10 ns overhead
    }
}

// With LTO and whole-program optimization:
// Compiler can devirtualize if it knows the exact type:
void handleOrders() {
    LimitOrderProcessor processor;
    std::vector<Order> orders = getOrders();

    // Compiler devirtualizes this to direct call
    for (const auto& order : orders) {
        processor.processOrder(order);  // Direct call: ~1 ns overhead
    }
}

// Result: 5-10x faster due to devirtualization
```

================================================================================
7. COMPILER-SPECIFIC FEATURES
================================================================================

7.1 GCC Specific Optimizations
-------------------------------

```cpp
// Function attributes
__attribute__((hot))
void criticalPath() {
    // Compiler optimizes aggressively for performance
}

__attribute__((cold))
void errorPath() {
    // Compiler optimizes for size, unlikely to execute
}

__attribute__((const))
double pureFunction(double x) {
    // Function has no side effects, can be optimized aggressively
    return x * x;
}

__attribute__((pure))
double pureFunctionWithGlobals(double x) {
    // Function only reads globals, no writes
    return x * globalConstant;
}

__attribute__((always_inline))
inline void mustInline() {
    // Force inlining regardless of optimization level
}

__attribute__((flatten))
void flattenFunction() {
    // Inline all function calls within this function
}

__attribute__((target("avx2,fma")))
void avx2Function() {
    // Use AVX2 instructions for this function only
}

// Builtin functions
inline bool likely(bool condition) {
    return __builtin_expect(condition, 1);
}

inline bool unlikely(bool condition) {
    return __builtin_expect(condition, 0);
}

// Prefetching
void prefetchData(const void* addr) {
    __builtin_prefetch(addr, 0, 3);  // Prefetch for read, high temporal locality
}
```

7.2 Clang Specific Optimizations
---------------------------------

```cpp
// Clang attributes
[[clang::always_inline]]
inline void mustInline() {
    // Force inlining
}

[[clang::noinline]]
void neverInline() {
    // Prevent inlining
}

// Loop hints
void optimizedLoop(double* data, size_t n) {
    #pragma clang loop vectorize(enable) interleave(enable) unroll(enable)
    for (size_t i = 0; i < n; ++i) {
        data[i] *= 2.0;
    }
}

// Builtin optimizations
int countOnes(uint64_t x) {
    return __builtin_popcountll(x);  // Single popcnt instruction
}

int leadingZeros(uint64_t x) {
    return __builtin_clzll(x);  // Single lzcnt instruction
}
```

7.3 Intel Compiler Optimizations
---------------------------------

```bash
# Intel C++ Compiler (icpx)
icpx -O3 -xHost -ipo -qopt-prefetch=4 -qopt-report=5 \
     -o trading_engine trading_engine.cpp

# Flags:
# -xHost: Optimize for current processor
# -ipo: Interprocedural optimization
# -qopt-prefetch=4: Aggressive prefetching
# -qopt-report=5: Detailed optimization report
```

================================================================================
8. INLINE ASSEMBLY OPTIMIZATION
================================================================================

8.1 Critical Hot Paths in Assembly
-----------------------------------

```cpp
#include <x86intrin.h>

// Fast timestamp using RDTSC
inline uint64_t rdtsc() {
    uint32_t lo, hi;
    __asm__ volatile (
        "rdtsc"
        : "=a"(lo), "=d"(hi)
    );
    return ((uint64_t)hi << 32) | lo;
}

// Or use intrinsic
inline uint64_t rdtsc_intrinsic() {
    return __rdtsc();
}

// Serialize execution
inline void serialize() {
    __asm__ volatile ("lfence" ::: "memory");
}

// Memory fence
inline void mfence() {
    __asm__ volatile ("mfence" ::: "memory");
}

// Compiler barrier (prevent reordering)
inline void compilerBarrier() {
    __asm__ volatile ("" ::: "memory");
}

// Pause instruction (for spin loops)
inline void pause() {
    __asm__ volatile ("pause" ::: "memory");
}

// Prefetch data
inline void prefetch(const void* addr) {
    __asm__ volatile ("prefetcht0 %0" :: "m"(*(const char*)addr));
}
```

8.2 SIMD Assembly
-----------------

```cpp
// AVX2 vector addition (manual)
void addVectorsASM(const double* a, const double* b, double* c, size_t n) {
    size_t i = 0;
    size_t vn = n & ~3;  // Round down to multiple of 4

    for (; i < vn; i += 4) {
        __asm__ volatile (
            "vmovupd (%1,%0,8), %%ymm0\n\t"      // Load a[i]
            "vmovupd (%2,%0,8), %%ymm1\n\t"      // Load b[i]
            "vaddpd %%ymm0, %%ymm1, %%ymm0\n\t"  // Add
            "vmovupd %%ymm0, (%3,%0,8)\n\t"      // Store c[i]
            :
            : "r"(i), "r"(a), "r"(b), "r"(c)
            : "%ymm0", "%ymm1", "memory"
        );
    }

    // Handle remainder
    for (; i < n; ++i) {
        c[i] = a[i] + b[i];
    }
}

// Better: Use intrinsics instead
void addVectorsIntrinsic(const double* a, const double* b, double* c, size_t n) {
    size_t i = 0;
    size_t vn = n & ~3;

    for (; i < vn; i += 4) {
        __m256d va = _mm256_loadu_pd(&a[i]);
        __m256d vb = _mm256_loadu_pd(&b[i]);
        __m256d vc = _mm256_add_pd(va, vb);
        _mm256_storeu_pd(&c[i], vc);
    }

    for (; i < n; ++i) {
        c[i] = a[i] + b[i];
    }
}
```

================================================================================
9. BENCHMARKING COMPILER SETTINGS
================================================================================

9.1 Systematic Compiler Flag Testing
-------------------------------------

```bash
#!/bin/bash
# benchmark_compiler_flags.sh

SOURCE="trading_engine.cpp"
WORKLOAD="./trading_engine --benchmark"

# Test different optimization levels
for OPT_LEVEL in -O0 -O1 -O2 -O3 -Ofast; do
    echo "Testing $OPT_LEVEL..."

    # Compile
    g++ $OPT_LEVEL -march=native -o test_binary $SOURCE

    # Benchmark
    TIME=$( { time $WORKLOAD; } 2>&1 | grep real | awk '{print $2}' )

    echo "$OPT_LEVEL: $TIME"
done

# Test LTO
echo "Testing LTO..."
g++ -O3 -march=native -flto -fuse-linker-plugin -o test_binary $SOURCE
TIME=$( { time $WORKLOAD; } 2>&1 | grep real | awk '{print $2}' )
echo "LTO: $TIME"

# Test PGO
echo "Testing PGO..."
g++ -O3 -march=native -fprofile-generate -o test_binary_gen $SOURCE
./test_binary_gen $WORKLOAD  # Generate profile
g++ -O3 -march=native -fprofile-use -o test_binary_pgo $SOURCE
TIME=$( { time ./test_binary_pgo $WORKLOAD; } 2>&1 | grep real | awk '{print $2}' )
echo "PGO: $TIME"
```

9.2 Performance Counter Measurement
------------------------------------

```bash
# Measure with perf stat
perf stat -e cycles,instructions,cache-misses,branch-misses \
    ./trading_engine --benchmark

# Compare IPC between builds
# Higher IPC = better optimization
```

9.3 Microbenchmark Framework
-----------------------------

```cpp
#include <benchmark/benchmark.h>

// Benchmark order processing
static void BM_OrderProcessing(benchmark::State& state) {
    Order order = createTestOrder();
    OrderProcessor processor;

    for (auto _ : state) {
        processor.process(order);
        benchmark::DoNotOptimize(order);
        benchmark::ClobberMemory();
    }

    state.SetItemsProcessed(state.iterations());
}
BENCHMARK(BM_OrderProcessing);

// Benchmark with different compiler flags
// Compile with: g++ -O3 -march=native benchmark.cpp -lbenchmark -lpthread
// Run: ./benchmark --benchmark_repetitions=10
```

================================================================================
10. MEASUREMENT TECHNIQUES
================================================================================

10.1 Compilation Time Measurement
----------------------------------

```bash
# Measure compilation time
time g++ -O3 -march=native -flto trading_engine.cpp -o trading_engine

# Output:
# real: Total wall-clock time
# user: CPU time spent in user mode
# sys: CPU time spent in kernel mode
```

10.2 Binary Size Analysis
--------------------------

```bash
# Check binary size
ls -lh trading_engine

# Analyze sections
size trading_engine

# Detailed section analysis
objdump -h trading_engine

# Find large symbols
nm --size-sort --radix=d trading_engine | tail -20
```

10.3 Optimization Report Analysis
----------------------------------

```bash
# GCC optimization report
g++ -O3 -march=native -fopt-info-vec-all=opt_report.txt \
    trading_engine.cpp

# Clang optimization report
clang++ -O3 -march=native -Rpass=.* -Rpass-analysis=.* \
        -Rpass-missed=.* trading_engine.cpp 2> opt_report.txt

# Example output:
# opt_report.txt:
# trading_engine.cpp:123:5: remark: vectorized loop (vectorization width: 4)
# trading_engine.cpp:156:5: remark: loop not vectorized: cannot identify array bounds
```

================================================================================
11. EXPECTED IMPROVEMENTS
================================================================================

Optimization Technique                    | Performance Gain | Compile Time
------------------------------------------|------------------|---------------
-O0 to -O3                                | 5-10x           | 3-5x slower
-march=native                             | 1.2-1.8x        | Same
LTO (-flto)                               | 1.1-1.3x        | 2-4x slower
PGO (-fprofile-use)                       | 1.15-1.35x      | 2x (two passes)
Auto-vectorization (AVX2)                 | 2-4x            | Same
Fast-math (-ffast-math)                   | 1.1-1.4x        | Same
Function inlining                         | 1.1-1.2x        | Slightly slower

Combined Effect (-O3 + native + LTO + PGO): 8-18x faster than -O0

Real-World HFT Example:
-----------------------
Order Processing Latency:

Compilation Flags           | Latency (ns) | IPC  | Binary Size
----------------------------|--------------|------|-------------
-O0 (debug)                 | 18,500       | 0.8  | 2.1 MB
-O2                         | 4,200        | 1.6  | 2.8 MB
-O3                         | 2,800        | 2.1  | 3.4 MB
-O3 -march=native           | 1,900        | 2.5  | 3.4 MB
-O3 -march=native -flto     | 1,600        | 2.7  | 3.2 MB
-O3 -march=native -flto -fprofile-use | 1,200 | 3.1 | 3.5 MB

Final improvement: 15.4x faster (18,500 ns → 1,200 ns)

Market Data Parsing:

No optimization: 25,000 messages/sec
Full optimization: 450,000 messages/sec (18x throughput)

Competitive Advantage:
- Process 17,300 ns faster per order
- At 100K orders/day: 1.73 seconds total time saved
- React to market data 18x faster
- Can process 18x more strategies simultaneously

================================================================================
COMPLETE MAKEFILE WITH PROGRESSIVE OPTIMIZATION
================================================================================

```makefile
# Makefile for HFT trading engine with multiple optimization targets

CXX = g++
CLANGXX = clang++

# Source files
SRCS = trading_engine.cpp order_processor.cpp market_data.cpp
OBJS = $(SRCS:.cpp=.o)

# Common flags
COMMON_FLAGS = -std=c++20 -Wall -Wextra

# Debug build
debug: CXXFLAGS = $(COMMON_FLAGS) -O0 -g3
debug: $(OBJS)
	$(CXX) $(CXXFLAGS) -o trading_engine_debug $(OBJS)

# Release build (basic)
release: CXXFLAGS = $(COMMON_FLAGS) -O3 -march=native
release: $(OBJS)
	$(CXX) $(CXXFLAGS) -o trading_engine_release $(OBJS)

# Release with LTO
release_lto: CXXFLAGS = $(COMMON_FLAGS) -O3 -march=native -flto -fuse-linker-plugin
release_lto: $(OBJS)
	$(CXX) $(CXXFLAGS) -o trading_engine_lto $(OBJS)

# PGO build (two-pass)
pgo_generate: CXXFLAGS = $(COMMON_FLAGS) -O3 -march=native -fprofile-generate
pgo_generate: $(OBJS)
	$(CXX) $(CXXFLAGS) -o trading_engine_pgo_gen $(OBJS)

pgo_use: CXXFLAGS = $(COMMON_FLAGS) -O3 -march=native -fprofile-use
pgo_use: $(OBJS)
	$(CXX) $(CXXFLAGS) -o trading_engine_pgo $(OBJS)

# Maximum optimization (LTO + PGO)
max: CXXFLAGS = $(COMMON_FLAGS) -O3 -march=native -flto -fuse-linker-plugin \
                -fprofile-use -ffast-math -funroll-loops -finline-functions
max: $(OBJS)
	$(CXX) $(CXXFLAGS) -o trading_engine_max $(OBJS)

# Clean
clean:
	rm -f $(OBJS) trading_engine_* *.gcda *.gcno

# PGO workflow
pgo: pgo_generate
	./trading_engine_pgo_gen --training-workload
	$(MAKE) pgo_use

.PHONY: debug release release_lto pgo_generate pgo_use max clean pgo
```

================================================================================
END OF COMPILER OPTIMIZATION GUIDE
================================================================================
