================================================================================
HFT NETWORK OPTIMIZATION TECHNIQUES
================================================================================

TABLE OF CONTENTS
-----------------
1. Theory and Fundamental Concepts
2. Kernel Bypass Technologies
3. NIC Tuning and Configuration
4. Protocol Optimization
5. Socket Optimization
6. Zero-Copy Techniques
7. Interrupt and Polling Strategies
8. Network Stack Tuning
9. Hardware Timestamping
10. Benchmarking and Measurement
11. Expected Improvements

================================================================================
1. THEORY AND FUNDAMENTAL CONCEPTS
================================================================================

Network Latency Components:
----------------------------
1. Application processing: 100-500 ns
2. System call overhead: 50-200 ns
3. Kernel network stack: 1-5 μs
4. NIC hardware processing: 500-2000 ns
5. Wire transmission: Distance dependent
6. Switch/router latency: 1-10 μs (per hop)

Total one-way latency budget:
- Same rack: 5-20 μs
- Same datacenter: 20-100 μs
- Co-located exchange: 50-500 μs

HFT Network Optimization Goals:
--------------------------------
1. Minimize packet processing latency
2. Eliminate kernel overhead (kernel bypass)
3. Reduce CPU cache misses
4. Maximize packet processing rate
5. Ensure deterministic latency
6. Minimize jitter

Kernel Bypass Benefits:
-----------------------
- Traditional Linux stack: 5-15 μs latency
- Kernel bypass (DPDK/RDMA): 0.5-2 μs latency
- Improvement: 5-30x faster

================================================================================
2. KERNEL BYPASS TECHNOLOGIES
================================================================================

2.1 DPDK (Data Plane Development Kit)
--------------------------------------

Installation and Setup:
```bash
# Install DPDK
wget https://fast.dpdk.org/rel/dpdk-23.11.tar.xz
tar xf dpdk-23.11.tar.xz
cd dpdk-23.11

# Install dependencies
apt-get install -y build-essential libnuma-dev python3-pyelftools

# Build DPDK
meson setup build
cd build
ninja
ninja install
ldconfig

# Configure huge pages (required for DPDK)
echo 1024 > /sys/kernel/mm/hugepages/hugepages-2048kB/nr_hugepages

# Bind NIC to DPDK
modprobe uio_pci_generic
dpdk-devbind.py --bind=uio_pci_generic 0000:03:00.0

# Verify
dpdk-devbind.py --status
```

Basic DPDK Application:
```cpp
#include <rte_eal.h>
#include <rte_ethdev.h>
#include <rte_mbuf.h>
#include <rte_cycles.h>

#define RX_RING_SIZE 1024
#define TX_RING_SIZE 1024
#define NUM_MBUFS 8191
#define MBUF_CACHE_SIZE 250

class DPDKNetworkEngine {
    uint16_t portId;
    struct rte_mempool* mbufPool;

public:
    int initialize(int argc, char** argv) {
        // Initialize EAL
        int ret = rte_eal_init(argc, argv);
        if (ret < 0) {
            return -1;
        }

        // Get number of available ports
        uint16_t nbPorts = rte_eth_dev_count_avail();
        if (nbPorts == 0) {
            fprintf(stderr, "No Ethernet ports available\n");
            return -1;
        }

        // Create mbuf pool
        mbufPool = rte_pktmbuf_pool_create("MBUF_POOL",
                                           NUM_MBUFS,
                                           MBUF_CACHE_SIZE,
                                           0,
                                           RTE_MBUF_DEFAULT_BUF_SIZE,
                                           rte_socket_id());
        if (mbufPool == nullptr) {
            fprintf(stderr, "Cannot create mbuf pool\n");
            return -1;
        }

        // Configure port
        portId = 0;
        if (configurePort() != 0) {
            return -1;
        }

        printf("DPDK initialized successfully\n");
        return 0;
    }

    int configurePort() {
        struct rte_eth_conf portConf = {};
        portConf.rxmode.mq_mode = RTE_ETH_MQ_RX_NONE;

        // Configure device
        int ret = rte_eth_dev_configure(portId, 1, 1, &portConf);
        if (ret != 0) {
            return ret;
        }

        // Setup RX queue
        ret = rte_eth_rx_queue_setup(portId, 0, RX_RING_SIZE,
                                     rte_eth_dev_socket_id(portId),
                                     nullptr, mbufPool);
        if (ret < 0) {
            return ret;
        }

        // Setup TX queue
        ret = rte_eth_tx_queue_setup(portId, 0, TX_RING_SIZE,
                                     rte_eth_dev_socket_id(portId),
                                     nullptr);
        if (ret < 0) {
            return ret;
        }

        // Start device
        ret = rte_eth_dev_start(portId);
        if (ret < 0) {
            return ret;
        }

        // Enable promiscuous mode
        rte_eth_promiscuous_enable(portId);

        printf("Port %u configured\n", portId);
        return 0;
    }

    void receivePackets() {
        struct rte_mbuf* bufs[32];

        while (true) {
            // Receive burst of packets
            const uint16_t nbRx = rte_eth_rx_burst(portId, 0, bufs, 32);

            if (nbRx == 0) {
                continue;
            }

            // Process packets
            for (uint16_t i = 0; i < nbRx; i++) {
                processPacket(bufs[i]);
                rte_pktmbuf_free(bufs[i]);
            }
        }
    }

    void processPacket(struct rte_mbuf* mbuf) {
        // Extract packet data
        uint8_t* pktData = rte_pktmbuf_mtod(mbuf, uint8_t*);
        uint16_t pktLen = rte_pktmbuf_data_len(mbuf);

        // Parse and process market data
        parseMarketData(pktData, pktLen);
    }

    void sendPacket(const void* data, size_t len) {
        // Allocate mbuf
        struct rte_mbuf* mbuf = rte_pktmbuf_alloc(mbufPool);
        if (mbuf == nullptr) {
            return;
        }

        // Copy data to mbuf
        uint8_t* pktData = rte_pktmbuf_mtod(mbuf, uint8_t*);
        rte_memcpy(pktData, data, len);
        mbuf->data_len = len;
        mbuf->pkt_len = len;

        // Send packet
        uint16_t nbTx = rte_eth_tx_burst(portId, 0, &mbuf, 1);

        if (nbTx == 0) {
            rte_pktmbuf_free(mbuf);
        }
    }

private:
    void parseMarketData(const uint8_t* data, uint16_t len) {
        // Market data parsing logic
    }
};

// Main function
int main(int argc, char** argv) {
    DPDKNetworkEngine engine;

    if (engine.initialize(argc, argv) != 0) {
        return -1;
    }

    engine.receivePackets();
    return 0;
}

// Compilation:
// gcc -O3 -march=native dpdk_app.cpp -o dpdk_app \
//     $(pkg-config --cflags --libs libdpdk)
```

2.2 OpenOnload (Solarflare NICs)
---------------------------------

Installation:
```bash
# Download OpenOnload
wget https://www.xilinx.com/content/dam/xilinx/publications/openonload-*.tgz
tar xf openonload-*.tgz
cd openonload-*

# Build and install
./scripts/onload_build
./scripts/onload_install

# Load driver
onload_tool reload

# Verify
onload_tool query
```

Using OpenOnload:
```cpp
// No code changes required - use standard socket API

#include <sys/socket.h>
#include <netinet/in.h>

class OpenOnloadSocket {
    int sockfd;

public:
    bool create() {
        sockfd = socket(AF_INET, SOCK_DGRAM, 0);
        return sockfd >= 0;
    }

    bool bind(uint16_t port) {
        struct sockaddr_in addr = {};
        addr.sin_family = AF_INET;
        addr.sin_port = htons(port);
        addr.sin_addr.s_addr = INADDR_ANY;

        return ::bind(sockfd, (struct sockaddr*)&addr, sizeof(addr)) == 0;
    }

    ssize_t receive(void* buffer, size_t len) {
        return recv(sockfd, buffer, len, 0);
    }

    ssize_t send(const void* data, size_t len, const char* ip, uint16_t port) {
        struct sockaddr_in dest = {};
        dest.sin_family = AF_INET;
        dest.sin_port = htons(port);
        inet_pton(AF_INET, ip, &dest.sin_addr);

        return sendto(sockfd, data, len, 0,
                     (struct sockaddr*)&dest, sizeof(dest));
    }
};

// Run application with OpenOnload acceleration
// onload --profile=latency ./trading_app
```

2.3 RDMA (Remote Direct Memory Access)
---------------------------------------

```cpp
#include <infiniband/verbs.h>

class RDMAEngine {
    struct ibv_context* context;
    struct ibv_pd* pd;
    struct ibv_qp* qp;
    struct ibv_cq* cq;
    struct ibv_mr* mr;

public:
    int initialize() {
        // Get device list
        int numDevices;
        struct ibv_device** deviceList = ibv_get_device_list(&numDevices);
        if (deviceList == nullptr || numDevices == 0) {
            fprintf(stderr, "No RDMA devices found\n");
            return -1;
        }

        // Open device
        context = ibv_open_device(deviceList[0]);
        if (context == nullptr) {
            ibv_free_device_list(deviceList);
            return -1;
        }

        ibv_free_device_list(deviceList);

        // Allocate protection domain
        pd = ibv_alloc_pd(context);
        if (pd == nullptr) {
            return -1;
        }

        // Create completion queue
        cq = ibv_create_cq(context, 128, nullptr, nullptr, 0);
        if (cq == nullptr) {
            return -1;
        }

        // Create queue pair
        struct ibv_qp_init_attr qpInitAttr = {};
        qpInitAttr.send_cq = cq;
        qpInitAttr.recv_cq = cq;
        qpInitAttr.qp_type = IBV_QPT_RC;
        qpInitAttr.cap.max_send_wr = 128;
        qpInitAttr.cap.max_recv_wr = 128;
        qpInitAttr.cap.max_send_sge = 1;
        qpInitAttr.cap.max_recv_sge = 1;

        qp = ibv_create_qp(pd, &qpInitAttr);
        if (qp == nullptr) {
            return -1;
        }

        printf("RDMA initialized successfully\n");
        return 0;
    }

    void* registerMemory(void* addr, size_t length) {
        mr = ibv_reg_mr(pd, addr, length,
                       IBV_ACCESS_LOCAL_WRITE | IBV_ACCESS_REMOTE_WRITE);
        return mr;
    }

    int postReceive(void* buffer, size_t len) {
        struct ibv_sge sge = {};
        sge.addr = (uintptr_t)buffer;
        sge.length = len;
        sge.lkey = mr->lkey;

        struct ibv_recv_wr wr = {};
        wr.wr_id = 0;
        wr.sg_list = &sge;
        wr.num_sge = 1;

        struct ibv_recv_wr* badWr;
        return ibv_post_recv(qp, &wr, &badWr);
    }

    int pollCompletion() {
        struct ibv_wc wc;
        int ret = ibv_poll_cq(cq, 1, &wc);

        if (ret > 0 && wc.status == IBV_WC_SUCCESS) {
            return wc.byte_len;
        }

        return -1;
    }
};
```

================================================================================
3. NIC TUNING AND CONFIGURATION
================================================================================

3.1 Hardware Offload Features
------------------------------

```bash
#!/bin/bash
# Configure NIC for ultra-low latency

NIC="eth0"

# Disable hardware offloads (reduce latency)
ethtool -K $NIC tso off          # TCP Segmentation Offload
ethtool -K $NIC gso off          # Generic Segmentation Offload
ethtool -K $NIC gro off          # Generic Receive Offload
ethtool -K $NIC lro off          # Large Receive Offload
ethtool -K $NIC sg off           # Scatter-Gather
ethtool -K $NIC rx-checksumming off
ethtool -K $NIC tx-checksumming off

# Enable flow control (prevent packet drops)
ethtool -A $NIC rx on tx on

# Set ring buffer sizes (larger = fewer interrupts)
ethtool -G $NIC rx 4096 tx 4096

# Disable interrupt coalescing (minimum latency)
ethtool -C $NIC rx-usecs 0
ethtool -C $NIC rx-frames 1
ethtool -C $NIC tx-usecs 0
ethtool -C $NIC tx-frames 1

# Set queue count (multi-queue)
ethtool -L $NIC combined 4

# Verify settings
ethtool -k $NIC
ethtool -g $NIC
ethtool -c $NIC
```

3.2 RSS (Receive Side Scaling) Configuration
---------------------------------------------

```bash
#!/bin/bash
# Configure RSS for multi-core processing

NIC="eth0"

# Enable RSS
ethtool -K $NIC rxhash on

# Configure RSS hash function
ethtool -X $NIC hfunc toeplitz

# Set RSS indirection table
# Route traffic to specific CPUs (2-5)
ethtool -X $NIC equal 4

# Configure RSS hash key (random key for security)
ethtool -X $NIC hkey 6d:5a:56:da:25:5b:0e:c2:41:67:25:3d:43:a3:8f:b0:d0:ca:2b:cb:ae:7b:30:b4:77:cb:2d:a3:80:30:f2:0c:6a:42:b7:3b:be:ac:01:fa

# Verify RSS configuration
ethtool -x $NIC
```

3.3 Interrupt Affinity
-----------------------

```bash
#!/bin/bash
# Set NIC interrupt affinity

NIC="eth0"

# Get IRQ numbers for NIC
IRQ_NUMS=$(grep $NIC /proc/interrupts | awk -F: '{print $1}')

# Assign each queue to a specific CPU
CPU=2
for IRQ in $IRQ_NUMS; do
    echo $CPU > /proc/irq/$IRQ/smp_affinity_list
    echo "IRQ $IRQ -> CPU $CPU"
    CPU=$((CPU + 1))
done
```

3.4 NIC Driver Tuning
----------------------

```bash
#!/bin/bash
# Tune NIC driver parameters

NIC="eth0"

# For Intel NICs (ixgbe/i40e)
# Increase RX/TX descriptor rings
ethtool -G $NIC rx 4096 tx 4096

# Enable adaptive interrupt moderation
ethtool -C $NIC adaptive-rx on adaptive-tx on

# Or disable for absolute minimum latency
ethtool -C $NIC adaptive-rx off adaptive-tx off rx-usecs 0 tx-usecs 0

# For Mellanox NICs
# Enable Data Center Bridging
mlnx_qos -i $NIC --pfc 0,0,0,1,0,0,0,0

# Set trust mode to DSCP
mlnx_qos -i $NIC --trust dscp
```

================================================================================
4. PROTOCOL OPTIMIZATION
================================================================================

4.1 UDP Optimization
--------------------

```cpp
#include <sys/socket.h>
#include <netinet/in.h>
#include <arpa/inet.h>

class OptimizedUDPSocket {
    int sockfd;

public:
    bool create() {
        sockfd = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP);
        if (sockfd < 0) {
            return false;
        }

        // Set socket options for low latency
        int optval = 1;

        // Reuse address
        setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &optval, sizeof(optval));

        // Reuse port (for load balancing)
        setsockopt(sockfd, SOL_SOCKET, SO_REUSEPORT, &optval, sizeof(optval));

        // Set socket priority
        int priority = 6;  // High priority
        setsockopt(sockfd, SOL_SOCKET, SO_PRIORITY, &priority, sizeof(priority));

        // Increase send/receive buffer sizes
        int bufsize = 16 * 1024 * 1024;  // 16 MB
        setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &bufsize, sizeof(bufsize));
        setsockopt(sockfd, SOL_SOCKET, SO_SNDBUF, &bufsize, sizeof(bufsize));

        // Set Type of Service (ToS) for QoS
        int tos = IPTOS_LOWDELAY;
        setsockopt(sockfd, IPPROTO_IP, IP_TOS, &tos, sizeof(tos));

        // Enable timestamps (optional - for latency measurement)
        setsockopt(sockfd, SOL_SOCKET, SO_TIMESTAMP, &optval, sizeof(optval));

        // Bind to specific CPU (via SO_INCOMING_CPU)
        int cpu = 2;
        setsockopt(sockfd, SOL_SOCKET, SO_INCOMING_CPU, &cpu, sizeof(cpu));

        return true;
    }

    bool bind(uint16_t port, const char* ip = nullptr) {
        struct sockaddr_in addr = {};
        addr.sin_family = AF_INET;
        addr.sin_port = htons(port);

        if (ip == nullptr) {
            addr.sin_addr.s_addr = INADDR_ANY;
        } else {
            inet_pton(AF_INET, ip, &addr.sin_addr);
        }

        return ::bind(sockfd, (struct sockaddr*)&addr, sizeof(addr)) == 0;
    }

    ssize_t receive(void* buffer, size_t len) {
        return recv(sockfd, buffer, len, MSG_DONTWAIT);
    }

    ssize_t send(const void* data, size_t len,
                const char* destIp, uint16_t destPort) {
        struct sockaddr_in dest = {};
        dest.sin_family = AF_INET;
        dest.sin_port = htons(destPort);
        inet_pton(AF_INET, destIp, &dest.sin_addr);

        return sendto(sockfd, data, len, MSG_DONTWAIT,
                     (struct sockaddr*)&dest, sizeof(dest));
    }

    int getFd() const { return sockfd; }
};
```

4.2 TCP Optimization (when required)
-------------------------------------

```cpp
class OptimizedTCPSocket {
    int sockfd;

public:
    bool create() {
        sockfd = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
        if (sockfd < 0) {
            return false;
        }

        int optval = 1;

        // Disable Nagle's algorithm (reduce latency)
        setsockopt(sockfd, IPPROTO_TCP, TCP_NODELAY, &optval, sizeof(optval));

        // Enable TCP_QUICKACK (immediate ACKs)
        setsockopt(sockfd, IPPROTO_TCP, TCP_QUICKACK, &optval, sizeof(optval));

        // Set TCP_CORK to 0 (don't buffer small packets)
        optval = 0;
        setsockopt(sockfd, IPPROTO_TCP, TCP_CORK, &optval, sizeof(optval));

        // Set keepalive
        optval = 1;
        setsockopt(sockfd, SOL_SOCKET, SO_KEEPALIVE, &optval, sizeof(optval));

        // Keepalive time (60 seconds)
        optval = 60;
        setsockopt(sockfd, IPPROTO_TCP, TCP_KEEPIDLE, &optval, sizeof(optval));

        // Keepalive interval (10 seconds)
        optval = 10;
        setsockopt(sockfd, IPPROTO_TCP, TCP_KEEPINTVL, &optval, sizeof(optval));

        // Keepalive probes (3)
        optval = 3;
        setsockopt(sockfd, IPPROTO_TCP, TCP_KEEPCNT, &optval, sizeof(optval));

        // Set send/receive buffer sizes
        int bufsize = 16 * 1024 * 1024;
        setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &bufsize, sizeof(bufsize));
        setsockopt(sockfd, SOL_SOCKET, SO_SNDBUF, &bufsize, sizeof(bufsize));

        return true;
    }
};
```

4.3 Multicast Optimization
---------------------------

```cpp
class MulticastReceiver {
    int sockfd;

public:
    bool create(const char* multicastGroup, uint16_t port, const char* localIp) {
        // Create socket
        sockfd = socket(AF_INET, SOCK_DGRAM, IPPROTO_UDP);
        if (sockfd < 0) {
            return false;
        }

        // Reuse address
        int optval = 1;
        setsockopt(sockfd, SOL_SOCKET, SO_REUSEADDR, &optval, sizeof(optval));

        // Bind to port
        struct sockaddr_in addr = {};
        addr.sin_family = AF_INET;
        addr.sin_port = htons(port);
        addr.sin_addr.s_addr = INADDR_ANY;

        if (::bind(sockfd, (struct sockaddr*)&addr, sizeof(addr)) < 0) {
            return false;
        }

        // Join multicast group
        struct ip_mreqn mreq = {};
        inet_pton(AF_INET, multicastGroup, &mreq.imr_multiaddr);
        inet_pton(AF_INET, localIp, &mreq.imr_address);
        mreq.imr_ifindex = 0;  // Use default interface

        if (setsockopt(sockfd, IPPROTO_IP, IP_ADD_MEMBERSHIP,
                      &mreq, sizeof(mreq)) < 0) {
            return false;
        }

        // Set receive buffer size
        int bufsize = 32 * 1024 * 1024;  // 32 MB for multicast
        setsockopt(sockfd, SOL_SOCKET, SO_RCVBUF, &bufsize, sizeof(bufsize));

        // Set multicast loop (disable for efficiency)
        unsigned char loop = 0;
        setsockopt(sockfd, IPPROTO_IP, IP_MULTICAST_LOOP, &loop, sizeof(loop));

        // Set multicast TTL
        unsigned char ttl = 64;
        setsockopt(sockfd, IPPROTO_IP, IP_MULTICAST_TTL, &ttl, sizeof(ttl));

        printf("Joined multicast group %s:%u\n", multicastGroup, port);
        return true;
    }

    ssize_t receive(void* buffer, size_t len) {
        return recv(sockfd, buffer, len, MSG_DONTWAIT);
    }
};
```

================================================================================
5. SOCKET OPTIMIZATION
================================================================================

5.1 Socket Buffer Tuning
-------------------------

```bash
# System-wide socket buffer settings
# Add to /etc/sysctl.d/99-network-buffers.conf

# Maximum socket buffer sizes (128 MB)
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728

# Default socket buffer sizes
net.core.rmem_default = 16777216
net.core.wmem_default = 16777216

# TCP buffer sizes (min, default, max)
net.ipv4.tcp_rmem = 4096 87380 134217728
net.ipv4.tcp_wmem = 4096 65536 134217728

# UDP buffer sizes
net.ipv4.udp_rmem_min = 16384
net.ipv4.udp_wmem_min = 16384

# Apply changes
sysctl -p /etc/sysctl.d/99-network-buffers.conf
```

5.2 Busy Polling
----------------

```bash
# Enable busy polling for ultra-low latency
# Add to /etc/sysctl.d/99-busy-poll.conf

# Busy poll timeout (microseconds)
net.core.busy_poll = 50
net.core.busy_read = 50

# Per-socket busy polling
# Set via setsockopt(SO_BUSY_POLL) in application

# Apply changes
sysctl -p /etc/sysctl.d/99-busy-poll.conf
```

```cpp
// Application-level busy polling
int sockfd = socket(AF_INET, SOCK_DGRAM, 0);

// Enable busy polling on this socket
int busyPoll = 50;  // 50 μs
setsockopt(sockfd, SOL_SOCKET, SO_BUSY_POLL, &busyPoll, sizeof(busyPoll));
```

5.3 Zero-Copy Techniques
-------------------------

```cpp
#include <sys/socket.h>
#include <sys/sendfile.h>

class ZeroCopySocket {
    int sockfd;

public:
    // MSG_ZEROCOPY for send (kernel 4.14+)
    ssize_t sendZeroCopy(const void* data, size_t len,
                         const char* destIp, uint16_t destPort) {
        struct sockaddr_in dest = {};
        dest.sin_family = AF_INET;
        dest.sin_port = htons(destPort);
        inet_pton(AF_INET, destIp, &dest.sin_addr);

        // Send with zero-copy flag
        return sendto(sockfd, data, len, MSG_ZEROCOPY,
                     (struct sockaddr*)&dest, sizeof(dest));
    }

    // recvmmsg for batched receive
    ssize_t receiveBatch(struct iovec* iovecs, size_t count) {
        struct mmsghdr msgs[count];
        memset(msgs, 0, sizeof(msgs));

        for (size_t i = 0; i < count; i++) {
            msgs[i].msg_hdr.msg_iov = &iovecs[i];
            msgs[i].msg_hdr.msg_iovlen = 1;
        }

        // Receive multiple messages in one syscall
        int ret = recvmmsg(sockfd, msgs, count, MSG_DONTWAIT, nullptr);

        return ret;
    }

    // sendmmsg for batched send
    ssize_t sendBatch(struct iovec* iovecs, size_t count,
                     const char* destIp, uint16_t destPort) {
        struct sockaddr_in dest = {};
        dest.sin_family = AF_INET;
        dest.sin_port = htons(destPort);
        inet_pton(AF_INET, destIp, &dest.sin_addr);

        struct mmsghdr msgs[count];
        memset(msgs, 0, sizeof(msgs));

        for (size_t i = 0; i < count; i++) {
            msgs[i].msg_hdr.msg_iov = &iovecs[i];
            msgs[i].msg_hdr.msg_iovlen = 1;
            msgs[i].msg_hdr.msg_name = &dest;
            msgs[i].msg_hdr.msg_namelen = sizeof(dest);
        }

        // Send multiple messages in one syscall
        int ret = sendmmsg(sockfd, msgs, count, MSG_DONTWAIT);

        return ret;
    }
};
```

================================================================================
6. ZERO-COPY TECHNIQUES
================================================================================

6.1 Memory-Mapped Networking
-----------------------------

```cpp
#include <sys/mman.h>
#include <linux/if_packet.h>

class PacketMmapSocket {
    int sockfd;
    void* rxRing;
    void* txRing;
    size_t ringSize;

public:
    bool create() {
        // Create packet socket
        sockfd = socket(AF_PACKET, SOCK_RAW, htons(ETH_P_ALL));
        if (sockfd < 0) {
            return false;
        }

        // Configure RX ring buffer
        struct tpacket_req req = {};
        req.tp_block_size = 4096;
        req.tp_block_nr = 256;
        req.tp_frame_size = 2048;
        req.tp_frame_nr = req.tp_block_size * req.tp_block_nr / req.tp_frame_size;

        if (setsockopt(sockfd, SOL_PACKET, PACKET_RX_RING,
                      &req, sizeof(req)) < 0) {
            return false;
        }

        // Calculate total ring size
        ringSize = req.tp_block_size * req.tp_block_nr;

        // Memory map the ring buffer
        rxRing = mmap(nullptr, ringSize, PROT_READ | PROT_WRITE,
                     MAP_SHARED | MAP_LOCKED, sockfd, 0);

        if (rxRing == MAP_FAILED) {
            return false;
        }

        printf("Packet mmap socket created with %zu byte ring buffer\n", ringSize);
        return true;
    }

    void receivePackets() {
        struct tpacket_hdr* header;
        uint8_t* frame = static_cast<uint8_t*>(rxRing);

        while (true) {
            header = reinterpret_cast<struct tpacket_hdr*>(frame);

            // Check if frame is ready
            if ((header->tp_status & TP_STATUS_USER) == 0) {
                // Poll for new packets
                continue;
            }

            // Process packet (zero-copy access)
            uint8_t* packet = frame + header->tp_net;
            processPacket(packet, header->tp_len);

            // Mark frame as free
            header->tp_status = TP_STATUS_KERNEL;

            // Move to next frame
            frame += 2048;  // tp_frame_size
        }
    }

private:
    void processPacket(const uint8_t* data, size_t len) {
        // Process packet data
    }
};
```

================================================================================
7. INTERRUPT AND POLLING STRATEGIES
================================================================================

7.1 Interrupt vs Polling Trade-offs
------------------------------------

Interrupt Mode:
- Advantages: CPU efficient, power efficient
- Disadvantages: Higher latency (5-50 μs per interrupt)
- Use case: Low traffic, power-constrained systems

Polling Mode:
- Advantages: Ultra-low latency (< 1 μs)
- Disadvantages: 100% CPU utilization
- Use case: HFT, high-frequency data

Hybrid Mode:
- Advantages: Balance between latency and CPU usage
- Use case: Moderate traffic patterns

7.2 Busy-Poll Implementation
-----------------------------

```cpp
class BusyPollReceiver {
    int sockfd;

public:
    void receiveLoop() {
        char buffer[65536];
        ssize_t len;

        // Pin to CPU
        CPUAffinity::pinThreadToCPU(2);

        // Set real-time priority
        CPUAffinity::setRealtimePriority(99);

        // Busy poll loop
        while (true) {
            // Non-blocking receive
            len = recv(sockfd, buffer, sizeof(buffer), MSG_DONTWAIT);

            if (len > 0) {
                // Process packet immediately
                processPacket(buffer, len);
            } else if (len < 0 && errno != EAGAIN && errno != EWOULDBLOCK) {
                // Real error
                perror("recv failed");
                break;
            }

            // No sleep - continuous polling for minimum latency
            // CPU utilization: 100%
            // Latency: < 500 ns
        }
    }

private:
    void processPacket(const char* data, size_t len) {
        // Fast packet processing
    }
};
```

7.3 Adaptive Polling
---------------------

```cpp
class AdaptivePoller {
    int sockfd;
    uint64_t pollCycles;
    uint64_t sleepCycles;

public:
    void receiveLoop() {
        char buffer[65536];
        ssize_t len;
        uint64_t emptyPolls = 0;
        constexpr uint64_t POLL_THRESHOLD = 1000;

        while (true) {
            len = recv(sockfd, buffer, sizeof(buffer), MSG_DONTWAIT);

            if (len > 0) {
                // Packet received - reset counter
                emptyPolls = 0;
                processPacket(buffer, len);
            } else {
                emptyPolls++;

                // Switch to sleep mode after threshold
                if (emptyPolls > POLL_THRESHOLD) {
                    // Use epoll or sleep for brief period
                    usleep(1);  // 1 μs sleep
                    emptyPolls = 0;
                }
            }
        }
    }

private:
    void processPacket(const char* data, size_t len) {
        // Process packet
    }
};
```

================================================================================
8. NETWORK STACK TUNING
================================================================================

8.1 Kernel Network Parameters
------------------------------

```bash
# Complete network tuning configuration
# /etc/sysctl.d/99-network-tuning.conf

# Core network settings
net.core.netdev_max_backlog = 100000      # Packet queue size
net.core.netdev_budget = 600              # Packets per NAPI poll
net.core.netdev_budget_usecs = 2000       # NAPI poll time budget
net.core.somaxconn = 4096                 # Listen backlog

# TCP settings
net.ipv4.tcp_timestamps = 1               # Enable timestamps
net.ipv4.tcp_tw_reuse = 1                 # Reuse TIME_WAIT sockets
net.ipv4.tcp_fin_timeout = 15             # Reduce FIN timeout
net.ipv4.tcp_max_syn_backlog = 4096       # SYN queue size
net.ipv4.tcp_syncookies = 1               # Enable SYN cookies
net.ipv4.tcp_slow_start_after_idle = 0    # Disable slow start

# TCP congestion control
net.ipv4.tcp_congestion_control = bbr     # Use BBR (or cubic)

# TCP window scaling
net.ipv4.tcp_window_scaling = 1
net.ipv4.tcp_max_tw_buckets = 2000000

# IP settings
net.ipv4.ip_local_port_range = 10000 65535   # Port range
net.ipv4.ip_forward = 0                      # Disable forwarding

# Neighbor cache
net.ipv4.neigh.default.gc_thresh1 = 8000
net.ipv4.neigh.default.gc_thresh2 = 16000
net.ipv4.neigh.default.gc_thresh3 = 32000

# Routing cache
net.ipv4.route.flush = 1

# Apply settings
sysctl -p /etc/sysctl.d/99-network-tuning.conf
```

================================================================================
9. HARDWARE TIMESTAMPING
================================================================================

9.1 PTP Hardware Timestamping
------------------------------

```cpp
#include <linux/net_tstamp.h>
#include <linux/sockios.h>
#include <sys/ioctl.h>

class HardwareTimestampSocket {
    int sockfd;

public:
    bool enableHardwareTimestamps() {
        // Enable hardware timestamping
        int flags = SOF_TIMESTAMPING_TX_HARDWARE |
                   SOF_TIMESTAMPING_RX_HARDWARE |
                   SOF_TIMESTAMPING_RAW_HARDWARE;

        if (setsockopt(sockfd, SOL_SOCKET, SO_TIMESTAMPING,
                      &flags, sizeof(flags)) < 0) {
            perror("Failed to enable hardware timestamping");
            return false;
        }

        printf("Hardware timestamping enabled\n");
        return true;
    }

    ssize_t receiveWithTimestamp(void* buffer, size_t len, uint64_t* hwTimestamp) {
        char controlBuf[1024];
        struct iovec iov = {buffer, len};
        struct msghdr msg = {};

        msg.msg_iov = &iov;
        msg.msg_iovlen = 1;
        msg.msg_control = controlBuf;
        msg.msg_controllen = sizeof(controlBuf);

        ssize_t ret = recvmsg(sockfd, &msg, 0);
        if (ret < 0) {
            return ret;
        }

        // Extract hardware timestamp
        struct cmsghdr* cmsg;
        for (cmsg = CMSG_FIRSTHDR(&msg); cmsg != nullptr;
             cmsg = CMSG_NXTHDR(&msg, cmsg)) {

            if (cmsg->cmsg_level == SOL_SOCKET &&
                cmsg->cmsg_type == SO_TIMESTAMPING) {

                struct timespec* ts = (struct timespec*)CMSG_DATA(cmsg);
                *hwTimestamp = ts[2].tv_sec * 1000000000ULL + ts[2].tv_nsec;
                break;
            }
        }

        return ret;
    }

    void measureOneWayLatency() {
        char buffer[1024];
        uint64_t hwTimestamp;

        ssize_t len = receiveWithTimestamp(buffer, sizeof(buffer), &hwTimestamp);

        uint64_t currentTime = getCurrentTimeNs();
        uint64_t latency = currentTime - hwTimestamp;

        printf("Packet received, HW timestamp: %lu ns, Latency: %lu ns\n",
               hwTimestamp, latency);
    }

private:
    uint64_t getCurrentTimeNs() {
        struct timespec ts;
        clock_gettime(CLOCK_REALTIME, &ts);
        return ts.tv_sec * 1000000000ULL + ts.tv_nsec;
    }
};
```

9.2 Software Timestamping
--------------------------

```cpp
class SoftwareTimestampSocket {
    int sockfd;

public:
    bool enableSoftwareTimestamps() {
        int flags = SOF_TIMESTAMPING_SOFTWARE |
                   SOF_TIMESTAMPING_TX_SOFTWARE |
                   SOF_TIMESTAMPING_RX_SOFTWARE;

        if (setsockopt(sockfd, SOL_SOCKET, SO_TIMESTAMPING,
                      &flags, sizeof(flags)) < 0) {
            return false;
        }

        // Also enable SO_TIMESTAMP for kernel timestamps
        int optval = 1;
        setsockopt(sockfd, SOL_SOCKET, SO_TIMESTAMP, &optval, sizeof(optval));

        return true;
    }
};
```

================================================================================
10. BENCHMARKING AND MEASUREMENT
================================================================================

10.1 Network Latency Measurement
---------------------------------

```cpp
class NetworkLatencyMeasurement {
public:
    static void measureRTT(const char* destIp, uint16_t port) {
        int sockfd = socket(AF_INET, SOCK_DGRAM, 0);

        struct sockaddr_in dest = {};
        dest.sin_family = AF_INET;
        dest.sin_port = htons(port);
        inet_pton(AF_INET, destIp, &dest.sin_addr);

        char sendBuf[64] = "PING";
        char recvBuf[64];

        std::vector<uint64_t> latencies;

        for (int i = 0; i < 10000; i++) {
            // Send timestamp
            uint64_t t1 = rdtsc();

            sendto(sockfd, sendBuf, sizeof(sendBuf), 0,
                  (struct sockaddr*)&dest, sizeof(dest));

            // Receive response
            recv(sockfd, recvBuf, sizeof(recvBuf), 0);

            uint64_t t2 = rdtsc();

            // Calculate RTT in nanoseconds
            uint64_t rtt = cyclesToNanoseconds(t2 - t1);
            latencies.push_back(rtt);
        }

        // Calculate statistics
        std::sort(latencies.begin(), latencies.end());

        printf("RTT Statistics (ns):\n");
        printf("  Min:  %lu\n", latencies.front());
        printf("  P50:  %lu\n", latencies[latencies.size() / 2]);
        printf("  P99:  %lu\n", latencies[latencies.size() * 99 / 100]);
        printf("  Max:  %lu\n", latencies.back());

        close(sockfd);
    }

private:
    static uint64_t rdtsc() {
        return __rdtsc();
    }

    static uint64_t cyclesToNanoseconds(uint64_t cycles) {
        static constexpr double TSC_FREQ = 2.4e9;  // 2.4 GHz
        return cycles * (1e9 / TSC_FREQ);
    }
};
```

10.2 Packet Rate Measurement
-----------------------------

```bash
#!/bin/bash
# Measure packet rate using iperf3

# UDP performance test
iperf3 -c 192.168.1.100 -u -b 10G -t 60

# TCP performance test
iperf3 -c 192.168.1.100 -t 60

# Multicast test
iperf3 -c 239.1.1.1 -u -b 1G -T 32 -t 60

# Monitor packet drops
netstat -su | grep "packet receive errors"
```

================================================================================
11. EXPECTED IMPROVEMENTS
================================================================================

Optimization Technique                    | Latency Reduction | Throughput Gain
------------------------------------------|-------------------|------------------
Kernel bypass (DPDK/RDMA)                 | 80-95%           | 10-50x
NIC tuning (offloads disabled)            | 20-40%           | 1.2-1.5x
Socket optimization                       | 15-30%           | 1.5-2x
Busy polling                              | 40-60%           | 1.1-1.3x
Zero-copy techniques                      | 25-45%           | 2-4x
Hardware timestamping                     | 10-20%           | 1.1-1.2x
Multi-queue RSS                           | 30-50%           | 2-5x
Interrupt affinity                        | 15-25%           | 1.3-1.8x

Combined Effect: 90-99% latency reduction, 20-100x throughput gain

Real-World HFT Example:
-----------------------
Market Data Reception Pipeline:

BEFORE network optimization (standard Linux stack):
- NIC to kernel: 5 μs
- Kernel processing: 8 μs
- Syscall overhead: 2 μs
- Application processing: 3 μs
Total: 18 μs (one-way latency)

AFTER network optimization (kernel bypass + tuning):
- NIC to userspace: 0.5 μs
- DPDK processing: 0.3 μs
- Application processing: 0.8 μs
Total: 1.6 μs (11x faster)

Improvement: 16.4 μs saved per packet
At 1M packets/day: 16.4 million μs = 16.4 seconds saved
Competitive advantage: See market data 16.4 μs earlier than competitors

Order Submission Pipeline:

BEFORE:
- Application to kernel: 2 μs
- Kernel to NIC: 8 μs
- NIC to wire: 2 μs
Total: 12 μs

AFTER:
- Application to NIC (DPDK): 0.5 μs
- NIC to wire: 0.4 μs
Total: 0.9 μs (13x faster)

Result: Orders reach exchange 11.1 μs faster

================================================================================
COMPLETE NETWORK OPTIMIZATION SCRIPT
================================================================================

```bash
#!/bin/bash
# network_optimization.sh - Complete HFT network setup

set -e

NIC="eth0"
ISOLATED_CPUS="2-7"

echo "HFT Network Optimization"
echo "======================="

# 1. NIC configuration
echo "Configuring NIC..."
ethtool -K $NIC tso off gso off gro off lro off
ethtool -K $NIC rx-checksumming off tx-checksumming off
ethtool -G $NIC rx 4096 tx 4096
ethtool -C $NIC rx-usecs 0 tx-usecs 0 rx-frames 1 tx-frames 1
ethtool -A $NIC rx on tx on

# 2. Kernel parameters
echo "Configuring kernel parameters..."
cat > /etc/sysctl.d/99-hft-network.conf <<EOF
net.core.rmem_max = 134217728
net.core.wmem_max = 134217728
net.core.netdev_max_backlog = 100000
net.core.busy_poll = 50
net.ipv4.tcp_congestion_control = bbr
EOF
sysctl -p /etc/sysctl.d/99-hft-network.conf

# 3. IRQ affinity
echo "Setting IRQ affinity..."
for irq in $(grep $NIC /proc/interrupts | awk -F: '{print $1}'); do
    echo "0,1" > /proc/irq/$irq/smp_affinity_list
done

# 4. RSS configuration
echo "Configuring RSS..."
ethtool -X $NIC equal 4

echo "Network optimization complete!"
echo "Verify with: ethtool -k $NIC && ethtool -g $NIC"
```

================================================================================
END OF NETWORK OPTIMIZATION GUIDE
================================================================================
