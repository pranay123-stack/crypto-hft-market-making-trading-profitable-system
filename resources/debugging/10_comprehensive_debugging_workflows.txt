================================================================================
            COMPREHENSIVE HFT DEBUGGING WORKFLOWS
================================================================================

TABLE OF CONTENTS
-----------------
1. Complete Debugging Methodology
2. Scenario-Based Debugging Playbooks
3. Tool Selection Matrix
4. Integration of All Techniques
5. Real-World Case Studies
6. Debugging Checklist
7. Emergency Response Procedures
8. Long-Term Optimization Strategy
9. Documentation and Knowledge Base
10. Continuous Improvement

================================================================================
1. COMPLETE DEBUGGING METHODOLOGY
================================================================================

THE HFT DEBUGGING PYRAMID
--------------------------

Level 1: Monitoring (Always On)
- Metrics collection
- Health checks
- Basic alerting

Level 2: Detection (Triggered)
- Anomaly detection
- Performance regression
- Error patterns

Level 3: Investigation (Active)
- Profiling
- Tracing
- Log analysis

Level 4: Root Cause (Deep Dive)
- Core dumps
- System analysis
- Code review

Level 5: Prevention (Proactive)
- Testing
- Code quality
- Architecture review

SYSTEMATIC APPROACH
-------------------

1. OBSERVE: What is the symptom?
2. HYPOTHESIZE: What could cause this?
3. TEST: Can I reproduce it?
4. MEASURE: What data do I need?
5. ANALYZE: What does the data show?
6. FIX: What is the solution?
7. VERIFY: Did it work?
8. DOCUMENT: How do I prevent recurrence?

================================================================================
2. SCENARIO-BASED PLAYBOOKS
================================================================================

PLAYBOOK 1: LATENCY SPIKE INVESTIGATION
----------------------------------------

Symptom: P99 latency increased from 10µs to 500µs

Step 1: Identify when spike occurs
```bash
# Check if periodic
perf stat -e cycles,context-switches -I 1000 -p $(pidof trading_engine)
```

Step 2: Correlate with system events
```bash
# Check CPU frequency
watch -n 1 'grep MHz /proc/cpuinfo'

# Check interrupts
watch -n 1 'cat /proc/interrupts | grep eth0'
```

Step 3: Profile during spike
```bash
# Capture profile when latency high
if [ latency_high ]; then
    perf record -g -p $(pidof trading_engine) -- sleep 1
fi
```

Step 4: Analyze and fix
- If CPU frequency drop → Disable scaling
- If high interrupts → Adjust IRQ affinity
- If context switches → Check for blocking

PLAYBOOK 2: MEMORY LEAK
------------------------

Symptom: RSS growing over time

Step 1: Confirm leak
```bash
# Monitor RSS
watch -n 10 'ps aux | grep trading_engine | awk "{print \$6}"'
```

Step 2: Identify leak source
```bash
# Run with AddressSanitizer
ASAN_OPTIONS=detect_leaks=1 ./trading_engine
```

Step 3: Heap profile
```bash
# With tcmalloc
HEAPPROFILE=/tmp/heap.prof ./trading_engine

# Analyze
pprof --text ./trading_engine /tmp/heap.prof.0001.heap
```

Step 4: Fix and verify
- Add missing delete/free
- Use smart pointers
- Verify RSS stable

PLAYBOOK 3: DEADLOCK
--------------------

Symptom: Process hung, not responding

Step 1: Confirm deadlock
```bash
# Check if CPU usage dropped to 0
top -p $(pidof trading_engine)
```

Step 2: Attach debugger
```bash
gdb -p $(pidof trading_engine)
(gdb) thread apply all bt
```

Step 3: Identify locks
```bash
# Look for pthread_mutex_lock in backtraces
# Identify lock ordering
```

Step 4: Fix
- Consistent lock ordering
- Use std::scoped_lock
- Timeout on locks

================================================================================
3. TOOL SELECTION MATRIX
================================================================================

CHOOSE THE RIGHT TOOL
----------------------

Problem           | Best Tool         | Alternative    | Avoid
------------------|-------------------|----------------|----------------
Memory leak       | AddressSanitizer  | Valgrind       | Manual tracking
Thread race       | ThreadSanitizer   | Helgrind       | Printf
Cache misses      | perf stat         | Cachegrind     | Guessing
Network packet    | tcpdump           | Wireshark      | Application log
Latency spike     | perf record       | strace         | Manual timing
Lock contention   | perf lock         | Custom mutex   | Ignoring
System call       | strace -c         | ltrace         | Manual trace
CPU hotspot       | perf record -g    | gprof          | Manual profiling
Post-mortem       | GDB + core        | Log analysis   | Nothing
Live production   | eBPF/bpftrace     | Logging        | GDB attach

TOOL OVERHEAD COMPARISON
-------------------------

Tool                Overhead    Use Case
--------------------|-----------|----------------------------------
perf record         2-10%      Production profiling
AddressSanitizer    2x         Development testing
ThreadSanitizer     5-15x      Development testing
Valgrind Memcheck   10-50x     Thorough testing only
Valgrind Helgrind   30-100x    Race condition hunting
eBPF                <1%        Production monitoring
strace              High       Syscall investigation
GDB (attached)      Pauses     Post-mortem only

================================================================================
4. INTEGRATION EXAMPLE
================================================================================

COMPREHENSIVE MONITORING FRAMEWORK
-----------------------------------

```cpp
class HFTDebugFramework {
    LatencyHistogram latency;
    MemoryTracker memory;
    SystemMonitor system;
    MetricsRingBuffer<10000> metrics;

public:
    void on_market_data(const Quote& quote) {
        uint64_t start = rdtsc();

        // Process quote...

        uint64_t end = rdtsc();
        uint64_t cycles = end - start;

        // Record metrics
        latency.record(cycles);
        metrics.record("md_latency", cycles);

        // Check for anomalies
        if (cycles > THRESHOLD) {
            alert_and_capture(cycles);
        }
    }

private:
    void alert_and_capture(uint64_t cycles) {
        // 1. Log context
        LOG_ERROR("High latency: " << cycles << " cycles");

        // 2. Dump system state
        system.dump_state("/tmp/spike_state.txt");

        // 3. Trigger profile (if not too frequent)
        if (should_profile()) {
            trigger_perf_record();
        }

        // 4. Alert monitoring system
        send_alert("LATENCY_SPIKE", cycles);
    }
};
```

================================================================================
5. REAL-WORLD CASE STUDIES
================================================================================

CASE STUDY 1: The Mysterious 1ms Spike
---------------------------------------

Problem: Random 1ms latency spikes every few minutes

Investigation:
1. perf record showed nothing unusual in application
2. System monitoring revealed CPU frequency drops
3. Correlation: Spikes occurred when idle for >100ms

Root Cause: CPU entered C-state during idle periods

Solution:
```bash
sudo cpupower idle-set -D 0  # Disable C-states
```

Result: Spikes eliminated, consistent sub-10µs latency

CASE STUDY 2: The Gradual Slowdown
-----------------------------------

Problem: Performance degrades over hours

Investigation:
1. Memory profiling showed no leaks
2. Cache analysis revealed increasing miss rate
3. Heap fragmentation was increasing

Root Cause: Frequent small allocations fragmenting heap

Solution:
```cpp
// Replace frequent small allocations with object pool
ObjectPool<Order, 10000> order_pool;
```

Result: Stable performance over days

CASE STUDY 3: The Phantom Packet Loss
--------------------------------------

Problem: Multicast packets randomly dropped

Investigation:
1. Network capture showed packets arriving at NIC
2. Application not receiving all packets
3. Socket buffer not overflowing

Root Cause: Kernel network stack dropping due to small backlog

Solution:
```bash
sudo sysctl -w net.core.netdev_max_backlog=10000
```

Result: Zero packet loss

================================================================================
6. DEBUGGING CHECKLIST
================================================================================

PRE-DEPLOYMENT CHECKLIST
-------------------------

[ ] Unit tests pass
[ ] Integration tests pass
[ ] Performance benchmarks meet targets
[ ] AddressSanitizer clean
[ ] ThreadSanitizer clean
[ ] Static analysis clean
[ ] Memory profiled (no leaks)
[ ] Latency profiled (p99 < target)
[ ] Load tested (sustained traffic)
[ ] Stress tested (burst traffic)
[ ] Failover tested
[ ] Monitoring configured
[ ] Alerts configured
[ ] Logging appropriate
[ ] Core dumps enabled
[ ] Documentation updated

PRODUCTION INCIDENT CHECKLIST
------------------------------

[ ] Capture system state immediately
[ ] Check recent changes/deployments
[ ] Review monitoring dashboards
[ ] Check system resources (CPU, memory, network)
[ ] Review application logs
[ ] Check for related alerts
[ ] Capture packet capture if network issue
[ ] Capture performance profile
[ ] Check for external factors (exchange issues)
[ ] Document timeline
[ ] Notify stakeholders
[ ] Implement immediate mitigation
[ ] Plan permanent fix
[ ] Schedule post-mortem review

POST-INCIDENT CHECKLIST
------------------------

[ ] Root cause identified
[ ] Fix implemented and tested
[ ] Monitoring improved to detect earlier
[ ] Alerts tuned to catch similar issues
[ ] Documentation updated
[ ] Team training conducted
[ ] Post-mortem report written
[ ] Prevention measures implemented
[ ] Similar code paths reviewed
[ ] Tests added to prevent regression

================================================================================
7. EMERGENCY RESPONSE
================================================================================

SEVERITY LEVELS
---------------

SEV1: Trading Stopped
- Response: Immediate (5 min)
- Actions: Stop trading, investigate, fix, restart
- Communication: Real-time

SEV2: Degraded Performance
- Response: Urgent (30 min)
- Actions: Monitor, investigate, schedule fix
- Communication: Hourly updates

SEV3: Minor Issue
- Response: Normal (next day)
- Actions: Log, investigate, fix in next release
- Communication: Daily summary

EMERGENCY COMMANDS
------------------

```bash
# Quick health check
ps aux | grep trading_engine
netstat -anp | grep trading_engine
top -p $(pidof trading_engine)

# Capture state
kill -USR1 $(pidof trading_engine)  # Trigger dump
perf record -p $(pidof trading_engine) -g -- sleep 5

# Safe restart
kill -TERM $(pidof trading_engine)  # Graceful shutdown
./trading_engine &

# Emergency kill
kill -9 $(pidof trading_engine)  # Last resort
```

================================================================================
END OF COMPREHENSIVE DEBUGGING WORKFLOWS
================================================================================
