================================================================================
                    HFT CONCURRENCY DEBUGGING GUIDE
================================================================================

TABLE OF CONTENTS
-----------------
1. Theory and Common Concurrency Issues in HFT
2. Lock Contention Analysis Tools
3. False Sharing Detection
4. Atomic Operation Optimization
5. Lock-Free Algorithm Debugging
6. Step-by-Step Debugging Workflows
7. Real HFT C++ Code Examples
8. Debugging Commands and Interpretation
9. Solutions and Fixes
10. Best Practices for HFT Concurrency

================================================================================
1. THEORY AND COMMON CONCURRENCY ISSUES IN HFT
================================================================================

OVERVIEW
--------
Concurrency issues in HFT can cause:
- Lock contention degrading latency
- False sharing causing cache line bouncing
- Scalability bottlenecks as threads increase
- Non-deterministic performance

COMMON CONCURRENCY PROBLEMS
----------------------------

A) LOCK CONTENTION
   Multiple threads competing for same lock
   Symptoms: High wait time, poor scaling

B) FALSE SHARING
   Multiple threads accessing different variables on same cache line
   Causes: Cache coherency traffic, performance degradation

C) MEMORY ORDERING
   Incorrect memory order in atomic operations
   Causes: Race conditions, inconsistent state

D) ABA PROBLEM
   Lock-free structures see value A, then B, then A again
   Causes: Incorrect assumptions about state

E) LOCK CONVOY
   Multiple threads waiting on lock in queue
   Causes: Serialization, poor throughput

================================================================================
2. LOCK CONTENTION ANALYSIS TOOLS
================================================================================

DETECTING LOCK CONTENTION
--------------------------

1. PERF LOCK
```bash
# Record lock events
perf lock record -g ./trading_engine

# Report contention
perf lock report

# Output shows:
# Name          acquired  contended  wait(ns)    wait(ns) max
# &mtx              10000       5000    500000         1000
```

2. PTHREAD TOOLS
```cpp
#include <pthread.h>

void monitor_mutex_contention() {
    pthread_mutexattr_t attr;
    pthread_mutexattr_init(&attr);
    pthread_mutexattr_settype(&attr, PTHREAD_MUTEX_ERRORCHECK);

    pthread_mutex_t mtx;
    pthread_mutex_init(&mtx, &attr);

    // Will detect deadlocks and return EDEADLK
}
```

3. CUSTOM INSTRUMENTATION
```cpp
class InstrumentedMutex {
    std::mutex mtx;
    std::atomic<uint64_t> lock_count{0};
    std::atomic<uint64_t> contention_count{0};
    std::atomic<uint64_t> total_wait_ns{0};

public:
    void lock() {
        lock_count.fetch_add(1, std::memory_order_relaxed);
        auto start = std::chrono::high_resolution_clock::now();

        if (!mtx.try_lock()) {
            contention_count.fetch_add(1, std::memory_order_relaxed);
            mtx.lock();

            auto end = std::chrono::high_resolution_clock::now();
            auto wait = std::chrono::duration_cast<std::chrono::nanoseconds>(end - start).count();
            total_wait_ns.fetch_add(wait, std::memory_order_relaxed);
        }
    }

    void unlock() {
        mtx.unlock();
    }

    void print_stats() {
        uint64_t locks = lock_count.load();
        uint64_t contentions = contention_count.load();
        uint64_t wait = total_wait_ns.load();

        std::cout << "Lock attempts: " << locks << "\n"
                  << "Contentions: " << contentions << " ("
                  << (100.0 * contentions / locks) << "%)\n"
                  << "Avg wait: " << (contentions > 0 ? wait / contentions : 0) << " ns\n";
    }
};
```

================================================================================
3. FALSE SHARING DETECTION
================================================================================

UNDERSTANDING FALSE SHARING
----------------------------

Cache lines are 64 bytes. When two threads write to different variables
on the same cache line, the cache line bounces between CPU cores.

Example:
```cpp
// BAD: False sharing!
struct Counters {
    std::atomic<uint64_t> thread1_count;  // Byte 0-7
    std::atomic<uint64_t> thread2_count;  // Byte 8-15
    // Both on same 64-byte cache line!
};
```

DETECTING FALSE SHARING
------------------------

1. PERF C2C (Cache-to-Cache)
```bash
# Requires Linux 4.10+
perf c2c record -g ./trading_engine
perf c2c report --stdio

# Shows cache line contention between CPUs
```

2. CACHEGRIND WITH ANNOTATIONS
```bash
valgrind --tool=cachegrind --cachegrind-out-file=cache.out ./program
cg_annotate --auto=yes cache.out
```

3. PERFORMANCE SYMPTOMS
```bash
# High L1 cache misses on write-heavy code
perf stat -e L1-dcache-store-misses,L1-dcache-stores ./program

# High memory ordering machine clears
perf stat -e machine_clears.memory_ordering ./program
```

PREVENTING FALSE SHARING
-------------------------

```cpp
// GOOD: Aligned and padded
struct alignas(64) Counters {
    std::atomic<uint64_t> thread1_count;
    char pad1[64 - sizeof(std::atomic<uint64_t>)];

    std::atomic<uint64_t> thread2_count;
    char pad2[64 - sizeof(std::atomic<uint64_t>)];
};

// Or use alignas on each member
struct Counters {
    alignas(64) std::atomic<uint64_t> thread1_count;
    alignas(64) std::atomic<uint64_t> thread2_count;
};
```

================================================================================
4. ATOMIC OPERATION OPTIMIZATION
================================================================================

MEMORY ORDERING
---------------

From weakest to strongest:
1. memory_order_relaxed: No synchronization
2. memory_order_acquire: Synchronize reads
3. memory_order_release: Synchronize writes
4. memory_order_acq_rel: Both acquire and release
5. memory_order_seq_cst: Sequential consistency (default, slowest)

CHOOSING MEMORY ORDER
----------------------

```cpp
class StatsCounter {
    std::atomic<uint64_t> count{0};
    std::atomic<bool> ready{false};
    int data = 0;

public:
    // Relaxed: Independent counter
    void increment() {
        count.fetch_add(1, std::memory_order_relaxed);
    }

    // Acquire-Release: Synchronize data access
    void publish() {
        data = 42;
        ready.store(true, std::memory_order_release);
    }

    int consume() {
        while (!ready.load(std::memory_order_acquire)) {
            _mm_pause();
        }
        return data;  // Guaranteed to see 42
    }
};
```

ATOMIC PERFORMANCE
------------------

```cpp
// Benchmark different atomic operations
void benchmark_atomics() {
    std::atomic<uint64_t> counter{0};

    // Test relaxed
    auto start = rdtsc();
    for (int i = 0; i < 1000000; ++i) {
        counter.fetch_add(1, std::memory_order_relaxed);
    }
    auto relaxed_cycles = rdtsc() - start;

    counter = 0;

    // Test seq_cst
    start = rdtsc();
    for (int i = 0; i < 1000000; ++i) {
        counter.fetch_add(1, std::memory_order_seq_cst);
    }
    auto seqcst_cycles = rdtsc() - start;

    std::cout << "Relaxed: " << relaxed_cycles / 1000000 << " cycles/op\n"
              << "Seq_cst: " << seqcst_cycles / 1000000 << " cycles/op\n";
}
```

================================================================================
5. LOCK-FREE ALGORITHM DEBUGGING
================================================================================

COMMON LOCK-FREE BUGS
----------------------

1. ABA PROBLEM
```cpp
// BUGGY: ABA problem in lock-free stack
template<typename T>
class LockFreeStack {
    struct Node {
        T data;
        Node* next;
    };

    std::atomic<Node*> head{nullptr};

public:
    void push(const T& value) {
        Node* node = new Node{value, nullptr};
        node->next = head.load();
        while (!head.compare_exchange_weak(node->next, node)) {
            // Retry
        }
    }

    bool pop(T& value) {
        Node* old_head = head.load();
        while (old_head && !head.compare_exchange_weak(old_head, old_head->next)) {
            // ABA: old_head may have been deleted and reallocated!
        }
        if (old_head) {
            value = old_head->data;
            delete old_head;  // Dangerous!
            return true;
        }
        return false;
    }
};
```

SOLUTION: Use hazard pointers or tagged pointers
```cpp
template<typename T>
class SafeLockFreeStack {
    struct Node {
        T data;
        Node* next;
        std::atomic<int> ref_count{0};
    };

    std::atomic<Node*> head{nullptr};
    std::atomic<Node*> to_delete_list{nullptr};

public:
    void push(const T& value) {
        Node* node = new Node{value, nullptr};
        node->next = head.load(std::memory_order_relaxed);
        while (!head.compare_exchange_weak(node->next, node,
                                          std::memory_order_release,
                                          std::memory_order_relaxed)) {
        }
    }

    bool pop(T& value) {
        Node* old_head = head.load(std::memory_order_acquire);
        while (old_head) {
            old_head->ref_count.fetch_add(1, std::memory_order_acquire);

            if (head.compare_exchange_strong(old_head, old_head->next,
                                            std::memory_order_release,
                                            std::memory_order_acquire)) {
                value = old_head->data;
                if (old_head->ref_count.fetch_sub(1, std::memory_order_release) == 1) {
                    delete old_head;
                }
                return true;
            }

            if (old_head->ref_count.fetch_sub(1, std::memory_order_release) == 1) {
                delete old_head;
            }
        }
        return false;
    }
};
```

================================================================================
6. STEP-BY-STEP DEBUGGING WORKFLOWS
================================================================================

WORKFLOW 1: INVESTIGATING LOCK CONTENTION
------------------------------------------

Step 1: Measure Baseline Performance
```bash
perf stat -e cycles,instructions ./trading_engine
```

Step 2: Profile Lock Contention
```bash
perf lock record -g ./trading_engine
perf lock report
```

Step 3: Identify Hot Locks
Look for locks with high contention %

Step 4: Reduce Lock Scope
```cpp
// Before: Long critical section
{
    std::lock_guard<std::mutex> lock(mtx);
    auto data = fetch_data();  // Slow!
    process(data);
}

// After: Minimal critical section
auto data = fetch_data();
{
    std::lock_guard<std::mutex> lock(mtx);
    process(data);
}
```

Step 5: Consider Lock-Free Alternative
Replace mutex with atomic operations if possible

Step 6: Verify Improvement
```bash
perf stat -e cycles,instructions ./trading_engine_optimized
```

WORKFLOW 2: DETECTING FALSE SHARING
------------------------------------

Step 1: Measure Cache Performance
```bash
perf stat -e L1-dcache-stores,L1-dcache-store-misses,\
    machine_clears.memory_ordering ./program
```

High miss rate on stores indicates potential false sharing

Step 2: Profile with perf c2c
```bash
perf c2c record -g ./program
perf c2c report --stdio
```

Step 3: Identify Shared Cache Lines
Look for variables accessed by multiple threads

Step 4: Add Alignment and Padding
```cpp
struct alignas(64) ThreadData {
    std::atomic<uint64_t> counter;
    char padding[64 - sizeof(std::atomic<uint64_t>)];
};
```

Step 5: Verify Improvement
```bash
perf stat -e machine_clears.memory_ordering ./program_fixed
```

================================================================================
7. REAL HFT C++ CODE EXAMPLES
================================================================================

EXAMPLE 1: READ-COPY-UPDATE (RCU) PATTERN
------------------------------------------

```cpp
#include <atomic>
#include <memory>

template<typename T>
class RCUProtected {
    std::atomic<T*> ptr{nullptr};

public:
    RCUProtected() {
        ptr.store(new T(), std::memory_order_release);
    }

    // Read (very fast, no locks)
    class ReadGuard {
        T* data;
    public:
        ReadGuard(std::atomic<T*>& p)
            : data(p.load(std::memory_order_acquire)) {}

        const T* operator->() const { return data; }
        const T& operator*() const { return *data; }
    };

    ReadGuard read() {
        return ReadGuard(ptr);
    }

    // Write (slower, but rare)
    void write(const T& new_value) {
        T* old_ptr = ptr.load(std::memory_order_acquire);
        T* new_ptr = new T(new_value);

        ptr.store(new_ptr, std::memory_order_release);

        // Defer deletion until all readers done
        std::this_thread::sleep_for(std::chrono::milliseconds(1));
        delete old_ptr;
    }

    ~RCUProtected() {
        delete ptr.load();
    }
};

// Usage: Read-heavy configuration
RCUProtected<TradingConfig> config;

void on_market_data() {
    auto guard = config.read();
    if (guard->enabled) {
        // Use configuration without locking
    }
}

void update_config(const TradingConfig& new_config) {
    config.write(new_config);  // Rare operation
}
```

EXAMPLE 2: WAIT-FREE SINGLE-PRODUCER-SINGLE-CONSUMER QUEUE
-----------------------------------------------------------

```cpp
template<typename T, size_t Size>
class WaitFreeSPSCQueue {
    static_assert((Size & (Size - 1)) == 0, "Size must be power of 2");

    struct alignas(64) CacheLine {
        std::atomic<size_t> value;
        char padding[64 - sizeof(std::atomic<size_t>)];
    };

    std::array<T, Size> buffer;
    CacheLine write_pos;
    CacheLine read_pos;

public:
    WaitFreeSPSCQueue() {
        write_pos.value.store(0, std::memory_order_relaxed);
        read_pos.value.store(0, std::memory_order_relaxed);
    }

    // Producer
    bool try_push(const T& item) {
        size_t write = write_pos.value.load(std::memory_order_relaxed);
        size_t next_write = (write + 1) & (Size - 1);

        if (next_write == read_pos.value.load(std::memory_order_acquire)) {
            return false;  // Full
        }

        buffer[write] = item;
        write_pos.value.store(next_write, std::memory_order_release);
        return true;
    }

    // Consumer
    bool try_pop(T& item) {
        size_t read = read_pos.value.load(std::memory_order_relaxed);

        if (read == write_pos.value.load(std::memory_order_acquire)) {
            return false;  // Empty
        }

        item = buffer[read];
        read_pos.value.store((read + 1) & (Size - 1), std::memory_order_release);
        return true;
    }
};
```

EXAMPLE 3: STRIPED LOCK FOR REDUCED CONTENTION
-----------------------------------------------

```cpp
template<typename T, size_t NumStripes = 16>
class StripedLock {
    struct alignas(64) Stripe {
        std::mutex mtx;
        std::unordered_map<uint64_t, T> data;
    };

    std::array<Stripe, NumStripes> stripes;

    size_t get_stripe(uint64_t key) const {
        return std::hash<uint64_t>{}(key) % NumStripes;
    }

public:
    void insert(uint64_t key, const T& value) {
        size_t stripe_idx = get_stripe(key);
        std::lock_guard<std::mutex> lock(stripes[stripe_idx].mtx);
        stripes[stripe_idx].data[key] = value;
    }

    bool find(uint64_t key, T& value) {
        size_t stripe_idx = get_stripe(key);
        std::lock_guard<std::mutex> lock(stripes[stripe_idx].mtx);
        auto it = stripes[stripe_idx].data.find(key);
        if (it != stripes[stripe_idx].data.end()) {
            value = it->second;
            return true;
        }
        return false;
    }

    void remove(uint64_t key) {
        size_t stripe_idx = get_stripe(key);
        std::lock_guard<std::mutex> lock(stripes[stripe_idx].mtx);
        stripes[stripe_idx].data.erase(key);
    }
};

// Usage: Order tracking with reduced contention
StripedLock<Order, 64> order_map;
```

================================================================================
8. DEBUGGING COMMANDS AND INTERPRETATION
================================================================================

PERF LOCK COMMANDS
------------------

Record lock events:
```bash
perf lock record -g ./program
```

Show contention report:
```bash
perf lock report

# Output:
# Name          acquired  contended  wait(ns)  wait(max)
# &order_mtx       10000       5000    500000     10000
# Contention: 50%
```

PERF C2C COMMANDS
-----------------

Record cache-to-cache transfers:
```bash
perf c2c record -g ./program
```

Show report:
```bash
perf c2c report --stdio

# Shows cache line contention between CPUs
```

THREAD DEBUGGING
----------------

Show thread CPU affinity:
```bash
taskset -cp $(pidof program)
```

Set CPU affinity:
```bash
taskset -c 0,2,4,6 ./program
```

Monitor context switches:
```bash
perf stat -e context-switches,cpu-migrations ./program
```

================================================================================
9. SOLUTIONS AND FIXES
================================================================================

SOLUTION 1: FIX LOCK CONTENTION
--------------------------------

Problem: High contention on order queue lock

Before:
```cpp
class OrderQueue {
    std::mutex mtx;
    std::queue<Order> orders;

public:
    void push(const Order& order) {
        std::lock_guard<std::mutex> lock(mtx);
        orders.push(order);  // Contention!
    }

    bool pop(Order& order) {
        std::lock_guard<std::mutex> lock(mtx);
        if (orders.empty()) return false;
        order = orders.front();
        orders.pop();
        return true;
    }
};
```

After (Lock-Free):
```cpp
class OrderQueue {
    WaitFreeSPSCQueue<Order, 65536> queue;

public:
    void push(const Order& order) {
        while (!queue.try_push(order)) {
            _mm_pause();
        }
    }

    bool pop(Order& order) {
        return queue.try_pop(order);
    }
};
```

SOLUTION 2: FIX FALSE SHARING
------------------------------

Problem: Thread counters on same cache line

Before:
```cpp
struct Stats {
    std::atomic<uint64_t> orders_sent;     // Byte 0-7
    std::atomic<uint64_t> orders_filled;   // Byte 8-15
    std::atomic<uint64_t> orders_rejected; // Byte 16-23
    // All on same 64-byte cache line!
};
```

After:
```cpp
struct Stats {
    alignas(64) std::atomic<uint64_t> orders_sent;
    alignas(64) std::atomic<uint64_t> orders_filled;
    alignas(64) std::atomic<uint64_t> orders_rejected;
    // Each on separate cache line
};
```

SOLUTION 3: OPTIMIZE MEMORY ORDERING
-------------------------------------

Problem: Unnecessary seq_cst overhead

Before:
```cpp
std::atomic<uint64_t> counter{0};

void increment() {
    counter.fetch_add(1);  // Default: seq_cst (slow!)
}
```

After:
```cpp
std::atomic<uint64_t> counter{0};

void increment() {
    counter.fetch_add(1, std::memory_order_relaxed);  // Much faster!
}
```

================================================================================
10. BEST PRACTICES FOR HFT CONCURRENCY
================================================================================

1. AVOID LOCKS IN HOT PATH
   Use lock-free algorithms when possible

2. MINIMIZE LOCK SCOPE
   Only lock what's absolutely necessary

3. USE CORRECT MEMORY ORDERING
   - relaxed for independent operations
   - acquire/release for synchronization
   - seq_cst only when required

4. PREVENT FALSE SHARING
   Align shared variables to cache line boundaries

5. USE THREAD-LOCAL STORAGE
   Avoid sharing when possible

6. PIN THREADS TO CORES
   Reduce context switches and cache thrashing

7. MEASURE CONTENTION
   Profile regularly with perf lock

8. PREFER WAIT-FREE TO LOCK-FREE
   More predictable latency

9. USE READER-WRITER PATTERNS
   When reads vastly outnumber writes

10. TEST UNDER LOAD
    Contention issues only appear under load

================================================================================
END OF CONCURRENCY DEBUGGING GUIDE
================================================================================
