================================================================================
                    LATENCY TYPES IN HIGH-FREQUENCY TRADING
================================================================================

DOCUMENT VERSION: 1.0
CLASSIFICATION: Technical Reference
TARGET LATENCY: < 10 microseconds end-to-end

================================================================================
TABLE OF CONTENTS
================================================================================

1. Latency Taxonomy
2. Network Latency
3. Processing Latency
4. Serialization Latency
5. Queue Latency
6. I/O Latency
7. Synchronization Latency
8. Measurement Techniques
9. Mitigation Strategies
10. Real-World Case Studies

================================================================================
1. LATENCY TAXONOMY
================================================================================

LATENCY CATEGORIES:
-------------------

1.1 DETERMINISTIC LATENCY (Predictable)
----------------------------------------
- CPU instruction execution: 0.3-1ns per instruction
- Cache access: L1=1ns, L2=4ns, L3=10-20ns
- Memory access: 50-100ns
- Network hop: 10-50us per hop

Characteristics:
- Consistent timing
- Can be measured and optimized
- Suitable for hard real-time systems

1.2 NON-DETERMINISTIC LATENCY (Variable)
-----------------------------------------
- Context switches: 1-10us
- Page faults: 10-1000us
- Garbage collection: 1-100ms
- Network retransmissions: 10-500ms

Characteristics:
- Unpredictable timing
- Creates jitter
- Must be avoided in critical path

LATENCY METRICS:
----------------
- Mean: Average latency (can be misleading)
- Median (P50): Middle value (more representative)
- P99: 99th percentile (important for SLAs)
- P99.9: 99.9th percentile (tail latency)
- P99.99: 99.99th percentile (extreme tail)
- Max: Maximum observed latency

GOAL: Minimize both mean AND tail latency (reduce jitter)

================================================================================
2. NETWORK LATENCY (10ns - 10ms)
================================================================================

2.1 PHYSICAL LAYER LATENCY
---------------------------

PROPAGATION DELAY:
Speed of light in fiber: ~200,000 km/s (67% of c)
Latency = Distance / Speed

Examples:
- 1 km fiber: 5 microseconds
- 10 km fiber: 50 microseconds
- 100 km fiber: 500 microseconds
- 1000 km fiber: 5 milliseconds

NEW YORK TO CHICAGO:
Distance: ~1200 km
Fiber latency: ~6 milliseconds
Microwave latency: ~4 milliseconds (straight line, faster than fiber)

SERIALIZATION LATENCY:
Time to transmit bits onto wire
Formula: Packet_Size / Link_Speed

Examples at 10 Gbps:
- 64 bytes (min Ethernet): 51.2 nanoseconds
- 1500 bytes (max Ethernet): 1.2 microseconds
- 9000 bytes (jumbo frame): 7.2 microseconds

2.2 NETWORK DEVICE LATENCY
---------------------------

NETWORK INTERFACE CARD (NIC):
- Traditional: 5-50 microseconds
- Smart NIC: 1-5 microseconds
- Kernel bypass (DPDK): 100-500 nanoseconds

SWITCH LATENCY:
- Store-and-forward: 10-100 microseconds
- Cut-through: 300-700 nanoseconds
- Ultra-low latency: 50-300 nanoseconds

ROUTER LATENCY:
- Software router: 100-1000 microseconds
- Hardware router: 10-50 microseconds
- High-end: 1-10 microseconds

2.3 NETWORK STACK LATENCY
--------------------------

TRADITIONAL LINUX STACK:
```
Application
    ↓ [syscall: 50-500ns]
Kernel Space
    ↓ [socket layer: 1-5us]
TCP/IP Stack
    ↓ [protocol processing: 2-10us]
Device Driver
    ↓ [DMA setup: 1-5us]
NIC Hardware
    ↓ [transmission: 50-500ns]
Wire
```
TOTAL: 5-50 microseconds

KERNEL BYPASS (DPDK):
```
Application
    ↓ [poll mode: 0ns]
User Space Driver
    ↓ [zero-copy: 100ns]
NIC Hardware (DMA to user space)
    ↓ [transmission: 50ns]
Wire
```
TOTAL: 100-500 nanoseconds

C++ IMPLEMENTATION - Network Latency Measurement:

```cpp
#include <chrono>
#include <netinet/in.h>
#include <sys/socket.h>
#include <arpa/inet.h>
#include <unistd.h>

class NetworkLatencyMeasurement {
private:
    struct LatencyStats {
        uint64_t min_ns;
        uint64_t max_ns;
        uint64_t total_ns;
        uint64_t count;
    };

public:
    // Measure RTT (Round Trip Time) using UDP ping
    static LatencyStats measure_udp_rtt(
        const char* dest_ip,
        uint16_t dest_port,
        size_t num_pings = 1000
    ) {
        LatencyStats stats{UINT64_MAX, 0, 0, 0};

        // Create UDP socket
        int sock = socket(AF_INET, SOCK_DGRAM, 0);
        if (sock < 0) {
            return stats;
        }

        // Set timeout
        struct timeval timeout;
        timeout.tv_sec = 0;
        timeout.tv_usec = 100000; // 100ms
        setsockopt(sock, SOL_SOCKET, SO_RCVTIMEO, &timeout, sizeof(timeout));

        // Destination address
        struct sockaddr_in dest_addr;
        dest_addr.sin_family = AF_INET;
        dest_addr.sin_port = htons(dest_port);
        inet_pton(AF_INET, dest_ip, &dest_addr.sin_addr);

        // Ping packet
        char send_buf[64] = {0};
        char recv_buf[64] = {0};

        for (size_t i = 0; i < num_pings; i++) {
            // Send timestamp
            auto start = std::chrono::steady_clock::now();

            sendto(sock, send_buf, sizeof(send_buf), 0,
                  (struct sockaddr*)&dest_addr, sizeof(dest_addr));

            // Receive response
            struct sockaddr_in from_addr;
            socklen_t from_len = sizeof(from_addr);
            ssize_t recv_len = recvfrom(sock, recv_buf, sizeof(recv_buf), 0,
                                       (struct sockaddr*)&from_addr, &from_len);

            auto end = std::chrono::steady_clock::now();

            if (recv_len > 0) {
                uint64_t rtt_ns = std::chrono::duration_cast<
                    std::chrono::nanoseconds>(end - start).count();

                stats.min_ns = std::min(stats.min_ns, rtt_ns);
                stats.max_ns = std::max(stats.max_ns, rtt_ns);
                stats.total_ns += rtt_ns;
                stats.count++;
            }

            // Wait between pings
            usleep(1000); // 1ms
        }

        close(sock);
        return stats;
    }

    // Measure one-way latency using hardware timestamps
    static uint64_t measure_oneway_latency_hw(
        const uint8_t* packet,
        size_t packet_len,
        uint64_t tx_timestamp_ns,
        uint64_t rx_timestamp_ns
    ) {
        // One-way latency = RX timestamp - TX timestamp
        // Requires synchronized clocks (PTP)
        return rx_timestamp_ns - tx_timestamp_ns;
    }

    // Print statistics
    static void print_stats(const LatencyStats& stats) {
        if (stats.count == 0) {
            printf("No successful measurements\n");
            return;
        }

        uint64_t avg_ns = stats.total_ns / stats.count;

        printf("Network Latency Statistics:\n");
        printf("  Count:   %lu\n", stats.count);
        printf("  Min:     %.2f us\n", stats.min_ns / 1000.0);
        printf("  Avg:     %.2f us\n", avg_ns / 1000.0);
        printf("  Max:     %.2f us\n", stats.max_ns / 1000.0);
    }
};
```

2.4 PROTOCOL OVERHEAD
---------------------

ETHERNET FRAME:
- Preamble: 8 bytes
- Destination MAC: 6 bytes
- Source MAC: 6 bytes
- EtherType: 2 bytes
- Payload: 46-1500 bytes
- CRC: 4 bytes
- Inter-frame gap: 12 bytes
TOTAL OVERHEAD: 38 bytes + payload

IPv4 HEADER: 20 bytes (minimum)
IPv6 HEADER: 40 bytes (minimum)
TCP HEADER: 20 bytes (minimum), up to 60 bytes with options
UDP HEADER: 8 bytes

Example for 100-byte payload:
- Ethernet: 38 bytes overhead
- IP: 20 bytes overhead
- UDP: 8 bytes overhead
TOTAL: 166 bytes on wire (66% overhead!)

OPTIMIZATION: Use jumbo frames (9000 bytes) to reduce overhead

================================================================================
3. PROCESSING LATENCY (1ns - 10us)
================================================================================

3.1 CPU INSTRUCTION LATENCY
----------------------------

BASIC OPERATIONS (Intel Skylake @ 3 GHz):

Instruction              Latency     Throughput
--------------------------------------------------
Integer ADD/SUB          1 cycle     0.33 cycles
Integer MUL              3 cycles    1 cycle
Integer DIV              26 cycles   6 cycles
Float ADD/SUB            4 cycles    0.5 cycles
Float MUL                4 cycles    0.5 cycles
Float DIV                13 cycles   4 cycles
Float SQRT               18 cycles   6 cycles
--------------------------------------------------

MEMORY ACCESS:
- L1 Cache: 4-5 cycles (~1.3ns @ 3GHz)
- L2 Cache: 12 cycles (~4ns)
- L3 Cache: 40 cycles (~13ns)
- Main Memory: 150-300 cycles (50-100ns)
- Memory (different NUMA node): 300-500 cycles (100-170ns)

BRANCH MISPREDICTION: 15-20 cycles (~5-7ns)

3.2 CACHE OPTIMIZATION
----------------------

```cpp
#include <cstdint>
#include <array>

// BAD: Poor cache locality
struct BadDataStructure {
    uint64_t field1;
    char padding1[56]; // Wastes cache space
    uint64_t field2;
    char padding2[56];
    uint64_t field3;
    char padding3[56];
}; // 192 bytes = 3 cache lines

// GOOD: Cache-friendly
struct alignas(64) GoodDataStructure {
    uint64_t field1;
    uint64_t field2;
    uint64_t field3;
    char padding[40]; // Fill to cache line
}; // 64 bytes = 1 cache line

// Array-of-Structures (AoS) - Poor cache usage
struct AoS {
    struct Data {
        uint64_t price;
        uint64_t quantity;
        char side;
        char padding[7];
    };
    std::array<Data, 1000> data;

    // Process all prices (poor cache usage)
    void process_prices() {
        for (const auto& d : data) {
            // Loads entire structure (16 bytes) for each price access
            // Only 8 bytes (price) actually used
            process(d.price);
        }
    }
};

// Structure-of-Arrays (SoA) - Better cache usage
struct SoA {
    std::array<uint64_t, 1000> prices;
    std::array<uint64_t, 1000> quantities;
    std::array<char, 1000> sides;

    // Process all prices (better cache usage)
    void process_prices() {
        for (const auto& price : prices) {
            // Sequential access, all prices in contiguous memory
            // Perfect cache line utilization
            process(price);
        }
    }
};

// Prefetching for predictable access patterns
class PrefetchOptimized {
public:
    void process_array(const uint64_t* data, size_t len) {
        for (size_t i = 0; i < len; i++) {
            // Prefetch data 8 elements ahead
            if (i + 8 < len) {
                __builtin_prefetch(&data[i + 8], 0, 3);
            }

            // Process current element
            process(data[i]);
        }
    }

private:
    void process(uint64_t value);
};
```

3.3 BRANCH PREDICTION OPTIMIZATION
-----------------------------------

```cpp
// BAD: Unpredictable branches
uint64_t bad_conditional(uint64_t value) {
    if (value % 2 == 0) {  // 50/50 branch - hard to predict
        return value * 2;
    } else {
        return value * 3;
    }
}

// GOOD: Branchless computation
uint64_t good_branchless(uint64_t value) {
    // Use arithmetic instead of branch
    uint64_t is_even = (value & 1) ^ 1;
    return value * (2 + is_even);
}

// C++20: Use [[likely]] and [[unlikely]] attributes
uint64_t with_hints(uint64_t value) {
    if (value < 1000) [[likely]] {  // Hot path
        return value * 2;
    } else [[unlikely]] {  // Cold path
        return value * 3;
    }
}

// Example: Fast max without branch
inline uint64_t fast_max(uint64_t a, uint64_t b) {
    // Use CMOV instruction (conditional move)
    return a > b ? a : b;  // Compiler generates CMOV
}

// Branchless binary search (for small arrays)
template<size_t N>
size_t branchless_binary_search(const uint64_t (&arr)[N], uint64_t target) {
    size_t idx = 0;
    size_t step = N / 2;

    while (step > 0) {
        idx += (arr[idx + step] <= target) * step;
        step /= 2;
    }

    return idx;
}
```

3.4 ALGORITHM COMPLEXITY
------------------------

CRITICAL PATH REQUIREMENTS:
- O(1): Best - constant time (hash table lookup, array access)
- O(log n): Acceptable - logarithmic (binary search, balanced tree)
- O(n): Avoid - linear search (only if n is small and bounded)
- O(n log n): NEVER - in critical path (sorting)
- O(n²): NEVER - quadratic algorithms

```cpp
#include <unordered_map>
#include <map>
#include <vector>

// O(1) - Hash table lookup (best for hot path)
class O1Lookup {
private:
    std::unordered_map<uint32_t, uint64_t> symbol_to_price_;

public:
    // Average case: O(1), ~10-50ns
    uint64_t get_price(uint32_t symbol_id) const {
        auto it = symbol_to_price_.find(symbol_id);
        return (it != symbol_to_price_.end()) ? it->second : 0;
    }
};

// O(log n) - Balanced tree (acceptable)
class OLogNLookup {
private:
    std::map<uint32_t, uint64_t> symbol_to_price_; // Red-black tree

public:
    // O(log n), ~20-100ns for typical sizes
    uint64_t get_price(uint32_t symbol_id) const {
        auto it = symbol_to_price_.find(symbol_id);
        return (it != symbol_to_price_.end()) ? it->second : 0;
    }
};

// O(n) - Linear search (only for small, bounded n)
class ONLookup {
private:
    static constexpr size_t MAX_SYMBOLS = 16; // Small, cache-friendly
    std::array<std::pair<uint32_t, uint64_t>, MAX_SYMBOLS> symbols_;
    size_t count_ = 0;

public:
    // O(n), but n <= 16, so ~5-20ns (all in L1 cache)
    uint64_t get_price(uint32_t symbol_id) const {
        for (size_t i = 0; i < count_; i++) {
            if (symbols_[i].first == symbol_id) [[likely]] {
                return symbols_[i].second;
            }
        }
        return 0;
    }
};
```

================================================================================
4. SERIALIZATION LATENCY (10ns - 10us)
================================================================================

4.1 SERIALIZATION FORMATS COMPARISON
-------------------------------------

FORMAT          Size    Encode   Decode   Use Case
---------------------------------------------------------
Raw Binary      100%    10ns     10ns     Ultra-low latency
SBE             105%    50ns     50ns     HFT market data
FlatBuffers     110%    100ns    50ns     Zero-copy access
Cap'n Proto     110%    50ns     50ns     Zero-copy RPC
Protobuf        60%     500ns    800ns    Enterprise systems
JSON            300%    2us      5us      Web APIs
XML             500%    10us     20us     Legacy systems
---------------------------------------------------------

4.2 SIMPLE BINARY ENCODING (SBE)
---------------------------------

```cpp
#include <cstdint>
#include <cstring>

// SBE Schema Definition
namespace sbe {

// Message header (8 bytes)
struct __attribute__((packed)) MessageHeader {
    uint16_t block_length;
    uint16_t template_id;
    uint16_t schema_id;
    uint16_t version;
};

// Market data message (40 bytes)
struct __attribute__((packed)) MarketDataMessage {
    MessageHeader header;
    uint64_t timestamp;
    uint32_t symbol_id;
    uint64_t price;
    uint64_t quantity;
    uint8_t side; // 0=Buy, 1=Sell
    uint8_t padding[7];
};

static_assert(sizeof(MarketDataMessage) == 40, "Wrong size");

class SBEEncoder {
public:
    // Encode market data (~30ns)
    static void encode(
        MarketDataMessage& msg,
        uint64_t timestamp,
        uint32_t symbol_id,
        uint64_t price,
        uint64_t quantity,
        uint8_t side
    ) {
        msg.header.block_length = 32;
        msg.header.template_id = 1;
        msg.header.schema_id = 1;
        msg.header.version = 0;
        msg.timestamp = timestamp;
        msg.symbol_id = symbol_id;
        msg.price = price;
        msg.quantity = quantity;
        msg.side = side;
    }

    // Decode market data (~20ns - zero copy!)
    static const MarketDataMessage* decode(const void* buffer) {
        return reinterpret_cast<const MarketDataMessage*>(buffer);
    }
};

} // namespace sbe
```

4.3 ZERO-COPY SERIALIZATION
----------------------------

```cpp
// Zero-copy buffer management
class ZeroCopyBuffer {
private:
    uint8_t* buffer_;
    size_t capacity_;
    size_t offset_;

public:
    ZeroCopyBuffer(uint8_t* buffer, size_t capacity)
        : buffer_(buffer), capacity_(capacity), offset_(0) {}

    // Allocate space for object (zero-copy)
    template<typename T>
    T* allocate() {
        if (offset_ + sizeof(T) > capacity_) {
            return nullptr;
        }

        T* ptr = reinterpret_cast<T*>(buffer_ + offset_);
        offset_ += sizeof(T);
        return ptr;
    }

    // Get typed pointer at specific offset (zero-copy)
    template<typename T>
    T* get_at(size_t offset) {
        if (offset + sizeof(T) > capacity_) {
            return nullptr;
        }
        return reinterpret_cast<T*>(buffer_ + offset);
    }

    void reset() {
        offset_ = 0;
    }

    size_t size() const { return offset_; }
    const uint8_t* data() const { return buffer_; }
};

// Usage example
void zero_copy_example() {
    alignas(64) uint8_t buffer[1024];
    ZeroCopyBuffer zcb(buffer, sizeof(buffer));

    // Allocate and populate message (zero-copy)
    auto* msg = zcb.allocate<sbe::MarketDataMessage>();
    sbe::SBEEncoder::encode(*msg, 123456789, 1, 100000, 500, 0);

    // Send buffer directly (zero-copy)
    send_to_network(zcb.data(), zcb.size());
}
```

4.4 COMPARISON: FIX vs SBE
--------------------------

```cpp
#include <cstdio>
#include <chrono>

// FIX encoding (text-based)
class FIXEncoder {
public:
    static size_t encode(char* buffer, size_t capacity,
                        uint64_t order_id, uint32_t symbol_id,
                        uint64_t price, uint64_t quantity, char side) {
        // FIX 4.4 NewOrderSingle
        size_t len = std::snprintf(buffer, capacity,
            "8=FIX.4.4\x01" "35=D\x01" "11=%lu\x01" "55=%u\x01"
            "54=%c\x01" "44=%lu\x01" "38=%lu\x01",
            order_id, symbol_id, side, price, quantity);

        // Calculate checksum
        uint8_t checksum = 0;
        for (size_t i = 0; i < len; i++) {
            checksum += buffer[i];
        }
        len += std::snprintf(buffer + len, capacity - len,
                            "10=%03d\x01", checksum % 256);

        return len;
    }
};

// Benchmark FIX vs SBE
void benchmark_serialization() {
    const size_t iterations = 1000000;

    // FIX encoding
    {
        char fix_buffer[512];
        auto start = std::chrono::high_resolution_clock::now();

        for (size_t i = 0; i < iterations; i++) {
            FIXEncoder::encode(fix_buffer, sizeof(fix_buffer),
                             i, 1, 100000, 500, 'B');
        }

        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();

        printf("FIX encoding: %.2f ns/op\n",
               static_cast<double>(duration) / iterations);
        printf("FIX message size: %zu bytes\n",
               strlen(fix_buffer));
    }

    // SBE encoding
    {
        sbe::MarketDataMessage sbe_msg;
        auto start = std::chrono::high_resolution_clock::now();

        for (size_t i = 0; i < iterations; i++) {
            sbe::SBEEncoder::encode(sbe_msg, i, 1, 100000, 500, 0);
        }

        auto end = std::chrono::high_resolution_clock::now();
        auto duration = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();

        printf("SBE encoding: %.2f ns/op\n",
               static_cast<double>(duration) / iterations);
        printf("SBE message size: %zu bytes\n",
               sizeof(sbe::MarketDataMessage));
    }
}

// Typical results:
// FIX encoding: 450 ns/op, message size: ~120 bytes
// SBE encoding: 35 ns/op, message size: 40 bytes
// Speedup: 12.8x faster, 3x smaller
```

================================================================================
5. QUEUE LATENCY (10ns - 10us)
================================================================================

5.1 LOCK-FREE QUEUES
--------------------

```cpp
#include <atomic>
#include <array>

// Single Producer Single Consumer (SPSC) Lock-Free Queue
template<typename T, size_t Size>
class SPSCQueue {
private:
    static constexpr size_t CACHE_LINE_SIZE = 64;

    alignas(CACHE_LINE_SIZE) std::atomic<size_t> head_{0};
    alignas(CACHE_LINE_SIZE) std::atomic<size_t> tail_{0};
    alignas(CACHE_LINE_SIZE) std::array<T, Size> buffer_;

    // Make size power of 2 for fast modulo
    static_assert((Size & (Size - 1)) == 0, "Size must be power of 2");

public:
    // Producer: Push element (~10-50ns)
    bool push(const T& item) {
        size_t current_tail = tail_.load(std::memory_order_relaxed);
        size_t next_tail = (current_tail + 1) & (Size - 1);

        // Check if queue is full
        if (next_tail == head_.load(std::memory_order_acquire)) [[unlikely]] {
            return false;
        }

        buffer_[current_tail] = item;
        tail_.store(next_tail, std::memory_order_release);
        return true;
    }

    // Consumer: Pop element (~10-50ns)
    bool pop(T& item) {
        size_t current_head = head_.load(std::memory_order_relaxed);

        // Check if queue is empty
        if (current_head == tail_.load(std::memory_order_acquire)) [[unlikely]] {
            return false;
        }

        item = buffer_[current_head];
        head_.store((current_head + 1) & (Size - 1), std::memory_order_release);
        return true;
    }

    // Check if empty
    bool empty() const {
        return head_.load(std::memory_order_acquire) ==
               tail_.load(std::memory_order_acquire);
    }

    // Get size (approximate)
    size_t size() const {
        size_t h = head_.load(std::memory_order_acquire);
        size_t t = tail_.load(std::memory_order_acquire);
        return (t - h) & (Size - 1);
    }
};

// Multiple Producer Single Consumer (MPSC) Lock-Free Queue
template<typename T, size_t Size>
class MPSCQueue {
private:
    static constexpr size_t CACHE_LINE_SIZE = 64;

    struct Node {
        alignas(CACHE_LINE_SIZE) std::atomic<T*> data{nullptr};
    };

    alignas(CACHE_LINE_SIZE) std::atomic<size_t> head_{0};
    alignas(CACHE_LINE_SIZE) std::atomic<size_t> tail_{0};
    alignas(CACHE_LINE_SIZE) std::array<Node, Size> buffer_;

    static_assert((Size & (Size - 1)) == 0, "Size must be power of 2");

public:
    // Multiple producers: Push element (~50-200ns with contention)
    bool push(T* item) {
        size_t tail = tail_.fetch_add(1, std::memory_order_acq_rel);
        size_t index = tail & (Size - 1);

        // Wait for slot to be available
        T* expected = nullptr;
        while (!buffer_[index].data.compare_exchange_weak(
            expected, item,
            std::memory_order_release,
            std::memory_order_relaxed)) {
            expected = nullptr;
            _mm_pause();
        }

        return true;
    }

    // Single consumer: Pop element (~20-100ns)
    T* pop() {
        size_t head = head_.load(std::memory_order_relaxed);
        size_t index = head & (Size - 1);

        T* item = buffer_[index].data.exchange(nullptr, std::memory_order_acquire);

        if (item != nullptr) {
            head_.store(head + 1, std::memory_order_relaxed);
        }

        return item;
    }
};
```

5.2 BATCHING TO REDUCE LATENCY
-------------------------------

```cpp
// Batch queue - amortize overhead across multiple items
template<typename T, size_t BatchSize, size_t QueueSize>
class BatchQueue {
private:
    SPSCQueue<std::array<T, BatchSize>, QueueSize> queue_;
    std::array<T, BatchSize> push_batch_;
    std::array<T, BatchSize> pop_batch_;
    size_t push_count_ = 0;
    size_t pop_count_ = 0;
    size_t pop_index_ = 0;

public:
    // Push with automatic batching
    bool push(const T& item) {
        push_batch_[push_count_++] = item;

        if (push_count_ >= BatchSize) {
            bool success = queue_.push(push_batch_);
            push_count_ = 0;
            return success;
        }

        return true;
    }

    // Flush partial batch
    bool flush() {
        if (push_count_ > 0) {
            // Pad batch with empty items
            for (size_t i = push_count_; i < BatchSize; i++) {
                push_batch_[i] = T{};
            }
            bool success = queue_.push(push_batch_);
            push_count_ = 0;
            return success;
        }
        return true;
    }

    // Pop with batching
    bool pop(T& item) {
        if (pop_index_ >= pop_count_) {
            // Fetch new batch
            if (!queue_.pop(pop_batch_)) {
                return false;
            }
            pop_index_ = 0;
            pop_count_ = BatchSize;
        }

        item = pop_batch_[pop_index_++];
        return true;
    }
};

// Benchmark: Single vs Batch
void benchmark_queue_latency() {
    constexpr size_t iterations = 1000000;

    // Single-item queue
    {
        SPSCQueue<uint64_t, 1024> queue;
        auto start = std::chrono::high_resolution_clock::now();

        for (size_t i = 0; i < iterations; i++) {
            queue.push(i);
            uint64_t val;
            queue.pop(val);
        }

        auto end = std::chrono::high_resolution_clock::now();
        auto ns = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();
        printf("Single-item queue: %.2f ns/op\n",
               static_cast<double>(ns) / iterations);
    }

    // Batch queue
    {
        BatchQueue<uint64_t, 32, 1024> queue;
        auto start = std::chrono::high_resolution_clock::now();

        for (size_t i = 0; i < iterations; i++) {
            queue.push(i);
            if ((i % 32) == 31) {
                queue.flush();
            }
            uint64_t val;
            queue.pop(val);
        }

        auto end = std::chrono::high_resolution_clock::now();
        auto ns = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();
        printf("Batch queue: %.2f ns/op\n",
               static_cast<double>(ns) / iterations);
    }
}

// Typical results:
// Single-item queue: 25 ns/op
// Batch queue: 8 ns/op (3x faster)
```

================================================================================
6. I/O LATENCY (100ns - 10ms)
================================================================================

6.1 DISK I/O LATENCY
--------------------

STORAGE MEDIA LATENCY:
- HDD (7200 RPM): 5-15 milliseconds
- SSD (SATA): 50-100 microseconds
- NVMe SSD: 10-50 microseconds
- Intel Optane (3D XPoint): 2-10 microseconds
- RAM: 50-100 nanoseconds

OPERATIONS:
- Sequential read: Fastest
- Sequential write: Fast
- Random read: Slow (seek time)
- Random write: Slowest (seek + write)

```cpp
#include <fcntl.h>
#include <unistd.h>
#include <sys/mman.h>

// Direct I/O (bypass page cache) for predictable latency
class DirectIOFile {
private:
    int fd_ = -1;
    static constexpr size_t BLOCK_SIZE = 4096;

public:
    bool open(const char* path) {
        // O_DIRECT: Bypass kernel page cache
        // O_SYNC: Synchronous writes
        fd_ = ::open(path, O_RDWR | O_DIRECT | O_SYNC);
        return fd_ >= 0;
    }

    // Write with direct I/O (10-50us for NVMe)
    ssize_t write(const void* buffer, size_t size) {
        // Buffer must be aligned for direct I/O
        return ::write(fd_, buffer, size);
    }

    // Read with direct I/O
    ssize_t read(void* buffer, size_t size) {
        return ::read(fd_, buffer, size);
    }

    ~DirectIOFile() {
        if (fd_ >= 0) {
            ::close(fd_);
        }
    }
};

// Memory-mapped file (fastest for random access)
class MemoryMappedFile {
private:
    void* mapped_ = nullptr;
    size_t size_ = 0;
    int fd_ = -1;

public:
    bool open(const char* path, size_t size) {
        fd_ = ::open(path, O_RDWR | O_CREAT, 0644);
        if (fd_ < 0) return false;

        // Set file size
        if (ftruncate(fd_, size) < 0) {
            ::close(fd_);
            return false;
        }

        // Map file to memory
        mapped_ = mmap(nullptr, size, PROT_READ | PROT_WRITE,
                      MAP_SHARED, fd_, 0);

        if (mapped_ == MAP_FAILED) {
            ::close(fd_);
            return false;
        }

        size_ = size;
        return true;
    }

    // Access is just memory read/write (~50-100ns)
    template<typename T>
    T* get_at(size_t offset) {
        if (offset + sizeof(T) > size_) return nullptr;
        return reinterpret_cast<T*>(static_cast<uint8_t*>(mapped_) + offset);
    }

    // Explicit sync to disk (10-50us)
    void sync() {
        if (mapped_) {
            msync(mapped_, size_, MS_SYNC);
        }
    }

    ~MemoryMappedFile() {
        if (mapped_) {
            munmap(mapped_, size_);
        }
        if (fd_ >= 0) {
            ::close(fd_);
        }
    }
};
```

6.2 AVOIDING I/O IN CRITICAL PATH
----------------------------------

```cpp
// Asynchronous logger - I/O off critical path
class AsyncLogger {
private:
    struct LogEntry {
        uint64_t timestamp;
        uint32_t level;
        char message[256];
    };

    SPSCQueue<LogEntry, 10000> queue_;
    std::thread writer_thread_;
    std::atomic<bool> running_{true};
    int fd_ = -1;

public:
    AsyncLogger(const char* log_file) {
        fd_ = open(log_file, O_WRONLY | O_CREAT | O_APPEND, 0644);

        // Start writer thread
        writer_thread_ = std::thread([this]() {
            this->writer_loop();
        });

        // Set thread affinity to non-critical core
        cpu_set_t cpuset;
        CPU_ZERO(&cpuset);
        CPU_SET(15, &cpuset); // Use last core
        pthread_setaffinity_np(writer_thread_.native_handle(),
                              sizeof(cpuset), &cpuset);
    }

    // Fast logging (~50-200ns - just queue push)
    void log(uint32_t level, const char* message) {
        LogEntry entry;
        entry.timestamp = TSCTimestamp::read_tsc_fast();
        entry.level = level;
        strncpy(entry.message, message, sizeof(entry.message) - 1);
        entry.message[sizeof(entry.message) - 1] = '\0';

        queue_.push(entry); // Non-blocking
    }

    ~AsyncLogger() {
        running_.store(false);
        if (writer_thread_.joinable()) {
            writer_thread_.join();
        }
        if (fd_ >= 0) {
            close(fd_);
        }
    }

private:
    void writer_loop() {
        LogEntry entry;
        while (running_.load(std::memory_order_relaxed)) {
            if (queue_.pop(entry)) {
                // Write to disk (slow, but off critical path)
                write(fd_, &entry, sizeof(entry));
            } else {
                // Queue empty, sleep briefly
                usleep(100); // 100 microseconds
            }
        }

        // Flush remaining entries
        while (queue_.pop(entry)) {
            write(fd_, &entry, sizeof(entry));
        }
    }
};
```

================================================================================
7. SYNCHRONIZATION LATENCY (10ns - 10us)
================================================================================

7.1 MUTEX vs SPINLOCK vs ATOMIC
--------------------------------

```cpp
#include <mutex>
#include <atomic>
#include <pthread.h>

// Mutex-based counter (SLOW: 50-500ns per operation)
class MutexCounter {
private:
    std::mutex mutex_;
    uint64_t value_ = 0;

public:
    void increment() {
        std::lock_guard<std::mutex> lock(mutex_);
        value_++;
    }

    uint64_t get() {
        std::lock_guard<std::mutex> lock(mutex_);
        return value_;
    }
};

// Spinlock-based counter (MEDIUM: 20-100ns per operation)
class SpinlockCounter {
private:
    std::atomic_flag lock_ = ATOMIC_FLAG_INIT;
    uint64_t value_ = 0;

public:
    void increment() {
        while (lock_.test_and_set(std::memory_order_acquire)) {
            // Spin wait
            _mm_pause();
        }
        value_++;
        lock_.clear(std::memory_order_release);
    }

    uint64_t get() {
        while (lock_.test_and_set(std::memory_order_acquire)) {
            _mm_pause();
        }
        uint64_t val = value_;
        lock_.clear(std::memory_order_release);
        return val;
    }
};

// Atomic counter (FAST: 5-20ns per operation)
class AtomicCounter {
private:
    std::atomic<uint64_t> value_{0};

public:
    void increment() {
        value_.fetch_add(1, std::memory_order_relaxed);
    }

    uint64_t get() const {
        return value_.load(std::memory_order_relaxed);
    }
};

// Benchmark synchronization primitives
void benchmark_synchronization() {
    constexpr size_t iterations = 1000000;

    // Mutex
    {
        MutexCounter counter;
        auto start = std::chrono::high_resolution_clock::now();
        for (size_t i = 0; i < iterations; i++) {
            counter.increment();
        }
        auto end = std::chrono::high_resolution_clock::now();
        auto ns = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();
        printf("Mutex: %.2f ns/op\n", static_cast<double>(ns) / iterations);
    }

    // Spinlock
    {
        SpinlockCounter counter;
        auto start = std::chrono::high_resolution_clock::now();
        for (size_t i = 0; i < iterations; i++) {
            counter.increment();
        }
        auto end = std::chrono::high_resolution_clock::now();
        auto ns = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();
        printf("Spinlock: %.2f ns/op\n", static_cast<double>(ns) / iterations);
    }

    // Atomic
    {
        AtomicCounter counter;
        auto start = std::chrono::high_resolution_clock::now();
        for (size_t i = 0; i < iterations; i++) {
            counter.increment();
        }
        auto end = std::chrono::high_resolution_clock::now();
        auto ns = std::chrono::duration_cast<std::chrono::nanoseconds>(
            end - start).count();
        printf("Atomic: %.2f ns/op\n", static_cast<double>(ns) / iterations);
    }
}

// Typical results (no contention):
// Mutex: 35 ns/op
// Spinlock: 15 ns/op
// Atomic: 3 ns/op
```

7.2 MEMORY ORDERING
-------------------

```cpp
// Memory ordering impact on performance

// RELAXED: Fastest, no ordering guarantees (5ns)
void relaxed_store(std::atomic<uint64_t>& var, uint64_t value) {
    var.store(value, std::memory_order_relaxed);
}

// ACQUIRE/RELEASE: Medium, ensures ordering (8-15ns)
void release_store(std::atomic<uint64_t>& var, uint64_t value) {
    var.store(value, std::memory_order_release);
}

uint64_t acquire_load(const std::atomic<uint64_t>& var) {
    return var.load(std::memory_order_acquire);
}

// SEQ_CST: Slowest, sequential consistency (15-30ns)
void seqcst_store(std::atomic<uint64_t>& var, uint64_t value) {
    var.store(value, std::memory_order_seq_cst);
}

// Best practice: Use weakest ordering that maintains correctness
```

================================================================================
8. MEASUREMENT TECHNIQUES
================================================================================

```cpp
#include <hdrhistogram/hdr_histogram.h>

// High-precision latency measurement framework
class LatencyMeasurement {
private:
    struct hdr_histogram* histogram_;
    TSCTimestamp tsc_;

public:
    LatencyMeasurement() {
        // 1ns to 1s range, 3 significant digits
        hdr_init(1, 1000000000, 3, &histogram_);
    }

    ~LatencyMeasurement() {
        hdr_close(histogram_);
    }

    // Measure function latency
    template<typename Func>
    auto measure(Func&& func) {
        uint64_t start = tsc_.read_tsc();
        auto result = func();
        uint64_t end = tsc_.read_tsc();

        uint64_t latency_ns = tsc_.tsc_to_ns(end - start);
        hdr_record_value(histogram_, latency_ns);

        return result;
    }

    // Print detailed statistics
    void print_stats(const char* label) {
        printf("\n=== %s Latency Statistics ===\n", label);
        printf("Count: %lu\n", histogram_->total_count);
        printf("Min: %.2f ns\n", hdr_min(histogram_) * 1.0);
        printf("P50: %.2f ns\n", hdr_value_at_percentile(histogram_, 50.0) * 1.0);
        printf("P90: %.2f ns\n", hdr_value_at_percentile(histogram_, 90.0) * 1.0);
        printf("P99: %.2f ns\n", hdr_value_at_percentile(histogram_, 99.0) * 1.0);
        printf("P99.9: %.2f ns\n", hdr_value_at_percentile(histogram_, 99.9) * 1.0);
        printf("P99.99: %.2f ns\n", hdr_value_at_percentile(histogram_, 99.99) * 1.0);
        printf("Max: %.2f ns\n", hdr_max(histogram_) * 1.0);
        printf("Mean: %.2f ns\n", hdr_mean(histogram_));
        printf("StdDev: %.2f ns\n", hdr_stddev(histogram_));
    }
};
```

================================================================================
9. MITIGATION STRATEGIES
================================================================================

LATENCY MITIGATION CHECKLIST:

NETWORK LATENCY:
[X] Use kernel bypass (DPDK, OpenOnload)
[X] Colocate with exchange
[X] Use dedicated low-latency switches
[X] Enable hardware timestamping
[X] Optimize network topology

PROCESSING LATENCY:
[X] Use cache-friendly data structures
[X] Optimize algorithms (O(1) or O(log n))
[X] Prefetch data
[X] Avoid branches in hot path
[X] Use SIMD instructions

SERIALIZATION LATENCY:
[X] Use binary protocols (SBE, FlatBuffers)
[X] Implement zero-copy parsing
[X] Pre-allocate buffers
[X] Avoid dynamic allocations

QUEUE LATENCY:
[X] Use lock-free queues
[X] Batch operations
[X] Size queues appropriately
[X] Align to cache lines

I/O LATENCY:
[X] Move I/O off critical path
[X] Use async I/O
[X] Use memory-mapped files
[X] Consider persistent memory (Optane)

SYNCHRONIZATION LATENCY:
[X] Use atomics instead of locks
[X] Use weakest memory ordering
[X] Avoid false sharing
[X] Minimize shared state

================================================================================
END OF DOCUMENT
================================================================================
