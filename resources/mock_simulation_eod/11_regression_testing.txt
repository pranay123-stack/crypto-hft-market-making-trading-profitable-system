================================================================================
REGRESSION TESTING
Strategy Regression Tests and Performance Comparison
================================================================================

VERSION: 1.0
DOCUMENT: Automated Regression Testing Framework
LAST UPDATED: 2025-11-26

================================================================================
TABLE OF CONTENTS
================================================================================

1. Overview
2. Test Suite Design
3. Performance Benchmarking
4. Automated Test Execution
5. Golden Run Comparison
6. Performance Degradation Detection
7. Continuous Integration
8. Complete Implementation

================================================================================
1. OVERVIEW
================================================================================

The Regression Testing Framework ensures strategy modifications don't degrade
performance through automated testing against historical baselines.

KEY FEATURES:
-------------
- Automated test suite execution
- Golden run baseline comparison
- Performance regression detection
- Historical backtest comparison
- Continuous integration support
- Automated alerting on degradation
- Test coverage metrics
- Performance trending

TEST TYPES:
-----------
1. Unit Tests: Individual component validation
2. Integration Tests: End-to-end workflow
3. Performance Tests: Latency and throughput
4. Accuracy Tests: P&L and signal correctness
5. Stability Tests: Consistent behavior
6. Benchmark Tests: Against known baselines

================================================================================
2. TEST SUITE DESIGN
================================================================================

2.1 TEST CASE STRUCTURE
------------------------

enum class TestStatus : uint8_t {
    PASSED,
    FAILED,
    SKIPPED,
    ERROR
};

enum class TestSeverity : uint8_t {
    CRITICAL,   // Blocker - must pass
    HIGH,       // Important - should pass
    MEDIUM,     // Normal - investigate if fails
    LOW         // Informational
};

struct TestCase {
    std::string test_id;
    std::string test_name;
    std::string description;
    TestSeverity severity;

    // Test configuration
    std::string strategy_name;
    std::string data_file;
    ParameterSet parameters;

    // Expected results (golden baseline)
    struct ExpectedResults {
        double expected_sharpe;
        double expected_return;
        double expected_max_dd;
        size_t expected_trades;

        // Tolerance ranges
        double sharpe_tolerance;
        double return_tolerance;
        double max_dd_tolerance;
        double trade_count_tolerance_pct;

        ExpectedResults()
            : expected_sharpe(0), expected_return(0),
              expected_max_dd(0), expected_trades(0),
              sharpe_tolerance(0.05), return_tolerance(0.01),
              max_dd_tolerance(0.005), trade_count_tolerance_pct(0.1) {}
    } expected;

    // Actual results
    struct ActualResults {
        double actual_sharpe;
        double actual_return;
        double actual_max_dd;
        size_t actual_trades;
        uint64_t execution_time_ms;

        ActualResults()
            : actual_sharpe(0), actual_return(0),
              actual_max_dd(0), actual_trades(0),
              execution_time_ms(0) {}
    } actual;

    // Test result
    TestStatus status;
    std::string failure_message;
    std::vector<std::string> warnings;

    TestCase() : severity(TestSeverity::MEDIUM), status(TestStatus::SKIPPED) {}
};

struct TestSuite {
    std::string suite_name;
    std::string version;
    std::vector<TestCase> test_cases;

    // Suite-level statistics
    size_t total_tests;
    size_t passed_tests;
    size_t failed_tests;
    size_t skipped_tests;
    size_t error_tests;

    double pass_rate;
    uint64_t total_execution_time_ms;

    TestSuite()
        : total_tests(0), passed_tests(0), failed_tests(0),
          skipped_tests(0), error_tests(0), pass_rate(0),
          total_execution_time_ms(0) {}

    void calculateStatistics() {
        total_tests = test_cases.size();
        passed_tests = std::count_if(test_cases.begin(), test_cases.end(),
            [](const TestCase& tc) { return tc.status == TestStatus::PASSED; });
        failed_tests = std::count_if(test_cases.begin(), test_cases.end(),
            [](const TestCase& tc) { return tc.status == TestStatus::FAILED; });
        skipped_tests = std::count_if(test_cases.begin(), test_cases.end(),
            [](const TestCase& tc) { return tc.status == TestStatus::SKIPPED; });
        error_tests = std::count_if(test_cases.begin(), test_cases.end(),
            [](const TestCase& tc) { return tc.status == TestStatus::ERROR; });

        pass_rate = total_tests > 0 ?
                   static_cast<double>(passed_tests) / total_tests : 0.0;

        total_execution_time_ms = std::accumulate(test_cases.begin(),
                                                  test_cases.end(), 0ULL,
            [](uint64_t sum, const TestCase& tc) {
                return sum + tc.actual.execution_time_ms;
            });
    }
};

2.2 TEST RUNNER
---------------

class RegressionTestRunner {
public:
    struct Config {
        bool fail_fast;             // Stop on first failure
        bool parallel_execution;    // Run tests in parallel
        size_t max_parallel_tests;  // Max concurrent tests
        bool update_baselines;      // Update golden baselines
        std::string output_dir;     // Test results directory

        Config() : fail_fast(false), parallel_execution(true),
                  max_parallel_tests(4), update_baselines(false),
                  output_dir("test_results") {}
    };

    explicit RegressionTestRunner(const Config& config)
        : config_(config) {}

    TestSuite runTestSuite(const TestSuite& suite) {
        TestSuite results = suite;
        results.test_cases.clear();

        std::cout << "Running test suite: " << suite.suite_name << std::endl;
        std::cout << "Total tests: " << suite.test_cases.size() << std::endl;

        if (config_.parallel_execution) {
            results.test_cases = runParallel(suite.test_cases);
        } else {
            results.test_cases = runSequential(suite.test_cases);
        }

        results.calculateStatistics();

        // Generate report
        generateTestReport(results);

        return results;
    }

private:
    Config config_;

    std::vector<TestCase> runSequential(const std::vector<TestCase>& tests) {
        std::vector<TestCase> results;

        for (const auto& test : tests) {
            auto result = runSingleTest(test);
            results.push_back(result);

            if (config_.fail_fast && result.status == TestStatus::FAILED) {
                std::cout << "Fail fast enabled. Stopping on first failure." << std::endl;
                break;
            }
        }

        return results;
    }

    std::vector<TestCase> runParallel(const std::vector<TestCase>& tests) {
        std::vector<TestCase> results(tests.size());
        std::atomic<size_t> next_test{0};
        std::atomic<bool> should_stop{false};

        auto worker = [&]() {
            while (true) {
                if (should_stop.load()) break;

                size_t idx = next_test.fetch_add(1);
                if (idx >= tests.size()) break;

                results[idx] = runSingleTest(tests[idx]);

                if (config_.fail_fast &&
                    results[idx].status == TestStatus::FAILED) {
                    should_stop.store(true);
                }
            }
        };

        // Launch worker threads
        std::vector<std::thread> threads;
        for (size_t i = 0; i < config_.max_parallel_tests; ++i) {
            threads.emplace_back(worker);
        }

        // Wait for completion
        for (auto& thread : threads) {
            thread.join();
        }

        return results;
    }

    TestCase runSingleTest(const TestCase& test) {
        TestCase result = test;

        std::cout << "Running test: " << test.test_name << "..." << std::endl;

        auto start_time = std::chrono::steady_clock::now();

        try {
            // Load strategy
            auto strategy = loadStrategy(test.strategy_name);
            strategy->setParameters(test.parameters);

            // Load market data
            auto market_data = loadMarketData(test.data_file);

            // Run backtest
            SimulationEngine sim;
            // ... setup simulation ...
            auto sim_results = sim.run();

            // Extract metrics
            result.actual.actual_sharpe = calculateSharpe(sim_results);
            result.actual.actual_return = calculateReturn(sim_results);
            result.actual.actual_max_dd = calculateMaxDrawdown(sim_results);
            result.actual.actual_trades = sim_results.trades.size();

            // Validate against expected results
            validateResults(result);

        } catch (const std::exception& e) {
            result.status = TestStatus::ERROR;
            result.failure_message = std::string("Exception: ") + e.what();
        }

        auto end_time = std::chrono::steady_clock::now();
        result.actual.execution_time_ms =
            std::chrono::duration_cast<std::chrono::milliseconds>(
                end_time - start_time).count();

        return result;
    }

    void validateResults(TestCase& test) {
        std::vector<std::string> failures;

        // Check Sharpe ratio
        double sharpe_diff = std::abs(test.actual.actual_sharpe -
                                     test.expected.expected_sharpe);
        if (sharpe_diff > test.expected.sharpe_tolerance) {
            std::ostringstream msg;
            msg << "Sharpe ratio deviation: expected "
                << test.expected.expected_sharpe << ", got "
                << test.actual.actual_sharpe;
            failures.push_back(msg.str());
        }

        // Check return
        double return_diff = std::abs(test.actual.actual_return -
                                     test.expected.expected_return);
        if (return_diff > test.expected.return_tolerance) {
            std::ostringstream msg;
            msg << "Return deviation: expected "
                << test.expected.expected_return << ", got "
                << test.actual.actual_return;
            failures.push_back(msg.str());
        }

        // Check max drawdown
        double dd_diff = std::abs(test.actual.actual_max_dd -
                                 test.expected.expected_max_dd);
        if (dd_diff > test.expected.max_dd_tolerance) {
            std::ostringstream msg;
            msg << "Max drawdown deviation: expected "
                << test.expected.expected_max_dd << ", got "
                << test.actual.actual_max_dd;
            failures.push_back(msg.str());
        }

        // Check trade count
        double trade_diff_pct = std::abs(
            static_cast<double>(test.actual.actual_trades) -
            test.expected.expected_trades) / test.expected.expected_trades;
        if (trade_diff_pct > test.expected.trade_count_tolerance_pct) {
            std::ostringstream msg;
            msg << "Trade count deviation: expected "
                << test.expected.expected_trades << ", got "
                << test.actual.actual_trades;
            failures.push_back(msg.str());
        }

        // Set test status
        if (failures.empty()) {
            test.status = TestStatus::PASSED;
        } else {
            test.status = TestStatus::FAILED;
            test.failure_message = "Validation failures:\n";
            for (const auto& failure : failures) {
                test.failure_message += "  - " + failure + "\n";
            }
        }
    }

    void generateTestReport(const TestSuite& results) {
        std::ofstream report(config_.output_dir + "/test_report.txt");

        report << "=== REGRESSION TEST REPORT ===" << std::endl;
        report << "Suite: " << results.suite_name << std::endl;
        report << "Version: " << results.version << std::endl;
        report << "Date: " << getCurrentDateTime() << std::endl;
        report << std::endl;

        report << "Summary:" << std::endl;
        report << "  Total: " << results.total_tests << std::endl;
        report << "  Passed: " << results.passed_tests << std::endl;
        report << "  Failed: " << results.failed_tests << std::endl;
        report << "  Skipped: " << results.skipped_tests << std::endl;
        report << "  Error: " << results.error_tests << std::endl;
        report << "  Pass Rate: " << (results.pass_rate * 100) << "%" << std::endl;
        report << "  Execution Time: " << results.total_execution_time_ms
               << " ms" << std::endl;
        report << std::endl;

        // Detailed results
        report << "Detailed Results:" << std::endl;
        for (const auto& test : results.test_cases) {
            report << std::endl;
            report << "Test: " << test.test_name << std::endl;
            report << "  Status: " << getStatusString(test.status) << std::endl;

            if (test.status == TestStatus::FAILED) {
                report << "  Failure: " << test.failure_message << std::endl;
            }

            report << "  Expected vs Actual:" << std::endl;
            report << "    Sharpe: " << test.expected.expected_sharpe
                   << " vs " << test.actual.actual_sharpe << std::endl;
            report << "    Return: " << test.expected.expected_return
                   << " vs " << test.actual.actual_return << std::endl;
            report << "    Max DD: " << test.expected.expected_max_dd
                   << " vs " << test.actual.actual_max_dd << std::endl;
            report << "    Trades: " << test.expected.expected_trades
                   << " vs " << test.actual.actual_trades << std::endl;
        }

        std::cout << "Test report written to: "
                 << config_.output_dir << "/test_report.txt" << std::endl;
    }

    std::string getStatusString(TestStatus status) {
        switch (status) {
            case TestStatus::PASSED: return "PASSED";
            case TestStatus::FAILED: return "FAILED";
            case TestStatus::SKIPPED: return "SKIPPED";
            case TestStatus::ERROR: return "ERROR";
            default: return "UNKNOWN";
        }
    }

    std::string getCurrentDateTime() {
        auto now = std::chrono::system_clock::now();
        auto time_t = std::chrono::system_clock::to_time_t(now);
        std::string time_str = std::ctime(&time_t);
        time_str.pop_back();  // Remove newline
        return time_str;
    }

    std::shared_ptr<IStrategy> loadStrategy(const std::string& name) {
        // Strategy loading implementation
        return nullptr;
    }

    std::vector<MarketData> loadMarketData(const std::string& file) {
        // Data loading implementation
        return {};
    }

    double calculateSharpe(const SimulationResults& results) {
        // Implementation
        return 0.0;
    }

    double calculateReturn(const SimulationResults& results) {
        // Implementation
        return 0.0;
    }

    double calculateMaxDrawdown(const SimulationResults& results) {
        // Implementation
        return 0.0;
    }
};

================================================================================
3. PERFORMANCE BENCHMARKING
================================================================================

3.1 BENCHMARK SUITE
--------------------

class PerformanceBenchmark {
public:
    struct BenchmarkResult {
        std::string benchmark_name;
        uint64_t execution_time_ns;
        uint64_t memory_usage_bytes;
        double throughput_ops_per_sec;

        // Percentiles
        uint64_t p50_latency_ns;
        uint64_t p95_latency_ns;
        uint64_t p99_latency_ns;
        uint64_t max_latency_ns;

        BenchmarkResult() : execution_time_ns(0), memory_usage_bytes(0),
                          throughput_ops_per_sec(0), p50_latency_ns(0),
                          p95_latency_ns(0), p99_latency_ns(0),
                          max_latency_ns(0) {}
    };

    static BenchmarkResult benchmarkStrategy(IStrategy& strategy,
                                            const std::vector<MarketData>& data,
                                            size_t iterations = 10) {
        BenchmarkResult result;
        result.benchmark_name = strategy.getName();

        std::vector<uint64_t> latencies;
        uint64_t total_time = 0;

        for (size_t i = 0; i < iterations; ++i) {
            auto start = std::chrono::high_resolution_clock::now();

            // Run strategy
            for (const auto& market_data : data) {
                strategy.onMarketData(market_data);
            }

            auto end = std::chrono::high_resolution_clock::now();
            uint64_t iteration_time = std::chrono::duration_cast<
                std::chrono::nanoseconds>(end - start).count();

            latencies.push_back(iteration_time);
            total_time += iteration_time;
        }

        // Calculate statistics
        result.execution_time_ns = total_time / iterations;

        std::sort(latencies.begin(), latencies.end());
        result.p50_latency_ns = latencies[latencies.size() * 50 / 100];
        result.p95_latency_ns = latencies[latencies.size() * 95 / 100];
        result.p99_latency_ns = latencies[latencies.size() * 99 / 100];
        result.max_latency_ns = latencies.back();

        // Throughput
        double total_sec = total_time / 1e9;
        result.throughput_ops_per_sec = (data.size() * iterations) / total_sec;

        return result;
    }
};

================================================================================
8. COMPLETE IMPLEMENTATION - USAGE EXAMPLE
================================================================================

#include "regression_testing.hpp"
#include <iostream>

int main() {
    using namespace hft::simulation;

    // Create test suite
    TestSuite suite;
    suite.suite_name = "Strategy Regression Tests";
    suite.version = "1.0.0";

    // Add test cases
    TestCase test1;
    test1.test_id = "TEST-001";
    test1.test_name = "Momentum Strategy - Bull Market";
    test1.description = "Test momentum strategy on bull market data";
    test1.severity = TestSeverity::CRITICAL;
    test1.strategy_name = "MomentumStrategy";
    test1.data_file = "data/bull_market_2023.csv";

    // Set expected results (golden baseline)
    test1.expected.expected_sharpe = 2.5;
    test1.expected.expected_return = 0.15;
    test1.expected.expected_max_dd = 0.05;
    test1.expected.expected_trades = 250;

    suite.test_cases.push_back(test1);

    // Add more test cases...
    TestCase test2;
    test2.test_id = "TEST-002";
    test2.test_name = "Momentum Strategy - Bear Market";
    test2.severity = TestSeverity::HIGH;
    test2.strategy_name = "MomentumStrategy";
    test2.data_file = "data/bear_market_2022.csv";
    test2.expected.expected_sharpe = 1.8;
    test2.expected.expected_return = 0.08;

    suite.test_cases.push_back(test2);

    // Configure test runner
    RegressionTestRunner::Config config;
    config.parallel_execution = true;
    config.max_parallel_tests = 4;
    config.fail_fast = false;
    config.output_dir = "test_results";

    // Run tests
    RegressionTestRunner runner(config);
    auto results = runner.runTestSuite(suite);

    // Print summary
    std::cout << "\n=== TEST SUMMARY ===" << std::endl;
    std::cout << "Total: " << results.total_tests << std::endl;
    std::cout << "Passed: " << results.passed_tests << std::endl;
    std::cout << "Failed: " << results.failed_tests << std::endl;
    std::cout << "Pass Rate: " << (results.pass_rate * 100) << "%" << std::endl;

    // Exit code based on results
    return results.failed_tests > 0 ? 1 : 0;
}

================================================================================
END OF DOCUMENT
================================================================================
