================================================================================
A/B TESTING FRAMEWORK
Strategy Comparison and Statistical Validation
================================================================================

VERSION: 1.0
DOCUMENT: A/B Testing Strategies in Simulation
LAST UPDATED: 2025-11-26

================================================================================
TABLE OF CONTENTS
================================================================================

1. Overview
2. A/B Test Design
3. Statistical Significance Testing
4. Sample Size Calculation
5. Multi-Armed Bandit Testing
6. Champion/Challenger Framework
7. Performance Comparison
8. Complete Implementation

================================================================================
1. OVERVIEW
================================================================================

The A/B Testing Framework enables rigorous comparison of trading strategies
through controlled experiments with statistical validation.

KEY FEATURES:
-------------
- Parallel strategy execution
- Statistical significance testing
- Sample size calculation
- Power analysis
- Multiple comparison correction
- Bootstrap confidence intervals
- Bayesian A/B testing
- Multi-armed bandit optimization

TEST TYPES:
-----------
1. Strategy vs Strategy: Compare two complete strategies
2. Parameter vs Parameter: Compare parameter variations
3. Feature vs Feature: Test individual signal components
4. Execution vs Execution: Compare execution algorithms
5. Champion/Challenger: Ongoing production testing

================================================================================
2. A/B TEST DESIGN
================================================================================

2.1 TEST CONFIGURATION
----------------------

struct ABTestConfig {
    std::string test_name;
    std::string variant_a_name;  // Control/baseline
    std::string variant_b_name;  // Treatment/challenger

    // Statistical parameters
    double significance_level;    // Alpha (e.g., 0.05 for 95% confidence)
    double minimum_effect_size;   // Minimum detectable effect (e.g., 0.05 = 5%)
    double statistical_power;     // Power (e.g., 0.80 for 80% power)

    // Sample requirements
    size_t min_trades_per_variant;
    size_t min_days;
    double min_notional_per_variant;

    // Allocation
    double traffic_split;  // % to variant B (0.5 = 50/50 split)

    // Early stopping
    bool enable_early_stopping;
    double early_stop_threshold;  // p-value for early stop

    ABTestConfig()
        : significance_level(0.05), minimum_effect_size(0.05),
          statistical_power(0.80), min_trades_per_variant(30),
          min_days(10), min_notional_per_variant(100000.0),
          traffic_split(0.5), enable_early_stopping(false),
          early_stop_threshold(0.001) {}
};

struct VariantResults {
    std::string variant_name;

    // Performance metrics
    double mean_return;
    double std_dev_return;
    double sharpe_ratio;
    double max_drawdown;
    double win_rate;
    double profit_factor;

    // Sample statistics
    size_t num_trades;
    size_t num_days;
    double total_notional;

    // Distribution
    std::vector<double> daily_returns;
    std::vector<double> trade_pnls;

    VariantResults()
        : mean_return(0), std_dev_return(0), sharpe_ratio(0),
          max_drawdown(0), win_rate(0), profit_factor(0),
          num_trades(0), num_days(0), total_notional(0) {}
};

struct ABTestResult {
    ABTestConfig config;
    VariantResults variant_a;
    VariantResults variant_b;

    // Statistical test results
    double p_value;
    double confidence_level;
    bool is_significant;
    std::string winner;  // "A", "B", or "No significant difference"

    // Effect size
    double absolute_difference;
    double relative_difference_pct;
    double cohens_d;  // Standardized effect size

    // Confidence intervals
    double ci_lower;
    double ci_upper;

    // Recommendations
    bool sufficient_sample_size;
    std::string recommendation;

    ABTestResult()
        : p_value(1.0), confidence_level(0), is_significant(false),
          absolute_difference(0), relative_difference_pct(0),
          cohens_d(0), ci_lower(0), ci_upper(0),
          sufficient_sample_size(false) {}
};

2.2 A/B TEST RUNNER
--------------------

class ABTestRunner {
public:
    explicit ABTestRunner(const ABTestConfig& config)
        : config_(config), rng_(std::random_device{}()) {}

    ABTestResult runTest(IStrategy& strategy_a,
                        IStrategy& strategy_b,
                        const std::vector<MarketData>& market_data) {

        ABTestResult result;
        result.config = config_;

        // Run both strategies in parallel
        std::cout << "Running A/B test: " << config_.test_name << std::endl;

        // Variant A (Control)
        std::cout << "Running variant A: " << config_.variant_a_name << "..." << std::endl;
        result.variant_a = runVariant(strategy_a, market_data, config_.variant_a_name);

        // Variant B (Treatment)
        std::cout << "Running variant B: " << config_.variant_b_name << "..." << std::endl;
        result.variant_b = runVariant(strategy_b, market_data, config_.variant_b_name);

        // Statistical analysis
        performStatisticalAnalysis(result);

        // Generate recommendation
        generateRecommendation(result);

        return result;
    }

private:
    ABTestConfig config_;
    std::mt19937_64 rng_;

    VariantResults runVariant(IStrategy& strategy,
                             const std::vector<MarketData>& market_data,
                             const std::string& variant_name) {

        VariantResults results;
        results.variant_name = variant_name;

        // Run simulation
        SimulationEngine sim;
        // ... setup simulation with strategy and market data ...

        auto sim_results = sim.run();

        // Extract metrics
        results.num_trades = sim_results.trades.size();
        results.total_notional = sim_results.total_notional;
        results.daily_returns = sim_results.daily_returns;

        // Calculate performance metrics
        results.mean_return = calculateMean(results.daily_returns);
        results.std_dev_return = calculateStdDev(results.daily_returns);
        results.sharpe_ratio = calculateSharpeRatio(results.daily_returns);
        results.max_drawdown = calculateMaxDrawdown(results.daily_returns);

        // Trade statistics
        for (const auto& trade : sim_results.trades) {
            results.trade_pnls.push_back(trade.realized_pnl);
        }

        results.win_rate = calculateWinRate(results.trade_pnls);
        results.profit_factor = calculateProfitFactor(results.trade_pnls);

        return results;
    }

    void performStatisticalAnalysis(ABTestResult& result) {
        // Check sample size
        result.sufficient_sample_size =
            result.variant_a.num_trades >= config_.min_trades_per_variant &&
            result.variant_b.num_trades >= config_.min_trades_per_variant;

        if (!result.sufficient_sample_size) {
            result.recommendation = "Insufficient sample size. Continue testing.";
            return;
        }

        // Perform two-sample t-test
        result.p_value = performTTest(result.variant_a.daily_returns,
                                      result.variant_b.daily_returns);

        result.is_significant = result.p_value < config_.significance_level;
        result.confidence_level = (1.0 - result.p_value) * 100.0;

        // Calculate effect size
        result.absolute_difference = result.variant_b.mean_return -
                                    result.variant_a.mean_return;

        if (result.variant_a.mean_return != 0) {
            result.relative_difference_pct =
                (result.absolute_difference / result.variant_a.mean_return) * 100.0;
        }

        result.cohens_d = calculateCohensD(result.variant_a.daily_returns,
                                          result.variant_b.daily_returns);

        // Bootstrap confidence intervals
        auto [ci_lower, ci_upper] = bootstrapConfidenceInterval(
            result.variant_a.daily_returns,
            result.variant_b.daily_returns,
            1.0 - config_.significance_level);

        result.ci_lower = ci_lower;
        result.ci_upper = ci_upper;

        // Determine winner
        if (result.is_significant) {
            if (result.variant_b.mean_return > result.variant_a.mean_return) {
                result.winner = "B";
            } else {
                result.winner = "A";
            }
        } else {
            result.winner = "No significant difference";
        }
    }

    double performTTest(const std::vector<double>& sample_a,
                       const std::vector<double>& sample_b) {
        // Welch's t-test (unequal variances)
        double mean_a = calculateMean(sample_a);
        double mean_b = calculateMean(sample_b);

        double var_a = calculateVariance(sample_a);
        double var_b = calculateVariance(sample_b);

        size_t n_a = sample_a.size();
        size_t n_b = sample_b.size();

        if (n_a == 0 || n_b == 0) return 1.0;

        // Welch's t-statistic
        double t_stat = (mean_b - mean_a) / std::sqrt(var_a / n_a + var_b / n_b);

        // Degrees of freedom (Welch-Satterthwaite)
        double num = std::pow(var_a / n_a + var_b / n_b, 2);
        double denom = std::pow(var_a / n_a, 2) / (n_a - 1) +
                       std::pow(var_b / n_b, 2) / (n_b - 1);
        double df = num / denom;

        // Convert to p-value (two-tailed)
        // Simplified: use normal approximation for large samples
        double p_value = 2.0 * (1.0 - normalCDF(std::abs(t_stat)));

        return p_value;
    }

    double calculateCohensD(const std::vector<double>& sample_a,
                           const std::vector<double>& sample_b) {
        double mean_a = calculateMean(sample_a);
        double mean_b = calculateMean(sample_b);

        double var_a = calculateVariance(sample_a);
        double var_b = calculateVariance(sample_b);

        size_t n_a = sample_a.size();
        size_t n_b = sample_b.size();

        // Pooled standard deviation
        double pooled_std = std::sqrt(((n_a - 1) * var_a + (n_b - 1) * var_b) /
                                     (n_a + n_b - 2));

        if (pooled_std == 0) return 0.0;

        return (mean_b - mean_a) / pooled_std;
    }

    std::pair<double, double> bootstrapConfidenceInterval(
        const std::vector<double>& sample_a,
        const std::vector<double>& sample_b,
        double confidence_level,
        size_t num_bootstraps = 10000) {

        std::vector<double> bootstrap_diffs;
        bootstrap_diffs.reserve(num_bootstraps);

        std::uniform_int_distribution<size_t> dist_a(0, sample_a.size() - 1);
        std::uniform_int_distribution<size_t> dist_b(0, sample_b.size() - 1);

        for (size_t i = 0; i < num_bootstraps; ++i) {
            // Bootstrap sample A
            double boot_mean_a = 0.0;
            for (size_t j = 0; j < sample_a.size(); ++j) {
                boot_mean_a += sample_a[dist_a(rng_)];
            }
            boot_mean_a /= sample_a.size();

            // Bootstrap sample B
            double boot_mean_b = 0.0;
            for (size_t j = 0; j < sample_b.size(); ++j) {
                boot_mean_b += sample_b[dist_b(rng_)];
            }
            boot_mean_b /= sample_b.size();

            bootstrap_diffs.push_back(boot_mean_b - boot_mean_a);
        }

        std::sort(bootstrap_diffs.begin(), bootstrap_diffs.end());

        double alpha = 1.0 - confidence_level;
        size_t lower_idx = static_cast<size_t>(alpha / 2.0 * num_bootstraps);
        size_t upper_idx = static_cast<size_t>((1.0 - alpha / 2.0) * num_bootstraps);

        return {bootstrap_diffs[lower_idx], bootstrap_diffs[upper_idx]};
    }

    void generateRecommendation(ABTestResult& result) {
        if (!result.sufficient_sample_size) {
            result.recommendation = "Continue testing - insufficient sample size";
            return;
        }

        if (!result.is_significant) {
            result.recommendation = "No significant difference detected. Consider:\n"
                                   "- Continuing test for more data\n"
                                   "- Testing with different market conditions\n"
                                   "- Revising strategies for larger effect size";
            return;
        }

        if (result.winner == "B") {
            double improvement = result.relative_difference_pct;

            std::ostringstream rec;
            rec << "Variant B (" << result.config.variant_b_name << ") wins!\n";
            rec << "- Improvement: " << improvement << "%\n";
            rec << "- Confidence: " << result.confidence_level << "%\n";
            rec << "- Effect size (Cohen's d): " << result.cohens_d << "\n";

            if (std::abs(result.cohens_d) > 0.8) {
                rec << "- Large effect size - strong evidence for deployment\n";
            } else if (std::abs(result.cohens_d) > 0.5) {
                rec << "- Medium effect size - recommend deployment\n";
            } else {
                rec << "- Small effect size - consider cost/benefit\n";
            }

            result.recommendation = rec.str();
        } else {
            result.recommendation = "Variant A (control) performs better. "
                                   "Do not deploy variant B.";
        }
    }

    // Helper statistical functions
    double calculateMean(const std::vector<double>& values) {
        if (values.empty()) return 0.0;
        return std::accumulate(values.begin(), values.end(), 0.0) / values.size();
    }

    double calculateVariance(const std::vector<double>& values) {
        if (values.empty()) return 0.0;

        double mean = calculateMean(values);
        double sq_sum = 0.0;

        for (double val : values) {
            sq_sum += (val - mean) * (val - mean);
        }

        return sq_sum / values.size();
    }

    double calculateStdDev(const std::vector<double>& values) {
        return std::sqrt(calculateVariance(values));
    }

    double calculateSharpeRatio(const std::vector<double>& returns) {
        if (returns.empty()) return 0.0;

        double mean = calculateMean(returns);
        double std_dev = calculateStdDev(returns);

        if (std_dev == 0) return 0.0;
        return mean / std_dev * std::sqrt(252.0);
    }

    double calculateMaxDrawdown(const std::vector<double>& returns) {
        double cumulative = 0.0;
        double peak = 0.0;
        double max_dd = 0.0;

        for (double ret : returns) {
            cumulative += ret;
            peak = std::max(peak, cumulative);
            double dd = peak - cumulative;
            max_dd = std::max(max_dd, dd);
        }

        return max_dd;
    }

    double calculateWinRate(const std::vector<double>& pnls) {
        if (pnls.empty()) return 0.0;

        size_t wins = std::count_if(pnls.begin(), pnls.end(),
            [](double pnl) { return pnl > 0; });

        return static_cast<double>(wins) / pnls.size();
    }

    double calculateProfitFactor(const std::vector<double>& pnls) {
        double total_profit = 0.0;
        double total_loss = 0.0;

        for (double pnl : pnls) {
            if (pnl > 0) total_profit += pnl;
            else total_loss += std::abs(pnl);
        }

        if (total_loss == 0) return total_profit > 0 ? 1000.0 : 0.0;
        return total_profit / total_loss;
    }

    double normalCDF(double x) {
        // Approximation of standard normal CDF
        return 0.5 * std::erfc(-x * M_SQRT1_2);
    }
};

================================================================================
3. STATISTICAL SIGNIFICANCE TESTING
================================================================================

3.1 MULTIPLE TESTING CORRECTION
--------------------------------

class MultipleTestingCorrection {
public:
    // Bonferroni correction
    static double bonferroni(double alpha, size_t num_tests) {
        return alpha / num_tests;
    }

    // Benjamini-Hochberg (FDR control)
    static std::vector<bool> benjaminiHochberg(
        const std::vector<double>& p_values,
        double fdr_level) {

        size_t n = p_values.size();
        std::vector<size_t> indices(n);
        std::iota(indices.begin(), indices.end(), 0);

        // Sort p-values
        std::sort(indices.begin(), indices.end(),
            [&p_values](size_t i, size_t j) {
                return p_values[i] < p_values[j];
            });

        std::vector<bool> is_significant(n, false);

        // Find largest i where p_i <= (i/n) * FDR
        for (int i = n - 1; i >= 0; --i) {
            double threshold = (static_cast<double>(i + 1) / n) * fdr_level;

            if (p_values[indices[i]] <= threshold) {
                // Mark this and all smaller p-values as significant
                for (size_t j = 0; j <= static_cast<size_t>(i); ++j) {
                    is_significant[indices[j]] = true;
                }
                break;
            }
        }

        return is_significant;
    }
};

================================================================================
4. SAMPLE SIZE CALCULATION
================================================================================

4.1 SAMPLE SIZE CALCULATOR
---------------------------

class SampleSizeCalculator {
public:
    struct Requirements {
        double effect_size;        // Expected effect (e.g., 0.05 = 5%)
        double significance_level; // Alpha (e.g., 0.05)
        double statistical_power;  // Power (e.g., 0.80)
        double baseline_std_dev;   // Std dev of baseline metric

        Requirements()
            : effect_size(0.05), significance_level(0.05),
              statistical_power(0.80), baseline_std_dev(0.02) {}
    };

    static size_t calculateSampleSize(const Requirements& req) {
        // Formula for two-sample t-test
        double z_alpha = normalInverseCDF(1.0 - req.significance_level / 2.0);
        double z_beta = normalInverseCDF(req.statistical_power);

        double numerator = 2.0 * std::pow(req.baseline_std_dev, 2) *
                          std::pow(z_alpha + z_beta, 2);
        double denominator = std::pow(req.effect_size, 2);

        return static_cast<size_t>(std::ceil(numerator / denominator));
    }

    static double calculateMinimumDetectableEffect(
        size_t sample_size,
        double significance_level,
        double statistical_power,
        double baseline_std_dev) {

        double z_alpha = normalInverseCDF(1.0 - significance_level / 2.0);
        double z_beta = normalInverseCDF(statistical_power);

        double numerator = (z_alpha + z_beta) * baseline_std_dev;
        double denominator = std::sqrt(static_cast<double>(sample_size) / 2.0);

        return numerator / denominator;
    }

private:
    static double normalInverseCDF(double p) {
        // Approximation of inverse normal CDF
        // Using Beasley-Springer-Moro algorithm (simplified)
        if (p <= 0 || p >= 1) return 0.0;

        double q = p - 0.5;

        if (std::abs(q) <= 0.425) {
            double r = 0.180625 - q * q;
            return q * (((((((2.5090809287301226727e+3 * r +
                           3.3430575583588128105e+4) * r +
                          6.7265770927008700853e+4) * r +
                         4.5921953931549871457e+4) * r +
                        1.3731693765509461125e+4) * r +
                       1.9715909503065514427e+3) * r +
                      1.3314166789178437745e+2) * r +
                     3.3871328727963666080e+0) /
                   (((((((5.2264952788528545610e+3 * r +
                         2.8729085735721942674e+4) * r +
                        3.9307895800092710610e+4) * r +
                       2.1213794301586595867e+4) * r +
                      5.3941960214247511077e+3) * r +
                     6.8718700749205790830e+2) * r +
                    4.2313330701600911252e+1) * r + 1.0);
        }

        return 0.0;  // Simplified
    }
};

================================================================================
8. COMPLETE IMPLEMENTATION - USAGE EXAMPLE
================================================================================

#include "ab_testing.hpp"
#include <iostream>

int main() {
    using namespace hft::simulation;

    // Configure A/B test
    ABTestConfig config;
    config.test_name = "Momentum vs Mean Reversion";
    config.variant_a_name = "Momentum Strategy";
    config.variant_b_name = "Mean Reversion Strategy";
    config.significance_level = 0.05;
    config.minimum_effect_size = 0.05;  // 5% improvement
    config.statistical_power = 0.80;
    config.min_trades_per_variant = 50;

    // Calculate required sample size
    SampleSizeCalculator::Requirements sample_req;
    sample_req.effect_size = 0.05;
    sample_req.baseline_std_dev = 0.02;
    size_t required_samples = SampleSizeCalculator::calculateSampleSize(sample_req);

    std::cout << "Required sample size per variant: " << required_samples << std::endl;

    // Create strategies
    MomentumStrategy strategy_a;
    MeanReversionStrategy strategy_b;

    // Load market data
    std::vector<MarketData> market_data;
    // ... load historical data ...

    // Run A/B test
    ABTestRunner test_runner(config);
    auto result = test_runner.runTest(strategy_a, strategy_b, market_data);

    // Print results
    std::cout << "\n=== A/B Test Results ===" << std::endl;
    std::cout << "Test: " << config.test_name << std::endl;
    std::cout << "\nVariant A (" << result.variant_a.variant_name << "):" << std::endl;
    std::cout << "  Mean Return: " << result.variant_a.mean_return * 100 << "%" << std::endl;
    std::cout << "  Sharpe Ratio: " << result.variant_a.sharpe_ratio << std::endl;
    std::cout << "  Trades: " << result.variant_a.num_trades << std::endl;

    std::cout << "\nVariant B (" << result.variant_b.variant_name << "):" << std::endl;
    std::cout << "  Mean Return: " << result.variant_b.mean_return * 100 << "%" << std::endl;
    std::cout << "  Sharpe Ratio: " << result.variant_b.sharpe_ratio << std::endl;
    std::cout << "  Trades: " << result.variant_b.num_trades << std::endl;

    std::cout << "\nStatistical Analysis:" << std::endl;
    std::cout << "  P-value: " << result.p_value << std::endl;
    std::cout << "  Significant: " << (result.is_significant ? "Yes" : "No") << std::endl;
    std::cout << "  Winner: " << result.winner << std::endl;
    std::cout << "  Effect Size (Cohen's d): " << result.cohens_d << std::endl;
    std::cout << "  Improvement: " << result.relative_difference_pct << "%" << std::endl;
    std::cout << "  95% CI: [" << result.ci_lower << ", "
             << result.ci_upper << "]" << std::endl;

    std::cout << "\nRecommendation:" << std::endl;
    std::cout << result.recommendation << std::endl;

    return 0;
}

================================================================================
END OF DOCUMENT
================================================================================
