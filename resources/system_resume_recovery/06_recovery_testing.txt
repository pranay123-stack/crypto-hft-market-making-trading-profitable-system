================================================================================
RECOVERY TESTING FOR HFT SYSTEMS
================================================================================

Author: HFT System Architecture Team
Last Updated: 2025-11-25
Version: 1.0
Category: System Resume & Recovery

================================================================================
TABLE OF CONTENTS
================================================================================

1. Why Recovery Testing is Critical
2. Monthly Recovery Drills
3. Chaos Engineering for HFT (Simulating Crashes)
4. Test Scenarios (Crash Types, Timing, Data Corruption)
5. Automated Recovery Tests
6. Measuring RTO/RPO
7. Validating Recovered State
8. Regression Testing for Recovery
9. Red Team Exercises
10. Best Practices & Checklists

================================================================================
1. WHY RECOVERY TESTING IS CRITICAL
================================================================================

1.1 THE RECOVERY PARADOX
-------------------------

"Recovery procedures that are not tested regularly WILL fail when needed."

Statistics from HFT industry:
- 73% of untested recovery procedures fail in production
- 89% of system failures happen during highest volatility (worst time)
- 94% of firms that test recovery monthly have <5 min RTO
- Only 31% of HFT firms test recovery procedures regularly

Real-world example:
-------------------
Firm X had perfect recovery documentation
- Never tested in production-like environment
- Real outage during market open
- Documentation was outdated (exchange API changed)
- Recovery took 3 hours instead of 30 minutes
- Lost $2M in missed opportunities
- Regulatory inquiry followed

1.2 REGULATORY REQUIREMENTS
----------------------------

SEC Regulation SCI (System Compliance and Integrity):
- Requires business continuity and disaster recovery (BC-DR) plans
- Must test BC-DR plans at least annually
- Must document test results
- Must remediate identified gaps

MiFID II:
- Requires trading system resilience testing
- Must demonstrate recovery capabilities
- Regular testing mandatory

FINRA:
- Requires business continuity plans
- Must test plans annually
- Must update plans based on test results

1.3 TYPES OF RECOVERY TO TEST
------------------------------

1. Hot failover (primary -> backup)
2. Warm restart from checkpoint
3. Cold restart from scratch
4. Partial component failure
5. Network partition recovery
6. Data corruption recovery
7. Exchange disconnect recovery
8. Database failure recovery
9. Message queue failure recovery
10. Multi-component cascading failure

1.4 COST OF NOT TESTING
------------------------

Direct costs:
- Extended downtime during real incident
- Lost trading opportunities
- Regulatory fines
- Emergency consulting fees

Indirect costs:
- Reputation damage
- Loss of confidence from traders
- Increased scrutiny from management
- Team morale impact
- Difficulty hiring (known reliability issues)

1.5 BENEFITS OF REGULAR TESTING
--------------------------------

1. Validates recovery procedures work
2. Identifies gaps in documentation
3. Trains team on recovery procedures
4. Discovers bugs before production incident
5. Reduces stress during real incidents
6. Improves RTO/RPO metrics
7. Builds confidence in system resilience
8. Satisfies regulatory requirements
9. Provides metrics for continuous improvement

================================================================================
2. MONTHLY RECOVERY DRILLS
================================================================================

2.1 DRILL SCHEDULE
------------------

Monthly Schedule:
-----------------
Week 1: Hot failover drill (production-like)
Week 2: Warm restart drill (staging)
Week 3: Cold restart drill (development)
Week 4: Chaos engineering session

Time of Day:
-----------
- Production drills: After market close (low risk)
- Staging drills: Anytime
- Development drills: Anytime

Participants:
------------
- Primary: On-call engineer
- Secondary: Team lead
- Observer: Another team member (for documentation)
- Optional: Management (quarterly)

2.2 DRILL TYPES
---------------

Type 1: Announced Drill
-----------------------
Team knows drill is happening
- Lower stress
- Good for training
- Can prepare tools/documentation
- Run monthly

Type 2: Surprise Drill
----------------------
Team doesn't know when (but knows it's coming this month)
- More realistic
- Tests actual response
- Identifies gaps in readiness
- Run quarterly

Type 3: Full Red Team Exercise
-------------------------------
External team simulates attack/failure
- Most realistic
- High stress
- Expensive
- Run annually

2.3 MONTHLY DRILL PROCEDURE
----------------------------

Step 1: Pre-Drill Preparation (Week before)
--------------------------------------------
[ ] Schedule drill time
[ ] Notify participants
[ ] Prepare test environment
[ ] Set up monitoring/logging
[ ] Create test scenarios
[ ] Prepare evaluation rubric

Step 2: Drill Execution
-----------------------
[ ] Start timer
[ ] Introduce failure
[ ] Team executes recovery procedures
[ ] Observer documents steps taken
[ ] Observer notes issues encountered
[ ] Stop timer when recovery complete

Step 3: Validation
------------------
[ ] Verify system state correct
[ ] Check positions match exchange
[ ] Verify orders restored
[ ] Check risk limits
[ ] Validate P&L calculations
[ ] Test trading functionality

Step 4: Post-Drill Review (Same day)
-------------------------------------
[ ] Calculate RTO achieved
[ ] Calculate RPO (data loss)
[ ] Review what went well
[ ] Identify what went wrong
[ ] Discuss improvement opportunities
[ ] Document lessons learned

Step 5: Follow-up (Next week)
------------------------------
[ ] Update recovery procedures
[ ] Fix identified bugs
[ ] Improve automation
[ ] Update documentation
[ ] Schedule training if needed

2.4 DRILL AUTOMATION SCRIPT
----------------------------

#!/bin/bash
# monthly_drill.sh

set -e

DRILL_TYPE=$1  # hot, warm, cold
DRILL_ID=$(date +%Y%m%d_%H%M%S)
LOG_DIR="/var/log/recovery_drills/${DRILL_ID}"

mkdir -p $LOG_DIR

echo "=== Recovery Drill Started ===" | tee $LOG_DIR/drill.log
echo "Drill Type: $DRILL_TYPE" | tee -a $LOG_DIR/drill.log
echo "Drill ID: $DRILL_ID" | tee -a $LOG_DIR/drill.log
echo "Start Time: $(date)" | tee -a $LOG_DIR/drill.log

START_TIME=$(date +%s)

# Take pre-drill snapshot
echo "Taking pre-drill snapshot..." | tee -a $LOG_DIR/drill.log
/opt/trading/bin/snapshot_state.sh > $LOG_DIR/pre_drill_state.json

# Introduce failure based on drill type
case $DRILL_TYPE in
    hot)
        echo "Simulating primary system failure..." | tee -a $LOG_DIR/drill.log
        ssh primary "sudo systemctl stop trading-engine"
        ;;
    warm)
        echo "Stopping system for warm restart..." | tee -a $LOG_DIR/drill.log
        ssh primary "sudo systemctl stop trading-engine"
        sleep 5
        ;;
    cold)
        echo "Wiping state for cold restart..." | tee -a $LOG_DIR/drill.log
        ssh primary "sudo systemctl stop trading-engine"
        ssh primary "redis-cli FLUSHDB"
        ;;
esac

# Monitor recovery
echo "Monitoring recovery..." | tee -a $LOG_DIR/drill.log
RECOVERED=false
TIMEOUT=1800  # 30 minutes max

for i in $(seq 1 $TIMEOUT); do
    # Check if system is back up
    STATUS=$(curl -s http://primary:9090/health || echo "DOWN")
    echo "[$i] Status: $STATUS" >> $LOG_DIR/recovery_progress.log

    if [ "$STATUS" == "OK" ]; then
        END_TIME=$(date +%s)
        RTO=$((END_TIME - START_TIME))
        echo "System recovered in ${RTO} seconds" | tee -a $LOG_DIR/drill.log
        RECOVERED=true
        break
    fi

    sleep 1
done

if [ "$RECOVERED" == false ]; then
    echo "[ERROR] Recovery timeout after ${TIMEOUT} seconds!" | tee -a $LOG_DIR/drill.log
    exit 1
fi

# Validate recovered state
echo "Validating recovered state..." | tee -a $LOG_DIR/drill.log
/opt/trading/bin/validate_state.sh > $LOG_DIR/validation_report.txt

if [ $? -eq 0 ]; then
    echo "[PASS] State validation passed" | tee -a $LOG_DIR/drill.log
else
    echo "[FAIL] State validation failed!" | tee -a $LOG_DIR/drill.log
fi

# Take post-drill snapshot
/opt/trading/bin/snapshot_state.sh > $LOG_DIR/post_drill_state.json

# Compare snapshots
echo "Comparing pre/post states..." | tee -a $LOG_DIR/drill.log
diff $LOG_DIR/pre_drill_state.json $LOG_DIR/post_drill_state.json > $LOG_DIR/state_diff.txt || true

# Generate report
echo "Generating drill report..." | tee -a $LOG_DIR/drill.log
/opt/trading/bin/generate_drill_report.py \
    --drill-id $DRILL_ID \
    --drill-type $DRILL_TYPE \
    --rto $RTO \
    --log-dir $LOG_DIR \
    --output $LOG_DIR/drill_report.html

echo "=== Recovery Drill Complete ===" | tee -a $LOG_DIR/drill.log
echo "Report: $LOG_DIR/drill_report.html"

# Send report to team
mail -s "Recovery Drill Report - $DRILL_TYPE - RTO: ${RTO}s" \
    team@trading.com < $LOG_DIR/drill_report.html

2.5 DRILL EVALUATION RUBRIC
----------------------------

Scoring (1-5 scale):

1. Detection Speed (How fast was failure detected?)
   5: <10 seconds
   4: 10-30 seconds
   3: 30-60 seconds
   2: 1-5 minutes
   1: >5 minutes

2. Response Time (How fast did team start recovery?)
   5: <1 minute
   4: 1-2 minutes
   3: 2-5 minutes
   2: 5-10 minutes
   1: >10 minutes

3. Recovery Speed (Total RTO)
   Hot: 5: <5s, 4: <30s, 3: <60s, 2: <5min, 1: >5min
   Warm: 5: <3min, 4: <5min, 3: <10min, 2: <15min, 1: >15min
   Cold: 5: <20min, 4: <30min, 3: <45min, 2: <60min, 1: >60min

4. Procedure Adherence (Did team follow documented procedures?)
   5: Followed exactly
   4: Minor deviations
   3: Some improvisation needed
   2: Major deviations
   1: Procedures didn't work

5. State Validation (Was recovered state correct?)
   5: Perfect match
   4: Minor discrepancies (auto-corrected)
   3: Moderate issues (manual correction needed)
   2: Major issues (required investigation)
   1: Failed validation

6. Communication (Was team coordination effective?)
   5: Excellent coordination
   4: Good coordination
   3: Adequate coordination
   2: Poor coordination
   1: Chaotic

Overall Score: Average of above (target: â‰¥4.0)

2.6 DRILL REPORT TEMPLATE
--------------------------

Recovery Drill Report
=====================

Drill Information:
- Drill ID: 20251125_143000
- Drill Type: Warm Restart
- Date: 2025-11-25
- Participants: Alice (lead), Bob (support), Carol (observer)

Metrics:
- Detection Time: 5 seconds
- Response Time: 30 seconds
- Recovery Time: 4 minutes 23 seconds
- Total RTO: 4:58 (target: <5:00) [PASS]
- Data Loss (RPO): 0 seconds [PASS]

Evaluation Scores:
- Detection Speed: 5/5
- Response Time: 4/5
- Recovery Speed: 4/5
- Procedure Adherence: 3/5
- State Validation: 5/5
- Communication: 4/5
- Overall: 4.2/5 [PASS]

What Went Well:
- Automatic failover detection worked perfectly
- State validation passed on first try
- Team coordination was smooth
- Documentation was mostly accurate

What Went Wrong:
- Step 3 in runbook was outdated (API endpoint changed)
- Had to improvise checkpoint loading (format version mismatch)
- Minor confusion about who should trigger restart

Action Items:
1. [Alice] Update runbook step 3 with new API endpoint (Due: 2025-11-27)
2. [Bob] Add checkpoint version migration logic (Due: 2025-12-02)
3. [Carol] Clarify roles in runbook (Due: 2025-11-26)
4. [All] Review updated procedures in next team meeting

Next Drill: 2025-12-25 (Hot Failover)

================================================================================
3. CHAOS ENGINEERING FOR HFT (SIMULATING CRASHES)
================================================================================

3.1 CHAOS ENGINEERING PRINCIPLES
---------------------------------

Chaos Engineering: Discipline of experimenting on a system to build confidence
in its capability to withstand turbulent conditions in production.

For HFT systems:
- Inject failures during testing
- Observe system behavior
- Identify weaknesses
- Improve resilience

Principles:
-----------
1. Build a hypothesis around steady-state behavior
2. Vary real-world events
3. Run experiments in production (carefully!)
4. Automate experiments
5. Minimize blast radius

3.2 HFT-SPECIFIC CHAOS SCENARIOS
---------------------------------

Scenario 1: Kill Random Process
--------------------------------
Randomly kill trading engine, risk manager, or market data feed

class ChaosKiller {
public:
    void kill_random_process() {
        vector<string> processes = {
            "trading_engine",
            "risk_manager",
            "market_data_feed",
            "order_router"
        };

        int idx = rand() % processes.size();
        string process = processes[idx];

        LOG_WARN("[CHAOS] Killing process: {}", process);
        system(("killall -9 " + process).c_str());
    }
};

Scenario 2: Introduce Network Latency
--------------------------------------
Add 100-500ms latency to exchange connections

#!/bin/bash
# chaos_latency.sh

INTERFACE="eth0"
LATENCY="100ms"

echo "[CHAOS] Adding ${LATENCY} latency to ${INTERFACE}"
tc qdisc add dev $INTERFACE root netem delay $LATENCY

sleep 300  # 5 minutes

echo "[CHAOS] Removing latency"
tc qdisc del dev $INTERFACE root

Scenario 3: Packet Loss
------------------------
Drop 5% of packets randomly

tc qdisc add dev eth0 root netem loss 5%

Scenario 4: Disk Full
----------------------
Fill up disk to test low-space handling

dd if=/dev/zero of=/var/tmp/fill bs=1M count=10000

Scenario 5: CPU Stress
----------------------
Consume CPU to test performance degradation

stress-ng --cpu 8 --timeout 300s

Scenario 6: Memory Pressure
----------------------------
Consume memory to test OOM handling

stress-ng --vm 4 --vm-bytes 4G --timeout 300s

Scenario 7: Clock Skew
-----------------------
Mess with system clock to test timestamp handling

#!/bin/bash
# Skew clock by 10 seconds
date -s "$(date -d '+10 seconds')"

Scenario 8: Corrupt Data File
------------------------------
Flip random bits in checkpoint file

#!/bin/bash
FILE="/var/lib/trading/checkpoint.dat"

# Corrupt 10 random bytes
for i in $(seq 1 10); do
    POS=$((RANDOM % $(stat -c%s $FILE)))
    printf '\xff' | dd of=$FILE bs=1 seek=$POS count=1 conv=notrunc
done

3.3 CHAOS TESTING FRAMEWORK
----------------------------

class ChaosEngine {
private:
    enum ChaosType {
        PROCESS_KILL,
        NETWORK_LATENCY,
        PACKET_LOSS,
        DISK_FULL,
        CPU_STRESS,
        MEMORY_PRESSURE,
        CLOCK_SKEW,
        DATA_CORRUPTION
    };

    bool chaos_mode_enabled_ = false;
    std::chrono::minutes chaos_interval_{10};

public:
    void start_chaos_testing() {
        if (!is_test_environment()) {
            LOG_ERROR("Chaos testing only allowed in test environments!");
            return;
        }

        chaos_mode_enabled_ = true;
        std::thread chaos_thread([this]() {
            run_chaos_loop();
        });
        chaos_thread.detach();
    }

private:
    void run_chaos_loop() {
        while (chaos_mode_enabled_) {
            std::this_thread::sleep_for(chaos_interval_);

            // Randomly select chaos type
            ChaosType type = static_cast<ChaosType>(rand() % 8);

            LOG_WARN("[CHAOS] Injecting failure: {}", chaos_type_name(type));

            try {
                inject_chaos(type);

                // Observe system behavior
                observe_system_behavior();
            }
            catch (const std::exception& ex) {
                LOG_ERROR("[CHAOS] Chaos injection failed: {}", ex.what());
            }
        }
    }

    void inject_chaos(ChaosType type) {
        switch (type) {
        case PROCESS_KILL:
            kill_random_process();
            break;
        case NETWORK_LATENCY:
            inject_network_latency();
            break;
        case PACKET_LOSS:
            inject_packet_loss();
            break;
        case DISK_FULL:
            fill_disk();
            break;
        case CPU_STRESS:
            stress_cpu();
            break;
        case MEMORY_PRESSURE:
            consume_memory();
            break;
        case CLOCK_SKEW:
            skew_clock();
            break;
        case DATA_CORRUPTION:
            corrupt_data_file();
            break;
        }
    }

    void observe_system_behavior() {
        // Measure key metrics after chaos injection
        auto metrics = collect_metrics();

        LOG_INFO("[CHAOS] Post-injection metrics:");
        LOG_INFO("  - System uptime: {}", metrics.uptime);
        LOG_INFO("  - Trading active: {}", metrics.trading_active);
        LOG_INFO("  - Order processing lag: {} ms", metrics.order_lag);
        LOG_INFO("  - Position accuracy: {}", metrics.position_accuracy);

        // Check if system recovered
        if (!metrics.trading_active) {
            LOG_ERROR("[CHAOS] System failed to recover!");
            alert_ops_team("Chaos test revealed system failure");
        }
    }
};

3.4 CHAOS TESTING SCHEDULE
---------------------------

Environment-specific schedules:

Development:
- Chaos always on (helps find bugs early)
- Low intensity (don't impede development)

Staging:
- Daily chaos sessions (1 hour)
- Medium intensity
- Automated

Production:
- Opt-in only (never without approval)
- Very careful (minimal blast radius)
- During low-volume periods only
- Manual supervision

3.5 CHAOS TESTING BEST PRACTICES
---------------------------------

1. Start small
   - Begin with simple failures (single process kill)
   - Gradually increase complexity
   - Don't try everything at once

2. Have a kill switch
   - Ability to instantly stop chaos testing
   - Restore system to normal state quickly

3. Monitor everything
   - Log all chaos injections
   - Track system metrics during/after
   - Alert if system doesn't recover

4. Limit blast radius
   - Test on subset of symbols
   - Test on single server (not whole cluster)
   - Have manual override ready

5. Learn from results
   - Document what broke
   - Fix the issues
   - Retest to verify fixes

6. Never in production during market hours!
   - Only during testing
   - Or off-hours in production
   - With explicit approval

================================================================================
4. TEST SCENARIOS (CRASH TYPES, TIMING, DATA CORRUPTION)
================================================================================

4.1 CRASH TYPE SCENARIOS
-------------------------

Scenario 1: Clean Shutdown
---------------------------
System receives SIGTERM, gracefully shuts down
- Expected: No data loss, clean restart
- Test: kill -TERM <pid>

Scenario 2: Abrupt Kill
-----------------------
System receives SIGKILL, no cleanup possible
- Expected: Some data loss, recovery from checkpoint
- Test: kill -KILL <pid>

Scenario 3: Segmentation Fault
-------------------------------
System crashes due to memory corruption
- Expected: Core dump, restart from checkpoint
- Test: Inject null pointer dereference

Scenario 4: Out of Memory
--------------------------
System killed by OOM killer
- Expected: Automatic restart, memory leak investigation
- Test: Consume all available memory

Scenario 5: Disk Full
----------------------
System cannot write logs/checkpoints
- Expected: Alert, graceful degradation
- Test: Fill disk to 100%

Scenario 6: Network Partition
------------------------------
System loses connection to exchange
- Expected: Reconnect, reconcile state
- Test: Block network traffic with iptables

Scenario 7: Database Failure
-----------------------------
Database becomes unavailable
- Expected: Queue writes, retry, eventual consistency
- Test: Stop PostgreSQL

Scenario 8: Cascading Failure
------------------------------
One component failure triggers others
- Expected: Isolation, partial degradation
- Test: Kill critical shared component

4.2 TIMING SCENARIOS
--------------------

Scenario A: Crash at Market Open
---------------------------------
Worst possible time, highest volatility

Test procedure:
1. Start system before market open
2. Kill system at 9:30:00 AM exactly
3. Measure recovery time
4. Verify no positions stuck

Expected RTO: <30 seconds (hot) or <5 minutes (warm)

Scenario B: Crash During Large Order
-------------------------------------
Order partially filled when crash occurs

Test procedure:
1. Submit large order (1000 shares)
2. Wait for partial fill (500 shares)
3. Kill system immediately
4. Verify recovery captures partial fill
5. Verify position updated correctly

Expected: No missing fills, correct position

Scenario C: Crash During Position Reconciliation
-------------------------------------------------
System crashes mid-reconciliation

Test procedure:
1. Introduce position discrepancy
2. Start reconciliation
3. Kill system mid-reconciliation
4. Restart and verify reconciliation completes

Expected: Idempotent reconciliation

Scenario D: Crash During Checkpoint
------------------------------------
System crashes while writing checkpoint

Test procedure:
1. Trigger checkpoint
2. Kill system mid-write (corrupted checkpoint)
3. Restart
4. Verify system uses previous valid checkpoint

Expected: Fallback to previous checkpoint

Scenario E: Multiple Rapid Crashes
-----------------------------------
System crashes repeatedly (crash loop)

Test procedure:
1. Introduce bug that causes crash on startup
2. Let system crash-loop 10 times
3. Measure behavior (does it give up? alert?)

Expected: Backoff strategy, alert after N retries

Scenario F: Crash at Day End
-----------------------------
During end-of-day settlement

Test procedure:
1. Wait for market close
2. Kill system during EOD processing
3. Verify positions settled correctly

Expected: Proper handling of unsettled positions

4.3 DATA CORRUPTION SCENARIOS
------------------------------

Scenario 1: Corrupted Checkpoint File
--------------------------------------
Checkpoint file has bit flips

Test:
-----
#!/bin/bash
# Corrupt checkpoint
CHECKPOINT="/var/lib/trading/checkpoint.dat"

# Flip 100 random bits
for i in $(seq 1 100); do
    POS=$((RANDOM % $(stat -c%s $CHECKPOINT)))
    BYTE=$(od -An -t u1 -N1 -j$POS $CHECKPOINT | tr -d ' ')
    FLIPPED=$((BYTE ^ (1 << (RANDOM % 8))))
    printf "\\x$(printf '%02x' $FLIPPED)" | \
        dd of=$CHECKPOINT bs=1 seek=$POS count=1 conv=notrunc 2>/dev/null
done

Expected behavior:
- Checksum validation fails
- System falls back to previous checkpoint
- Alert generated

Scenario 2: Corrupted Position File
------------------------------------
In-memory position data corrupted (cosmic ray?)

Test with memory bit flip injection:
-------------------------------------
class MemoryCorruptor {
public:
    void corrupt_random_position() {
        auto positions = position_manager_->get_all_positions_raw();

        // Pick random position
        int idx = rand() % positions.size();

        // Flip random bit in quantity field
        int64_t* qty_ptr = &positions[idx].quantity;
        int bit = rand() % 64;
        *qty_ptr ^= (1ULL << bit);

        LOG_ERROR("[CORRUPTION] Flipped bit {} in position {}",
                 bit, positions[idx].symbol);
    }
};

Expected behavior:
- Checksum validation detects corruption
- Position reconciliation catches discrepancy
- System corrects from exchange API

Scenario 3: Corrupted Order Book
---------------------------------
Order book data structure corrupted

Test:
-----
Overwrite random memory in order book data structure

Expected behavior:
- Order book validation fails
- Rebuild order book from exchange
- Alert generated

Scenario 4: Corrupted Log File
-------------------------------
Incremental log file corrupted

Test:
-----
Truncate log file midway through event

Expected behavior:
- Log replay stops at corruption point
- Fall back to checkpoint
- Alert missing events

Scenario 5: Timestamp Corruption
---------------------------------
Timestamps become nonsensical (year 2099, negative, etc.)

Test:
-----
void corrupt_timestamp(Fill& fill) {
    fill.timestamp = timestamp_t(99999999999999);  // Far future
}

Expected behavior:
- Timestamp validation rejects event
- Event logged for manual review
- System continues operation

4.4 COMPREHENSIVE TEST SUITE
-----------------------------

class RecoveryTestSuite {
public:
    void run_all_tests() {
        LOG_INFO("=== Starting Comprehensive Recovery Test Suite ===");

        // Crash type tests
        test_clean_shutdown();
        test_abrupt_kill();
        test_segfault();
        test_oom();
        test_disk_full();
        test_network_partition();
        test_database_failure();
        test_cascading_failure();

        // Timing tests
        test_crash_at_market_open();
        test_crash_during_order();
        test_crash_during_reconciliation();
        test_crash_during_checkpoint();
        test_rapid_crashes();
        test_crash_at_day_end();

        // Corruption tests
        test_corrupted_checkpoint();
        test_corrupted_position();
        test_corrupted_order_book();
        test_corrupted_log();
        test_timestamp_corruption();

        // Generate report
        generate_test_report();
    }

private:
    void test_clean_shutdown() {
        LOG_INFO("Test: Clean Shutdown");

        setup_test_environment();

        // Gracefully shutdown
        system("kill -TERM $(pidof trading_engine)");

        // Wait for shutdown
        wait_for_process_exit("trading_engine", 30);

        // Restart
        auto start = now();
        system("/opt/trading/bin/trading_engine &");

        // Wait for recovery
        wait_for_system_ready();
        auto end = now();

        // Validate
        assert(validate_positions());
        assert(validate_orders());

        auto rto = duration_cast<seconds>(end - start).count();
        LOG_INFO("Clean shutdown RTO: {} seconds [{}]",
                rto, rto < 60 ? "PASS" : "FAIL");
    }

    // ... implement other test methods similarly ...
};

================================================================================
5. AUTOMATED RECOVERY TESTS
================================================================================

5.1 CONTINUOUS TESTING PIPELINE
--------------------------------

                  [Developer Commits Code]
                           |
                           v
                  [Build & Unit Tests]
                           |
                           v
                  [Deploy to Test Environment]
                           |
                           v
            [Automated Recovery Tests] <-- Run on every deploy
                           |
                  +--------+--------+
                  |                 |
              [PASS]            [FAIL]
                  |                 |
                  v                 v
          [Deploy to Staging]  [Block Deploy]
                                    |
                                    v
                              [Alert Team]

5.2 TEST AUTOMATION FRAMEWORK
------------------------------

class AutomatedRecoveryTesting {
private:
    struct TestResult {
        string test_name;
        bool passed;
        int rto_seconds;
        int rpo_seconds;
        string error_message;
        vector<string> warnings;
    };

    vector<TestResult> results_;

public:
    void run_automated_tests() {
        LOG_INFO("=== Starting Automated Recovery Tests ===");

        // Run each test
        results_.push_back(run_test("hot_failover", &test_hot_failover));
        results_.push_back(run_test("warm_restart", &test_warm_restart));
        results_.push_back(run_test("cold_restart", &test_cold_restart));
        results_.push_back(run_test("network_partition", &test_network_partition));
        results_.push_back(run_test("database_failure", &test_database_failure));

        // Generate report
        generate_report();

        // Send alerts if any tests failed
        if (any_failures()) {
            alert_team("Recovery tests failed!");
        }
    }

private:
    TestResult run_test(const string& name,
                       std::function<void()> test_func) {
        LOG_INFO("Running test: {}", name);

        TestResult result;
        result.test_name = name;

        try {
            auto start = now();

            // Run the test
            test_func();

            auto end = now();
            result.rto_seconds = duration_cast<seconds>(end - start).count();

            // Validate recovered state
            result.passed = validate_state();

            if (result.passed) {
                LOG_INFO("Test {} PASSED (RTO: {}s)",
                        name, result.rto_seconds);
            } else {
                LOG_ERROR("Test {} FAILED", name);
                result.error_message = "State validation failed";
            }
        }
        catch (const std::exception& ex) {
            result.passed = false;
            result.error_message = ex.what();
            LOG_ERROR("Test {} FAILED: {}", name, ex.what());
        }

        return result;
    }

    void generate_report() {
        LOG_INFO("=== Test Results ===");

        int passed = 0;
        int failed = 0;

        for (const auto& result : results_) {
            if (result.passed) {
                passed++;
                LOG_INFO("[PASS] {} (RTO: {}s)",
                        result.test_name, result.rto_seconds);
            } else {
                failed++;
                LOG_ERROR("[FAIL] {}: {}",
                         result.test_name, result.error_message);
            }
        }

        LOG_INFO("Total: {} passed, {} failed", passed, failed);

        // Write report to file
        write_report_to_file();

        // Send to metrics system
        send_metrics_to_monitoring();
    }
};

5.3 JENKINS/CI PIPELINE
------------------------

Jenkinsfile:
------------

pipeline {
    agent any

    stages {
        stage('Build') {
            steps {
                sh 'make clean'
                sh 'make all'
            }
        }

        stage('Unit Tests') {
            steps {
                sh 'make test'
            }
        }

        stage('Deploy to Test') {
            steps {
                sh './deploy_to_test.sh'
            }
        }

        stage('Recovery Tests') {
            steps {
                sh './run_recovery_tests.sh'
            }
            post {
                always {
                    junit 'test-results/*.xml'
                    publishHTML([
                        reportDir: 'test-results',
                        reportFiles: 'recovery_report.html',
                        reportName: 'Recovery Test Report'
                    ])
                }
            }
        }

        stage('Performance Tests') {
            steps {
                sh './run_performance_tests.sh'
            }
        }
    }

    post {
        failure {
            mail to: 'team@trading.com',
                 subject: "Build Failed: ${env.JOB_NAME} - ${env.BUILD_NUMBER}",
                 body: "Recovery tests failed. Check ${env.BUILD_URL}"
        }
    }
}

5.4 AUTOMATED TEST SCRIPT
--------------------------

#!/bin/bash
# run_recovery_tests.sh

set -e

TEST_ENV="test-env"
RESULTS_DIR="test-results"

mkdir -p $RESULTS_DIR

echo "=== Automated Recovery Tests ==="
echo "Test Environment: $TEST_ENV"
echo "Results Directory: $RESULTS_DIR"

# Test 1: Hot Failover
echo "[1/5] Testing hot failover..."
ssh $TEST_ENV "sudo /opt/trading/test/test_hot_failover.sh" \
    > $RESULTS_DIR/hot_failover.log 2>&1

if [ $? -eq 0 ]; then
    echo "  [PASS] Hot failover"
else
    echo "  [FAIL] Hot failover"
fi

# Test 2: Warm Restart
echo "[2/5] Testing warm restart..."
ssh $TEST_ENV "sudo /opt/trading/test/test_warm_restart.sh" \
    > $RESULTS_DIR/warm_restart.log 2>&1

if [ $? -eq 0 ]; then
    echo "  [PASS] Warm restart"
else
    echo "  [FAIL] Warm restart"
fi

# Test 3: Cold Restart
echo "[3/5] Testing cold restart..."
ssh $TEST_ENV "sudo /opt/trading/test/test_cold_restart.sh" \
    > $RESULTS_DIR/cold_restart.log 2>&1

if [ $? -eq 0 ]; then
    echo "  [PASS] Cold restart"
else
    echo "  [FAIL] Cold restart"
fi

# Test 4: Network Partition
echo "[4/5] Testing network partition..."
ssh $TEST_ENV "sudo /opt/trading/test/test_network_partition.sh" \
    > $RESULTS_DIR/network_partition.log 2>&1

if [ $? -eq 0 ]; then
    echo "  [PASS] Network partition"
else
    echo "  [FAIL] Network partition"
fi

# Test 5: Database Failure
echo "[5/5] Testing database failure..."
ssh $TEST_ENV "sudo /opt/trading/test/test_database_failure.sh" \
    > $RESULTS_DIR/database_failure.log 2>&1

if [ $? -eq 0 ]; then
    echo "  [PASS] Database failure"
else
    echo "  [FAIL] Database failure"
fi

# Generate HTML report
python3 /opt/trading/test/generate_report.py \
    --results-dir $RESULTS_DIR \
    --output $RESULTS_DIR/recovery_report.html

echo "=== Tests Complete ==="
echo "Report: $RESULTS_DIR/recovery_report.html"

================================================================================
6. MEASURING RTO/RPO
================================================================================

6.1 RTO MEASUREMENT
--------------------

RTO (Recovery Time Objective) = Time from failure to full recovery

Components of RTO:
------------------
1. Detection Time: Time to detect failure
2. Response Time: Time to start recovery
3. Recovery Time: Time to complete recovery
4. Validation Time: Time to verify state
5. Resume Time: Time to start trading

Total RTO = Detection + Response + Recovery + Validation + Resume

6.2 RTO MEASUREMENT IMPLEMENTATION
-----------------------------------

class RTOMeasurement {
private:
    struct RTOBreakdown {
        timestamp_t failure_time;
        timestamp_t detection_time;
        timestamp_t response_start_time;
        timestamp_t recovery_complete_time;
        timestamp_t validation_complete_time;
        timestamp_t trading_resumed_time;

        int detection_ms() const {
            return duration_cast<milliseconds>(
                detection_time - failure_time
            ).count();
        }

        int response_ms() const {
            return duration_cast<milliseconds>(
                response_start_time - detection_time
            ).count();
        }

        int recovery_ms() const {
            return duration_cast<milliseconds>(
                recovery_complete_time - response_start_time
            ).count();
        }

        int validation_ms() const {
            return duration_cast<milliseconds>(
                validation_complete_time - recovery_complete_time
            ).count();
        }

        int resume_ms() const {
            return duration_cast<milliseconds>(
                trading_resumed_time - validation_complete_time
            ).count();
        }

        int total_rto_ms() const {
            return duration_cast<milliseconds>(
                trading_resumed_time - failure_time
            ).count();
        }
    };

public:
    RTOBreakdown measure_rto() {
        RTOBreakdown rto;

        // Inject failure
        rto.failure_time = now_utc();
        inject_failure();

        // Wait for detection
        wait_for_detection();
        rto.detection_time = now_utc();

        // Wait for response to start
        wait_for_recovery_start();
        rto.response_start_time = now_utc();

        // Wait for recovery to complete
        wait_for_recovery_complete();
        rto.recovery_complete_time = now_utc();

        // Wait for validation
        wait_for_validation_complete();
        rto.validation_complete_time = now_utc();

        // Wait for trading to resume
        wait_for_trading_resumed();
        rto.trading_resumed_time = now_utc();

        // Log breakdown
        LOG_INFO("RTO Breakdown:");
        LOG_INFO("  Detection: {} ms", rto.detection_ms());
        LOG_INFO("  Response: {} ms", rto.response_ms());
        LOG_INFO("  Recovery: {} ms", rto.recovery_ms());
        LOG_INFO("  Validation: {} ms", rto.validation_ms());
        LOG_INFO("  Resume: {} ms", rto.resume_ms());
        LOG_INFO("  Total RTO: {} ms", rto.total_rto_ms());

        return rto;
    }
};

6.3 RPO MEASUREMENT
--------------------

RPO (Recovery Point Objective) = Maximum acceptable data loss

For HFT: RPO should be ZERO (no data loss acceptable)

Measuring RPO:
--------------
1. Record last processed event before failure
2. After recovery, query for first missing event
3. RPO = time between last processed and first missing

class RPOMeasurement {
public:
    int measure_rpo() {
        // Get last event processed before failure
        uint64_t last_processed_seq = get_last_processed_sequence();
        timestamp_t last_processed_time = get_last_processed_timestamp();

        // Inject failure
        inject_failure();

        // Recover
        perform_recovery();

        // Get first event processed after recovery
        uint64_t first_recovered_seq = get_first_recovered_sequence();
        timestamp_t first_recovered_time = get_first_recovered_timestamp();

        // Check for gaps
        int missed_events = first_recovered_seq - last_processed_seq - 1;

        if (missed_events == 0) {
            LOG_INFO("RPO: 0 (no data loss)");
            return 0;
        } else {
            auto rpo = duration_cast<milliseconds>(
                first_recovered_time - last_processed_time
            ).count();

            LOG_WARN("RPO: {} ms ({} missed events)",
                    rpo, missed_events);

            return rpo;
        }
    }
};

6.4 RTO/RPO TRACKING OVER TIME
-------------------------------

Track RTO/RPO for each test to see trends:

class RTORPOTracker {
private:
    struct TestMetrics {
        timestamp_t test_time;
        string test_type;
        int rto_ms;
        int rpo_ms;
        bool passed;
    };

    vector<TestMetrics> history_;

public:
    void record_test(const string& type, int rto_ms, int rpo_ms, bool passed) {
        TestMetrics metrics{
            .test_time = now_utc(),
            .test_type = type,
            .rto_ms = rto_ms,
            .rpo_ms = rpo_ms,
            .passed = passed
        };

        history_.push_back(metrics);

        // Persist to database
        db_->insert("test_metrics", metrics);

        // Send to monitoring
        monitoring_->gauge("recovery.rto_ms", rto_ms, {{"type", type}});
        monitoring_->gauge("recovery.rpo_ms", rpo_ms, {{"type", type}});
    }

    void analyze_trends() {
        // Calculate average RTO over last 30 days
        auto since = now_utc() - std::chrono::days(30);

        int total_rto = 0;
        int count = 0;

        for (const auto& metric : history_) {
            if (metric.test_time >= since) {
                total_rto += metric.rto_ms;
                count++;
            }
        }

        if (count > 0) {
            int avg_rto = total_rto / count;
            LOG_INFO("Average RTO (last 30 days): {} ms", avg_rto);

            // Alert if trending worse
            if (avg_rto > get_target_rto_ms() * 1.5) {
                LOG_WARN("RTO trending worse than target!");
                alert_team("RTO degradation detected");
            }
        }
    }
};

6.5 RTO/RPO REPORTING DASHBOARD
--------------------------------

SQL queries for dashboard:

-- Average RTO by test type (last 30 days)
SELECT
    test_type,
    AVG(rto_ms) as avg_rto_ms,
    MIN(rto_ms) as min_rto_ms,
    MAX(rto_ms) as max_rto_ms,
    STDDEV(rto_ms) as stddev_rto_ms
FROM test_metrics
WHERE test_time > NOW() - INTERVAL '30 days'
GROUP BY test_type
ORDER BY avg_rto_ms DESC;

-- RTO trend over time
SELECT
    DATE_TRUNC('day', test_time) as test_date,
    test_type,
    AVG(rto_ms) as avg_rto_ms
FROM test_metrics
WHERE test_time > NOW() - INTERVAL '90 days'
GROUP BY test_date, test_type
ORDER BY test_date, test_type;

-- Success rate by test type
SELECT
    test_type,
    COUNT(*) as total_tests,
    SUM(CASE WHEN passed THEN 1 ELSE 0 END) as passed_tests,
    ROUND(100.0 * SUM(CASE WHEN passed THEN 1 ELSE 0 END) / COUNT(*), 2) as success_rate
FROM test_metrics
WHERE test_time > NOW() - INTERVAL '30 days'
GROUP BY test_type
ORDER BY success_rate ASC;

Grafana dashboard config:
--------------------------

{
  "dashboard": {
    "title": "Recovery Testing Metrics",
    "panels": [
      {
        "title": "RTO Trend",
        "type": "graph",
        "targets": [
          {
            "expr": "avg_over_time(recovery_rto_ms[1h])"
          }
        ]
      },
      {
        "title": "RPO",
        "type": "gauge",
        "targets": [
          {
            "expr": "recovery_rpo_ms"
          }
        ]
      },
      {
        "title": "Success Rate",
        "type": "stat",
        "targets": [
          {
            "expr": "rate(recovery_tests_passed[24h]) / rate(recovery_tests_total[24h])"
          }
        ]
      }
    ]
  }
}

================================================================================
7. VALIDATING RECOVERED STATE
================================================================================

7.1 STATE VALIDATION CHECKLIST
-------------------------------

After recovery, validate:

[ ] Positions match exchange
[ ] Open orders restored correctly
[ ] Risk limits accurate
[ ] P&L calculations correct
[ ] Market data feed active
[ ] Exchange connections established
[ ] Strategy state restored
[ ] Configuration loaded
[ ] Monitoring active
[ ] Logging functional
[ ] Database accessible
[ ] Message queues working
[ ] Timers/schedulers running
[ ] No memory leaks
[ ] No stuck threads

7.2 COMPREHENSIVE STATE VALIDATOR
----------------------------------

class StateValidator {
public:
    struct ValidationReport {
        bool overall_pass;
        map<string, bool> component_results;
        vector<string> errors;
        vector<string> warnings;
    };

    ValidationReport validate_all() {
        ValidationReport report;
        report.overall_pass = true;

        // Validate each component
        report.component_results["positions"] = validate_positions();
        report.component_results["orders"] = validate_orders();
        report.component_results["risk"] = validate_risk();
        report.component_results["pnl"] = validate_pnl();
        report.component_results["connectivity"] = validate_connectivity();
        report.component_results["market_data"] = validate_market_data();
        report.component_results["strategy"] = validate_strategy();
        report.component_results["infrastructure"] = validate_infrastructure();

        // Check if any component failed
        for (const auto& [component, passed] : report.component_results) {
            if (!passed) {
                report.overall_pass = false;
                report.errors.push_back(
                    format("Component validation failed: {}", component)
                );
            }
        }

        return report;
    }

private:
    bool validate_positions() {
        LOG_INFO("Validating positions...");

        auto internal_positions = position_manager_->get_all_positions();
        bool all_valid = true;

        for (const auto& [symbol, internal_pos] : internal_positions) {
            auto exchange_pos = exchange_->get_position(symbol);

            if (internal_pos.quantity != exchange_pos.quantity) {
                LOG_ERROR("Position mismatch for {}: internal={}, exchange={}",
                         symbol, internal_pos.quantity, exchange_pos.quantity);
                all_valid = false;
            }

            // Validate position is within risk limits
            if (std::abs(internal_pos.quantity) > get_position_limit(symbol)) {
                LOG_ERROR("Position {} exceeds limit", symbol);
                all_valid = false;
            }
        }

        return all_valid;
    }

    bool validate_orders() {
        LOG_INFO("Validating orders...");

        auto internal_orders = order_manager_->get_open_orders();
        bool all_valid = true;

        for (const auto& order : internal_orders) {
            auto exchange_status = exchange_->get_order_status(order.order_id);

            // Order should exist on exchange
            if (!exchange_status.exists) {
                LOG_ERROR("Order {} exists internally but not on exchange",
                         order.order_id);
                all_valid = false;
            }

            // States should match
            if (order.state != exchange_status.state) {
                LOG_ERROR("Order {} state mismatch: internal={}, exchange={}",
                         order.order_id,
                         state_to_string(order.state),
                         state_to_string(exchange_status.state));
                all_valid = false;
            }
        }

        return all_valid;
    }

    bool validate_risk() {
        LOG_INFO("Validating risk limits...");

        // Check position limits
        if (!risk_manager_->check_position_limits()) {
            LOG_ERROR("Position limits breached!");
            return false;
        }

        // Check capital usage
        if (!risk_manager_->check_capital_limits()) {
            LOG_ERROR("Capital limits breached!");
            return false;
        }

        // Check concentration
        if (!risk_manager_->check_concentration_limits()) {
            LOG_ERROR("Concentration limits breached!");
            return false;
        }

        return true;
    }

    bool validate_pnl() {
        LOG_INFO("Validating P&L...");

        // Recalculate P&L from positions
        double calculated_pnl = pnl_calculator_->calculate_from_positions();

        // Compare with stored P&L
        double stored_pnl = pnl_manager_->get_total_pnl();

        double diff = std::abs(calculated_pnl - stored_pnl);

        if (diff > 100.0) {  // $100 tolerance
            LOG_ERROR("P&L mismatch: calculated={}, stored={}, diff={}",
                     calculated_pnl, stored_pnl, diff);
            return false;
        }

        return true;
    }

    bool validate_connectivity() {
        LOG_INFO("Validating connectivity...");

        // Check exchange connections
        for (const auto& exchange : exchanges_) {
            if (!exchange->is_connected()) {
                LOG_ERROR("Exchange {} not connected", exchange->get_name());
                return false;
            }
        }

        // Check database
        if (!database_->is_connected()) {
            LOG_ERROR("Database not connected");
            return false;
        }

        // Check message queues
        if (!message_queue_->is_connected()) {
            LOG_ERROR("Message queue not connected");
            return false;
        }

        return true;
    }

    bool validate_market_data() {
        LOG_INFO("Validating market data...");

        // Check feed is current (last update < 1 second ago)
        for (const auto& symbol : get_subscribed_symbols()) {
            auto last_update = market_data_->get_last_update_time(symbol);
            auto age = now_utc() - last_update;

            if (age > std::chrono::seconds(1)) {
                LOG_WARN("Stale market data for {}: {} seconds old",
                        symbol, duration_cast<seconds>(age).count());
                return false;
            }
        }

        return true;
    }

    bool validate_strategy() {
        LOG_INFO("Validating strategy...");

        // Check strategy is initialized
        if (!strategy_->is_initialized()) {
            LOG_ERROR("Strategy not initialized");
            return false;
        }

        // Check indicators have valid values
        if (!strategy_->validate_indicators()) {
            LOG_ERROR("Strategy indicators invalid");
            return false;
        }

        return true;
    }

    bool validate_infrastructure() {
        LOG_INFO("Validating infrastructure...");

        // Check system resources
        auto cpu_usage = get_cpu_usage();
        auto memory_usage = get_memory_usage();
        auto disk_usage = get_disk_usage();

        if (cpu_usage > 90.0) {
            LOG_WARN("High CPU usage: {}%", cpu_usage);
        }

        if (memory_usage > 90.0) {
            LOG_WARN("High memory usage: {}%", memory_usage);
        }

        if (disk_usage > 90.0) {
            LOG_ERROR("High disk usage: {}%", disk_usage);
            return false;
        }

        // Check no stuck threads
        if (has_stuck_threads()) {
            LOG_ERROR("Stuck threads detected");
            return false;
        }

        return true;
    }
};

7.3 AUTOMATED VALIDATION SCRIPT
--------------------------------

#!/bin/bash
# validate_state.sh

echo "=== State Validation ==="

ERRORS=0

# Validate positions
echo "[1/8] Validating positions..."
/opt/trading/bin/validate_positions
if [ $? -ne 0 ]; then
    echo "  [FAIL] Position validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] Positions valid"
fi

# Validate orders
echo "[2/8] Validating orders..."
/opt/trading/bin/validate_orders
if [ $? -ne 0 ]; then
    echo "  [FAIL] Order validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] Orders valid"
fi

# Validate risk
echo "[3/8] Validating risk..."
/opt/trading/bin/validate_risk
if [ $? -ne 0 ]; then
    echo "  [FAIL] Risk validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] Risk valid"
fi

# Validate P&L
echo "[4/8] Validating P&L..."
/opt/trading/bin/validate_pnl
if [ $? -ne 0 ]; then
    echo "  [FAIL] P&L validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] P&L valid"
fi

# Validate connectivity
echo "[5/8] Validating connectivity..."
/opt/trading/bin/validate_connectivity
if [ $? -ne 0 ]; then
    echo "  [FAIL] Connectivity validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] Connectivity valid"
fi

# Validate market data
echo "[6/8] Validating market data..."
/opt/trading/bin/validate_market_data
if [ $? -ne 0 ]; then
    echo "  [FAIL] Market data validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] Market data valid"
fi

# Validate strategy
echo "[7/8] Validating strategy..."
/opt/trading/bin/validate_strategy
if [ $? -ne 0 ]; then
    echo "  [FAIL] Strategy validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] Strategy valid"
fi

# Validate infrastructure
echo "[8/8] Validating infrastructure..."
/opt/trading/bin/validate_infrastructure
if [ $? -ne 0 ]; then
    echo "  [FAIL] Infrastructure validation failed"
    ERRORS=$((ERRORS + 1))
else
    echo "  [PASS] Infrastructure valid"
fi

echo "=== Validation Complete ==="
echo "Errors: $ERRORS"

if [ $ERRORS -eq 0 ]; then
    echo "[PASS] All validations passed"
    exit 0
else
    echo "[FAIL] $ERRORS validations failed"
    exit 1
fi

================================================================================
8. REGRESSION TESTING FOR RECOVERY
================================================================================

8.1 RECOVERY REGRESSION TEST SUITE
-----------------------------------

Maintain a regression test suite of all past recovery issues:

class RecoveryRegressionTests {
public:
    void run_all_regression_tests() {
        // Test for Issue #123: Missed fills during recovery
        test_issue_123_missed_fills();

        // Test for Issue #456: Position corruption after crash
        test_issue_456_position_corruption();

        // Test for Issue #789: Deadlock during restart
        test_issue_789_deadlock();

        // ... add test for every past issue
    }

private:
    void test_issue_123_missed_fills() {
        LOG_INFO("Regression test: Issue #123 - Missed fills during recovery");

        // Setup: Create scenario that caused original bug
        create_partial_fill_scenario();

        // Inject crash
        inject_crash();

        // Recover
        perform_recovery();

        // Validate: Ensure fill was not missed this time
        assert(all_fills_recovered());

        LOG_INFO("[PASS] Issue #123 regression test");
    }

    // ... implement other regression tests
};

8.2 ISSUE TRACKING FOR RECOVERY BUGS
-------------------------------------

Track all recovery-related bugs:

CREATE TABLE recovery_issues (
    issue_id INT PRIMARY KEY,
    title VARCHAR(255),
    description TEXT,
    root_cause TEXT,
    fix_description TEXT,
    regression_test_added BOOLEAN,
    discovered_date DATE,
    fixed_date DATE,
    severity VARCHAR(20)
);

INSERT INTO recovery_issues VALUES
(123, 'Missed fills during recovery',
     'System sometimes misses fills that happen during restart',
     'Fill recovery logic had race condition',
     'Added sequence number tracking',
     true, '2025-01-15', '2025-01-16', 'HIGH'),

(456, 'Position corruption after crash',
     'Positions sometimes wrong after crash',
     'Checkpoint had no checksum validation',
     'Added SHA256 checksum to checkpoint',
     true, '2025-02-10', '2025-02-12', 'CRITICAL'),

(789, 'Deadlock during restart',
     'System hangs on restart',
     'Resource acquisition order bug',
     'Fixed lock ordering',
     true, '2025-03-05', '2025-03-05', 'MEDIUM');

8.3 PREVENTING REGRESSIONS
---------------------------

Process to prevent regressions:

1. Bug discovered in recovery
   â†“
2. Create minimal reproduction test
   â†“
3. Verify test fails (reproduces bug)
   â†“
4. Fix the bug
   â†“
5. Verify test now passes
   â†“
6. Add test to regression suite
   â†“
7. Run regression suite on every build
   â†“
8. Update documentation

================================================================================
9. RED TEAM EXERCISES
================================================================================

9.1 RED TEAM CONCEPT
--------------------

Red Team: External team (or separate internal team) that attempts to break
the system and recovery procedures.

Goals:
- Find weaknesses in recovery procedures
- Test under adversarial conditions
- Validate incident response
- Train team under stress

9.2 RED TEAM EXERCISE PLAN
---------------------------

Frequency: Annually or bi-annually

Duration: 4-8 hours

Participants:
- Red Team: 2-3 people (try to break recovery)
- Blue Team: Operations team (defend and recover)
- Observer: Management (evaluate response)

Rules:
- Red team can do anything except physical damage
- Blue team doesn't know when/how attack will come
- Everything must be documented
- System must be restored at end

9.3 RED TEAM SCENARIOS
----------------------

Scenario 1: Multi-Component Cascading Failure
----------------------------------------------
Red team triggers multiple failures in sequence to overwhelm blue team

Scenario 2: Subtle Data Corruption
-----------------------------------
Red team introduces hard-to-detect corruption that breaks recovery

Scenario 3: Documentation Sabotage
-----------------------------------
Red team subtly alters recovery documentation to cause confusion

Scenario 4: Insider Threat
---------------------------
Red team simulates malicious insider with system access

Scenario 5: Zero-Day Exploit
-----------------------------
Red team uses unknown vulnerability to crash system

Scenario 6: Social Engineering
-------------------------------
Red team tries to manipulate blue team during recovery

9.4 RED TEAM EXERCISE PROCEDURE
--------------------------------

Phase 1: Planning (1 week before)
----------------------------------
- Red team plans attack scenarios
- Observer sets up monitoring
- Blue team notified exercise will happen (not when/how)

Phase 2: Execution (Day of)
----------------------------
- Red team launches attack at surprise time
- Blue team must detect and respond
- Observer documents everything
- Exercise continues until full recovery

Phase 3: Debrief (Same day)
----------------------------
- Red team explains what they did
- Blue team explains their response
- Observer provides feedback
- Lessons learned documented

Phase 4: Follow-up (Next week)
-------------------------------
- Fix identified issues
- Update procedures
- Add tests to prevent issues

9.5 RED TEAM EVALUATION CRITERIA
---------------------------------

Score Blue Team on:
-------------------
1. Detection time (How fast did they notice the attack?)
2. Response quality (Did they follow procedures?)
3. Communication (How well did team coordinate?)
4. Adaptability (Could they handle unexpected situations?)
5. Recovery time (How fast did they recover?)
6. Validation (Did they properly validate recovered state?)

Score Red Team on:
------------------
1. Attack creativity (Novel scenarios?)
2. Attack effectiveness (Did they find weaknesses?)
3. Documentation (Did they document their actions?)

9.6 RED TEAM EXERCISE EXAMPLE
------------------------------

Example: 2025 Annual Red Team Exercise
---------------------------------------

Red Team Attack:
1. Corrupt checkpoint file subtly (flip 1 bit in position quantity)
2. When blue team restores from checkpoint, position is wrong
3. Wait for them to notice and recover
4. While they're recovering, introduce network partition
5. Force them to handle two simultaneous issues

Blue Team Response:
1. Restored from corrupted checkpoint (didn't notice corruption initially)
2. Started trading with wrong position
3. Noticed position discrepancy 5 minutes later (monitoring caught it)
4. Initiated recovery reconciliation
5. Network partition occurred during reconciliation
6. Switched to backup recovery procedure
7. Full recovery achieved in 18 minutes

Lessons Learned:
1. Need better checkpoint validation (checksum)
2. Position monitoring needs to alert faster
3. Recovery procedures need to handle concurrent failures
4. Good: Team adapted well to unexpected network partition
5. Good: Communication was clear throughout

Action Items:
1. Add SHA256 checksum to checkpoints (Done: 2025-11-30)
2. Reduce position monitoring interval from 5 min to 1 min (Done: 2025-11-28)
3. Create procedure for handling multiple simultaneous failures (Done: 2025-12-05)

================================================================================
10. BEST PRACTICES & CHECKLISTS
================================================================================

10.1 RECOVERY TESTING BEST PRACTICES
-------------------------------------

1. Test regularly (not just once)
   - Monthly drills minimum
   - More frequent for critical systems

2. Test in production-like environment
   - Same hardware, same configuration
   - Real exchange connections (testnet)

3. Test different failure scenarios
   - Don't just test one type of crash
   - Cover timing variations

4. Document everything
   - Record every test
   - Track metrics over time
   - Learn from each test

5. Improve after each test
   - Fix issues found
   - Update procedures
   - Add regression tests

6. Involve the whole team
   - Rotate who leads drills
   - Cross-train team members
   - Share knowledge

7. Measure and track RTO/RPO
   - Set targets
   - Track actuals
   - Trend over time

8. Automate as much as possible
   - Automated test execution
   - Automated validation
   - Automated reporting

9. Have fun with it
   - Gamify the drills
   - Reward good performance
   - Don't make it pure stress

10. Learn from other industries
    - Aviation: Simulator training
    - Military: War games
    - Fire departments: Drills

10.2 MONTHLY DRILL CHECKLIST
-----------------------------

Week Before:
[ ] Schedule drill time
[ ] Select drill type (hot/warm/cold)
[ ] Assign roles (lead, support, observer)
[ ] Prepare test environment
[ ] Verify monitoring in place
[ ] Create test scenario
[ ] Brief participants

Day Of:
[ ] Take system snapshot (pre-drill)
[ ] Start recording/logging
[ ] Inject failure
[ ] Start timer
[ ] Observer documents actions
[ ] Team executes recovery
[ ] Stop timer when complete
[ ] Validate recovered state
[ ] Take system snapshot (post-drill)

After:
[ ] Calculate RTO/RPO
[ ] Review recording
[ ] Discuss what went well
[ ] Discuss what went wrong
[ ] Identify action items
[ ] Generate report
[ ] Send report to team
[ ] Update procedures
[ ] Schedule next drill

10.3 RECOVERY TESTING ANTI-PATTERNS
------------------------------------

Don't:
-----
1. Only test during off-hours
   - Need to test during high-load periods too

2. Only test clean shutdowns
   - Crashes are usually abrupt, not clean

3. Only test single component failures
   - Real incidents often involve multiple failures

4. Never fail intentionally
   - If you never test failure, it will fail in production

5. Assume documentation is correct
   - Test validates documentation

6. Skip validation after recovery
   - Recovery isn't complete until validated

7. Only test hot failover
   - Need to test all restart types

8. Blame team for drill failures
   - Point of drill is to find issues safely

9. Run drills too infrequently
   - Skills decay, systems change

10. Ignore drill results
    - If you don't fix issues found, why test?

10.4 RECOVERY TESTING METRICS
------------------------------

Track these metrics:

1. Drill frequency (per month)
   Target: â‰¥1

2. Average RTO (by type)
   Hot: <5 sec
   Warm: <5 min
   Cold: <30 min

3. Average RPO
   Target: 0 (no data loss)

4. Drill success rate
   Target: >95%

5. Issues found per drill
   Target: 1-2 (finding issues is good!)

6. Time to fix issues
   Target: <1 week

7. Regression rate
   Target: <5% (same issue recurring)

8. Team confidence score (survey)
   Target: >4/5

9. Documentation accuracy
   Target: >90% of steps correct

10. Test coverage
    Target: All recovery scenarios tested annually

10.5 TROUBLESHOOTING RECOVERY TESTS
------------------------------------

Problem: Tests taking too long to run
Solution:
- Parallelize tests
- Use smaller test data sets
- Optimize validation steps

Problem: Tests failing randomly (flaky)
Solution:
- Add retries for network operations
- Increase timeouts
- Fix race conditions

Problem: Tests failing in CI but passing locally
Solution:
- Ensure CI environment matches local
- Check for timing differences
- Review CI logs for clues

Problem: Hard to reproduce production issues in test
Solution:
- Use production-like data
- Use real exchange testnet
- Increase test complexity

Problem: Team doesn't take drills seriously
Solution:
- Get management support
- Make drills more realistic
- Share success stories
- Gamify with friendly competition

================================================================================
END OF DOCUMENT
================================================================================