================================================================================
SECURITY AUDIT LOGGING
High-Frequency Trading System - Comprehensive Audit Trail
================================================================================

VERSION: 2.0
LAST UPDATED: 2025-11-26
CLASSIFICATION: CONFIDENTIAL

================================================================================
TABLE OF CONTENTS
================================================================================

1. Overview
2. Audit Log Requirements
3. Event Types and Classification
4. Log Data Structure
5. C++ Audit Logger Implementation
6. Python Log Analysis Scripts
7. SIEM Integration
8. Compliance Reporting
9. Log Retention and Archival
10. Security Monitoring

================================================================================
1. OVERVIEW
================================================================================

Audit logging is critical for:
- Security incident investigation
- Compliance (SOC 2, PCI-DSS, FINRA, MiFID II)
- Accountability and non-repudiation
- Anomaly detection
- Performance analysis
- Debugging and troubleshooting

REGULATORY REQUIREMENTS:

FINRA Rule 4511: Recordkeeping
- Maintain records of all orders, executions, and modifications
- Records must be preserved for 6 years
- Must be readily accessible for 2 years

SOC 2 Type II:
- Log all access to sensitive data
- Monitor for unauthorized access attempts
- Retain logs for audit purposes
- Implement log integrity controls

PCI-DSS Requirement 10:
- Track and monitor all access to cardholder data
- Log all authentication attempts
- Retain audit logs for at least 1 year
- Implement automated audit trail review

MiFID II Article 16(6):
- Record all order-related communications
- Maintain comprehensive audit trail
- Records must be tamper-proof

AUDIT LOG PRINCIPLES:

Completeness: Log all security-relevant events
Accuracy: Precise timestamps and event details
Integrity: Protect logs from tampering
Confidentiality: Secure log storage and transmission
Availability: Logs accessible for authorized reviewers
Retention: Meet regulatory retention requirements

================================================================================
2. AUDIT LOG REQUIREMENTS
================================================================================

2.1 WHAT TO LOG
---------------

AUTHENTICATION EVENTS:
✓ Login attempts (successful and failed)
✓ Logout events
✓ MFA challenges and responses
✓ Session creation and termination
✓ Token issuance and refresh
✓ Password changes
✓ Account lockouts

AUTHORIZATION EVENTS:
✓ Access granted or denied
✓ Role assignments and changes
✓ Permission modifications
✓ Policy updates
✓ Privilege escalation attempts

API ACCESS:
✓ All API requests (endpoint, method, parameters)
✓ Response codes and latency
✓ Rate limit violations
✓ API key usage
✓ Service-to-service calls

DATA ACCESS:
✓ Read/write operations on sensitive data
✓ Database queries
✓ File access
✓ Export operations
✓ Bulk data retrieval

CONFIGURATION CHANGES:
✓ System configuration modifications
✓ Security policy updates
✓ Network configuration changes
✓ Application deployments

SECURITY EVENTS:
✓ Intrusion detection alerts
✓ Malware detection
✓ Firewall rule violations
✓ Certificate errors
✓ Encryption failures

TRADING EVENTS:
✓ Order creation, modification, cancellation
✓ Trade executions
✓ Risk limit breaches
✓ Algorithm start/stop
✓ Emergency kill switch activation

2.2 WHAT NOT TO LOG (PII/SECRETS)
----------------------------------

❌ Passwords (even hashed)
❌ API secrets
❌ Private keys
❌ Credit card numbers (PCI-DSS)
❌ Social security numbers
❌ Full authentication tokens (log token ID only)
❌ Personal health information

================================================================================
3. LOG DATA STRUCTURE
================================================================================

3.1 STANDARD LOG FORMAT (JSON)
-------------------------------

```json
{
  "timestamp": "2025-11-26T19:45:00.123456Z",
  "event_id": "evt_abc123xyz456",
  "event_type": "AUTHENTICATION",
  "event_subtype": "LOGIN_SUCCESS",
  "severity": "INFO",
  "actor": {
    "user_id": "user_12345",
    "username": "trader@example.com",
    "ip_address": "203.0.113.42",
    "user_agent": "Mozilla/5.0...",
    "session_id": "session_xyz789"
  },
  "target": {
    "resource_type": "USER_ACCOUNT",
    "resource_id": "user_12345",
    "resource_name": "trader@example.com"
  },
  "action": "LOGIN",
  "result": "SUCCESS",
  "details": {
    "authentication_method": "PASSWORD_MFA",
    "mfa_type": "TOTP",
    "login_duration_ms": 234
  },
  "context": {
    "service": "auth-service",
    "environment": "production",
    "region": "us-east-1",
    "hostname": "auth-prod-01"
  },
  "correlation_id": "req_abc123",
  "request_id": "req_def456"
}
```

3.2 EVENT SEVERITY LEVELS
--------------------------

CRITICAL: System compromise, data breach, unauthorized admin access
ERROR: Authentication failure, authorization denial, system errors
WARNING: Failed login attempts, rate limit violations, unusual activity
INFO: Successful operations, routine events
DEBUG: Detailed diagnostic information (non-production)

================================================================================
4. C++ AUDIT LOGGER IMPLEMENTATION
================================================================================

4.1 AUDIT LOGGER CLASS
-----------------------

```cpp
#include <json/json.h>
#include <chrono>
#include <string>
#include <fstream>
#include <mutex>
#include <queue>
#include <thread>
#include <kafka/KafkaProducer.h>

class AuditLogger {
public:
    enum class EventType {
        AUTHENTICATION,
        AUTHORIZATION,
        DATA_ACCESS,
        CONFIGURATION_CHANGE,
        SECURITY_EVENT,
        TRADING_EVENT,
        SYSTEM_EVENT
    };

    enum class Severity {
        DEBUG,
        INFO,
        WARNING,
        ERROR,
        CRITICAL
    };

    struct AuditEvent {
        std::string event_id;
        EventType event_type;
        std::string event_subtype;
        Severity severity;

        std::string actor_user_id;
        std::string actor_username;
        std::string actor_ip;
        std::string actor_user_agent;
        std::string actor_session_id;

        std::string target_resource_type;
        std::string target_resource_id;
        std::string target_resource_name;

        std::string action;
        std::string result;  // SUCCESS, FAILURE, DENIED

        Json::Value details;

        std::string service;
        std::string environment;
        std::string correlation_id;

        std::chrono::system_clock::time_point timestamp;
    };

    AuditLogger(const std::string& service_name,
               const std::string& environment,
               const std::string& kafka_brokers)
        : service_name_(service_name),
          environment_(environment),
          running_(true) {

        // Initialize Kafka producer for real-time streaming
        kafka_producer_ = std::make_unique<kafka::KafkaProducer>(kafka_brokers);

        // Start async log writer thread
        writer_thread_ = std::thread(&AuditLogger::logWriterThread, this);
    }

    ~AuditLogger() {
        running_ = false;
        cv_.notify_one();
        if (writer_thread_.joinable()) {
            writer_thread_.join();
        }
    }

    // Log authentication event
    void logAuthentication(const std::string& user_id,
                          const std::string& username,
                          const std::string& ip_address,
                          const std::string& action,
                          bool success,
                          const std::string& method = "PASSWORD") {
        AuditEvent event;
        event.event_id = generateEventID();
        event.event_type = EventType::AUTHENTICATION;
        event.event_subtype = action + (success ? "_SUCCESS" : "_FAILURE");
        event.severity = success ? Severity::INFO : Severity::WARNING;

        event.actor_user_id = user_id;
        event.actor_username = username;
        event.actor_ip = ip_address;

        event.target_resource_type = "USER_ACCOUNT";
        event.target_resource_id = user_id;

        event.action = action;
        event.result = success ? "SUCCESS" : "FAILURE";

        event.details["authentication_method"] = method;

        event.service = service_name_;
        event.environment = environment_;
        event.timestamp = std::chrono::system_clock::now();

        enqueueEvent(event);
    }

    // Log authorization decision
    void logAuthorization(const std::string& user_id,
                         const std::string& action,
                         const std::string& resource_type,
                         const std::string& resource_id,
                         bool allowed,
                         const std::string& reason = "") {
        AuditEvent event;
        event.event_id = generateEventID();
        event.event_type = EventType::AUTHORIZATION;
        event.event_subtype = "ACCESS_" + std::string(allowed ? "GRANTED" : "DENIED");
        event.severity = allowed ? Severity::INFO : Severity::WARNING;

        event.actor_user_id = user_id;

        event.target_resource_type = resource_type;
        event.target_resource_id = resource_id;

        event.action = action;
        event.result = allowed ? "GRANTED" : "DENIED";

        if (!reason.empty()) {
            event.details["reason"] = reason;
        }

        event.service = service_name_;
        event.environment = environment_;
        event.timestamp = std::chrono::system_clock::now();

        enqueueEvent(event);
    }

    // Log data access
    void logDataAccess(const std::string& user_id,
                      const std::string& operation,
                      const std::string& table_name,
                      const std::string& record_id,
                      int records_affected = 1) {
        AuditEvent event;
        event.event_id = generateEventID();
        event.event_type = EventType::DATA_ACCESS;
        event.event_subtype = operation;
        event.severity = Severity::INFO;

        event.actor_user_id = user_id;

        event.target_resource_type = "DATABASE_TABLE";
        event.target_resource_name = table_name;
        event.target_resource_id = record_id;

        event.action = operation;
        event.result = "SUCCESS";

        event.details["records_affected"] = records_affected;

        event.service = service_name_;
        event.environment = environment_;
        event.timestamp = std::chrono::system_clock::now();

        enqueueEvent(event);
    }

    // Log trading event
    void logTradingEvent(const std::string& trader_id,
                        const std::string& event_type,
                        const std::string& symbol,
                        double quantity,
                        double price,
                        const std::string& order_id) {
        AuditEvent event;
        event.event_id = generateEventID();
        event.event_type = EventType::TRADING_EVENT;
        event.event_subtype = event_type;
        event.severity = Severity::INFO;

        event.actor_user_id = trader_id;

        event.target_resource_type = "TRADING_ORDER";
        event.target_resource_id = order_id;

        event.action = event_type;
        event.result = "SUCCESS";

        event.details["symbol"] = symbol;
        event.details["quantity"] = quantity;
        event.details["price"] = price;

        event.service = service_name_;
        event.environment = environment_;
        event.timestamp = std::chrono::system_clock::now();

        enqueueEvent(event);
    }

    // Log security event
    void logSecurityEvent(const std::string& event_subtype,
                         Severity severity,
                         const std::string& source_ip,
                         const std::string& description) {
        AuditEvent event;
        event.event_id = generateEventID();
        event.event_type = EventType::SECURITY_EVENT;
        event.event_subtype = event_subtype;
        event.severity = severity;

        event.actor_ip = source_ip;

        event.action = event_subtype;
        event.result = "DETECTED";

        event.details["description"] = description;

        event.service = service_name_;
        event.environment = environment_;
        event.timestamp = std::chrono::system_clock::now();

        enqueueEvent(event);
    }

private:
    std::string service_name_;
    std::string environment_;

    std::queue<AuditEvent> event_queue_;
    std::mutex queue_mutex_;
    std::condition_variable cv_;
    std::thread writer_thread_;
    std::atomic<bool> running_;

    std::unique_ptr<kafka::KafkaProducer> kafka_producer_;

    std::string generateEventID() {
        static std::atomic<uint64_t> counter{0};
        auto now = std::chrono::system_clock::now();
        auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(
            now.time_since_epoch()).count();

        std::stringstream ss;
        ss << "evt_" << ms << "_" << counter++;
        return ss.str();
    }

    void enqueueEvent(const AuditEvent& event) {
        std::lock_guard<std::mutex> lock(queue_mutex_);
        event_queue_.push(event);
        cv_.notify_one();
    }

    void logWriterThread() {
        while (running_) {
            std::unique_lock<std::mutex> lock(queue_mutex_);
            cv_.wait(lock, [this] { return !event_queue_.empty() || !running_; });

            while (!event_queue_.empty()) {
                AuditEvent event = event_queue_.front();
                event_queue_.pop();
                lock.unlock();

                // Write to multiple destinations
                writeToFile(event);
                writeToKafka(event);
                writeToSIEM(event);

                lock.lock();
            }
        }
    }

    void writeToFile(const AuditEvent& event) {
        Json::Value json = eventToJSON(event);
        Json::FastWriter writer;
        std::string json_str = writer.write(json);

        // Write to daily log file
        auto t = std::chrono::system_clock::to_time_t(event.timestamp);
        std::tm* tm = std::localtime(&t);
        char date[11];
        std::strftime(date, sizeof(date), "%Y-%m-%d", tm);

        std::string log_file = "/var/log/hft/audit/audit_" + std::string(date) + ".log";

        std::ofstream ofs(log_file, std::ios::app);
        if (ofs.is_open()) {
            ofs << json_str << std::endl;
            ofs.close();
        }
    }

    void writeToKafka(const AuditEvent& event) {
        Json::Value json = eventToJSON(event);
        Json::FastWriter writer;
        std::string json_str = writer.write(json);

        // Send to Kafka topic for real-time streaming
        kafka_producer_->send("audit-logs", json_str);
    }

    void writeToSIEM(const AuditEvent& event) {
        // Send to SIEM (Splunk, ELK, etc.) via syslog or HTTP
        // Implementation depends on SIEM system
    }

    Json::Value eventToJSON(const AuditEvent& event) {
        Json::Value json;

        // Timestamp
        auto ms = std::chrono::duration_cast<std::chrono::milliseconds>(
            event.timestamp.time_since_epoch()).count();
        json["timestamp"] = static_cast<Json::Int64>(ms);

        json["event_id"] = event.event_id;
        json["event_type"] = eventTypeToString(event.event_type);
        json["event_subtype"] = event.event_subtype;
        json["severity"] = severityToString(event.severity);

        // Actor
        Json::Value actor;
        actor["user_id"] = event.actor_user_id;
        actor["username"] = event.actor_username;
        actor["ip_address"] = event.actor_ip;
        actor["user_agent"] = event.actor_user_agent;
        actor["session_id"] = event.actor_session_id;
        json["actor"] = actor;

        // Target
        Json::Value target;
        target["resource_type"] = event.target_resource_type;
        target["resource_id"] = event.target_resource_id;
        target["resource_name"] = event.target_resource_name;
        json["target"] = target;

        json["action"] = event.action;
        json["result"] = event.result;
        json["details"] = event.details;

        // Context
        Json::Value context;
        context["service"] = event.service;
        context["environment"] = event.environment;
        json["context"] = context;

        json["correlation_id"] = event.correlation_id;

        return json;
    }

    std::string eventTypeToString(EventType type) {
        switch (type) {
            case EventType::AUTHENTICATION: return "AUTHENTICATION";
            case EventType::AUTHORIZATION: return "AUTHORIZATION";
            case EventType::DATA_ACCESS: return "DATA_ACCESS";
            case EventType::CONFIGURATION_CHANGE: return "CONFIGURATION_CHANGE";
            case EventType::SECURITY_EVENT: return "SECURITY_EVENT";
            case EventType::TRADING_EVENT: return "TRADING_EVENT";
            case EventType::SYSTEM_EVENT: return "SYSTEM_EVENT";
            default: return "UNKNOWN";
        }
    }

    std::string severityToString(Severity severity) {
        switch (severity) {
            case Severity::DEBUG: return "DEBUG";
            case Severity::INFO: return "INFO";
            case Severity::WARNING: return "WARNING";
            case Severity::ERROR: return "ERROR";
            case Severity::CRITICAL: return "CRITICAL";
            default: return "UNKNOWN";
        }
    }
};
```

================================================================================
5. PYTHON LOG ANALYSIS SCRIPTS
================================================================================

5.1 LOG ANALYZER
----------------

```python
#!/usr/bin/env python3
"""
Audit Log Analysis and Reporting
"""

import json
import os
from datetime import datetime, timedelta
from collections import defaultdict
from typing import List, Dict
import pandas as pd

class AuditLogAnalyzer:
    """Analyzes audit logs for security insights"""

    def __init__(self, log_directory: str):
        self.log_directory = log_directory

    def load_logs(self, start_date: datetime, end_date: datetime) -> List[Dict]:
        """Load audit logs for date range"""
        logs = []
        current = start_date

        while current <= end_date:
            log_file = os.path.join(
                self.log_directory,
                f"audit_{current.strftime('%Y-%m-%d')}.log"
            )

            if os.path.exists(log_file):
                with open(log_file, 'r') as f:
                    for line in f:
                        try:
                            log_entry = json.loads(line.strip())
                            logs.append(log_entry)
                        except json.JSONDecodeError:
                            continue

            current += timedelta(days=1)

        return logs

    def detect_failed_login_attempts(self, logs: List[Dict],
                                     threshold: int = 5) -> List[Dict]:
        """Detect multiple failed login attempts"""
        failed_logins = defaultdict(list)

        for log in logs:
            if (log.get('event_type') == 'AUTHENTICATION' and
                log.get('result') == 'FAILURE'):

                user_id = log.get('actor', {}).get('user_id', 'unknown')
                failed_logins[user_id].append(log)

        # Find users with >= threshold failures
        suspicious_users = []
        for user_id, attempts in failed_logins.items():
            if len(attempts) >= threshold:
                suspicious_users.append({
                    'user_id': user_id,
                    'failed_attempts': len(attempts),
                    'ips': list(set([a.get('actor', {}).get('ip_address')
                                   for a in attempts])),
                    'timestamps': [a.get('timestamp') for a in attempts]
                })

        return suspicious_users

    def detect_privilege_escalation(self, logs: List[Dict]) -> List[Dict]:
        """Detect potential privilege escalation"""
        role_changes = []

        for log in logs:
            if (log.get('event_type') == 'AUTHORIZATION' and
                'role' in log.get('event_subtype', '').lower()):

                role_changes.append(log)

        return role_changes

    def detect_after_hours_access(self, logs: List[Dict],
                                  work_hours: tuple = (9, 17)) -> List[Dict]:
        """Detect access outside of normal business hours"""
        after_hours = []

        for log in logs:
            timestamp = log.get('timestamp')
            if timestamp:
                dt = datetime.fromtimestamp(timestamp / 1000)
                hour = dt.hour

                if hour < work_hours[0] or hour >= work_hours[1]:
                    after_hours.append(log)

        return after_hours

    def generate_user_activity_report(self, logs: List[Dict], user_id: str) -> Dict:
        """Generate activity report for specific user"""
        user_logs = [log for log in logs
                    if log.get('actor', {}).get('user_id') == user_id]

        report = {
            'user_id': user_id,
            'total_events': len(user_logs),
            'event_types': defaultdict(int),
            'actions': defaultdict(int),
            'resources_accessed': set(),
            'ip_addresses': set(),
            'failed_authentications': 0,
            'denied_authorizations': 0
        }

        for log in user_logs:
            event_type = log.get('event_type', 'UNKNOWN')
            action = log.get('action', 'UNKNOWN')

            report['event_types'][event_type] += 1
            report['actions'][action] += 1

            target = log.get('target', {})
            if target.get('resource_id'):
                report['resources_accessed'].add(target['resource_id'])

            ip = log.get('actor', {}).get('ip_address')
            if ip:
                report['ip_addresses'].add(ip)

            if (event_type == 'AUTHENTICATION' and
                log.get('result') == 'FAILURE'):
                report['failed_authentications'] += 1

            if (event_type == 'AUTHORIZATION' and
                log.get('result') == 'DENIED'):
                report['denied_authorizations'] += 1

        # Convert sets to lists for JSON serialization
        report['resources_accessed'] = list(report['resources_accessed'])
        report['ip_addresses'] = list(report['ip_addresses'])

        return report

    def generate_compliance_report(self, logs: List[Dict],
                                   start_date: datetime,
                                   end_date: datetime) -> Dict:
        """Generate compliance report"""
        report = {
            'report_period': {
                'start': start_date.isoformat(),
                'end': end_date.isoformat()
            },
            'total_events': len(logs),
            'authentication_events': 0,
            'authorization_events': 0,
            'data_access_events': 0,
            'trading_events': 0,
            'security_events': 0,
            'unique_users': set(),
            'failed_login_attempts': 0,
            'denied_access_attempts': 0,
            'critical_events': 0
        }

        for log in logs:
            event_type = log.get('event_type')

            if event_type == 'AUTHENTICATION':
                report['authentication_events'] += 1
                if log.get('result') == 'FAILURE':
                    report['failed_login_attempts'] += 1

            elif event_type == 'AUTHORIZATION':
                report['authorization_events'] += 1
                if log.get('result') == 'DENIED':
                    report['denied_access_attempts'] += 1

            elif event_type == 'DATA_ACCESS':
                report['data_access_events'] += 1

            elif event_type == 'TRADING_EVENT':
                report['trading_events'] += 1

            elif event_type == 'SECURITY_EVENT':
                report['security_events'] += 1

            if log.get('severity') == 'CRITICAL':
                report['critical_events'] += 1

            user_id = log.get('actor', {}).get('user_id')
            if user_id:
                report['unique_users'].add(user_id)

        report['unique_users'] = len(report['unique_users'])

        return report

def main():
    analyzer = AuditLogAnalyzer('/var/log/hft/audit')

    # Analyze last 7 days
    end_date = datetime.now()
    start_date = end_date - timedelta(days=7)

    print("Loading audit logs...")
    logs = analyzer.load_logs(start_date, end_date)
    print(f"Loaded {len(logs)} log entries")

    # Detect suspicious activity
    print("\n=== Failed Login Attempts ===")
    failed_logins = analyzer.detect_failed_login_attempts(logs, threshold=5)
    for user in failed_logins:
        print(f"User: {user['user_id']}, Attempts: {user['failed_attempts']}, "
              f"IPs: {user['ips']}")

    # After-hours access
    print("\n=== After-Hours Access ===")
    after_hours = analyzer.detect_after_hours_access(logs)
    print(f"Found {len(after_hours)} after-hours access events")

    # Compliance report
    print("\n=== Compliance Report ===")
    compliance = analyzer.generate_compliance_report(logs, start_date, end_date)
    print(json.dumps(compliance, indent=2, default=str))

if __name__ == '__main__':
    main()
```

================================================================================
6. LOG RETENTION AND ARCHIVAL
================================================================================

6.1 LOG RETENTION POLICY
-------------------------

RETENTION PERIODS:
- Hot storage (searchable): 90 days
- Warm storage (compressed): 1 year
- Cold storage (archived): 7 years (compliance)

IMPLEMENTATION:

```python
#!/usr/bin/env python3
"""
Audit Log Retention and Archival
"""

import os
import gzip
import shutil
from datetime import datetime, timedelta
import boto3

class LogRetentionManager:
    """Manages log retention and archival"""

    def __init__(self, log_dir: str, archive_bucket: str):
        self.log_dir = log_dir
        self.s3_client = boto3.client('s3')
        self.archive_bucket = archive_bucket

    def compress_old_logs(self, days_old: int = 30):
        """Compress logs older than specified days"""
        cutoff_date = datetime.now() - timedelta(days=days_old)

        for filename in os.listdir(self.log_dir):
            if not filename.endswith('.log'):
                continue

            filepath = os.path.join(self.log_dir, filename)
            file_date = datetime.fromtimestamp(os.path.getctime(filepath))

            if file_date < cutoff_date and not filename.endswith('.gz'):
                # Compress file
                with open(filepath, 'rb') as f_in:
                    with gzip.open(filepath + '.gz', 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)

                # Delete original
                os.remove(filepath)
                print(f"Compressed: {filename}")

    def archive_to_s3(self, days_old: int = 90):
        """Archive logs to S3 for long-term storage"""
        cutoff_date = datetime.now() - timedelta(days=days_old)

        for filename in os.listdir(self.log_dir):
            if not filename.endswith('.gz'):
                continue

            filepath = os.path.join(self.log_dir, filename)
            file_date = datetime.fromtimestamp(os.path.getctime(filepath))

            if file_date < cutoff_date:
                # Upload to S3
                s3_key = f"audit-logs/{datetime.now().year}/{filename}"
                self.s3_client.upload_file(filepath, self.archive_bucket, s3_key)

                # Delete local file after successful upload
                os.remove(filepath)
                print(f"Archived to S3: {filename}")

    def delete_old_archives(self, days_old: int = 2555):  # 7 years
        """Delete archives older than retention period"""
        cutoff_date = datetime.now() - timedelta(days=days_old)

        # List S3 objects
        response = self.s3_client.list_objects_v2(
            Bucket=self.archive_bucket,
            Prefix='audit-logs/'
        )

        for obj in response.get('Contents', []):
            last_modified = obj['LastModified'].replace(tzinfo=None)

            if last_modified < cutoff_date:
                self.s3_client.delete_object(
                    Bucket=self.archive_bucket,
                    Key=obj['Key']
                )
                print(f"Deleted: {obj['Key']}")

def main():
    manager = LogRetentionManager(
        log_dir='/var/log/hft/audit',
        archive_bucket='hft-audit-logs-archive'
    )

    # Compress logs older than 30 days
    manager.compress_old_logs(days_old=30)

    # Archive to S3 logs older than 90 days
    manager.archive_to_s3(days_old=90)

    # Delete archives older than 7 years
    manager.delete_old_archives(days_old=2555)

if __name__ == '__main__':
    main()
```

================================================================================
7. BEST PRACTICES
================================================================================

1. LOG IN STRUCTURED FORMAT (JSON)
   - Easy parsing and analysis
   - Consistent schema
   - Support for nested data

2. USE CENTRALIZED LOGGING
   - Send to Kafka, Elasticsearch, or SIEM
   - Enable real-time monitoring
   - Facilitate cross-service correlation

3. PROTECT LOG INTEGRITY
   - Use write-once storage
   - Implement log signing
   - Restrict log modification access

4. IMPLEMENT LOG ROTATION
   - Daily log files
   - Automatic compression
   - Archive to cold storage

5. MONITOR LOG VOLUME
   - Alert on unusual spikes
   - Implement sampling if needed
   - Prevent disk space exhaustion

6. INCLUDE CORRELATION IDS
   - Track requests across services
   - Facilitate debugging
   - Enable end-to-end tracing

7. SANITIZE SENSITIVE DATA
   - Never log passwords or secrets
   - Mask PII when necessary
   - Comply with privacy regulations

8. IMPLEMENT AUTOMATED ANALYSIS
   - Real-time anomaly detection
   - Daily security reports
   - Compliance dashboards

================================================================================
END OF DOCUMENT
================================================================================
