================================================================================
VALGRIND - MEMORY PROFILING FOR HFT C++
================================================================================

THEORY:
-------
Valgrind is a dynamic analysis framework with multiple tools for memory
debugging and profiling. It works by running your program in a virtual
machine that instruments every memory access.

KEY TOOLS:
- Memcheck: Memory error detection (leaks, invalid access)
- Massif: Heap profiler
- Cachegrind: Cache and branch prediction profiler
- Callgrind: Call-graph generating cache profiler
- Helgrind: Thread error detector
- DRD: Data race detector

In HFT, memory management is crucial - memory leaks cause degradation over
time, and inefficient allocation patterns increase latency.

USE CASES IN HFT:
-----------------
1. Detect memory leaks in long-running trading systems
2. Find heap allocation bottlenecks
3. Identify cache-inefficient memory access patterns
4. Debug segmentation faults and memory corruption
5. Optimize memory layout for better cache performance

INSTALLATION:
-------------
sudo apt-get install valgrind

MEMCHECK - MEMORY ERROR DETECTION:
-----------------------------------

THEORY:
Memcheck detects:
- Memory leaks (allocated but never freed)
- Use of uninitialized memory
- Invalid memory access (buffer overflows)
- Double frees
- Mismatched allocation/deallocation

PROCESS:
valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all \
         --track-origins=yes --verbose ./your_hft_app

Options explained:
--leak-check=full        : Detailed leak information
--show-leak-kinds=all    : Show all types of leaks
--track-origins=yes      : Track origin of uninitialized values
--verbose                : More detailed output
--log-file=memcheck.log  : Save output to file

HFT C++ EXAMPLE (Memory Issues):

// memory_issues_example.cpp
#include <iostream>
#include <vector>
#include <cstring>

class MarketDataHandler {
    double* prices;
    size_t capacity;

public:
    MarketDataHandler(size_t cap) : capacity(cap) {
        prices = new double[capacity];
        // BUG: Uninitialized memory
    }

    ~MarketDataHandler() {
        delete[] prices;
    }

    void updatePrice(size_t index, double price) {
        // BUG: No bounds checking - potential overflow
        prices[index] = price;
    }

    double getAverage() {
        double sum = 0;
        for (size_t i = 0; i < capacity; ++i) {
            // BUG: Reading uninitialized memory
            sum += prices[i];
        }
        return sum / capacity;
    }
};

void memoryLeak() {
    // BUG: Memory leak
    int* leak = new int[1000];
    // Never deleted!
}

int main() {
    MarketDataHandler handler(1000);

    // Update some prices
    for (size_t i = 0; i < 100; ++i) {
        handler.updatePrice(i, 100.0 + i * 0.01);
    }

    // BUG: Reading uninitialized memory for indices 100-999
    double avg = handler.getAverage();
    std::cout << "Average: " << avg << std::endl;

    // BUG: Buffer overflow
    handler.updatePrice(1500, 100.0);

    memoryLeak();

    return 0;
}

WORKFLOW:
---------
# Compile with debug symbols, no optimizations for better traces
g++ -g -O0 -std=c++17 memory_issues_example.cpp -o mem_test

# Run Memcheck
valgrind --tool=memcheck --leak-check=full --track-origins=yes ./mem_test

# Save detailed report
valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all \
         --track-origins=yes --log-file=memcheck_report.txt ./mem_test


MASSIF - HEAP PROFILER:
------------------------

THEORY:
Massif profiles heap memory usage over time. It shows:
- Total heap usage
- Memory allocated at each point in time
- Call stacks responsible for allocations
- Peak memory usage

Critical for HFT: Identify allocation hotspots that increase latency.

PROCESS:
valgrind --tool=massif --massif-out-file=massif.out ./your_hft_app
ms_print massif.out > massif_report.txt

Advanced options:
--heap=yes              : Profile heap (default)
--stacks=yes            : Also profile stack memory
--time-unit=ms          : Use milliseconds instead of instructions
--detailed-freq=1       : More frequent snapshots

HFT C++ EXAMPLE (Heap Usage):

// heap_profiling_example.cpp
#include <iostream>
#include <vector>
#include <memory>
#include <unordered_map>

struct Order {
    uint64_t order_id;
    double price;
    uint64_t quantity;
    uint64_t timestamp;
};

class OrderManager {
    std::unordered_map<uint64_t, std::unique_ptr<Order>> orders;
    std::vector<Order> order_history;

public:
    void processOrders(size_t count) {
        for (size_t i = 0; i < count; ++i) {
            // Heap allocation via unique_ptr
            auto order = std::make_unique<Order>();
            order->order_id = i;
            order->price = 100.0 + (i % 1000) * 0.01;
            order->quantity = 100;
            order->timestamp = i;

            orders[i] = std::move(order);

            // Also keep in history (additional memory)
            order_history.push_back(*orders[i]);
        }
    }

    void clearOldOrders(size_t keep_recent) {
        if (orders.size() > keep_recent) {
            auto it = orders.begin();
            std::advance(it, orders.size() - keep_recent);
            orders.erase(orders.begin(), it);
        }
    }
};

int main() {
    OrderManager manager;

    // Simulate order processing with growing heap
    for (int round = 0; round < 10; ++round) {
        manager.processOrders(10000);
        std::cout << "Round " << round << " complete\n";
    }

    return 0;
}

WORKFLOW:
---------
# Compile
g++ -g -O2 -std=c++17 heap_profiling_example.cpp -o heap_test

# Run Massif
valgrind --tool=massif --massif-out-file=massif.out ./heap_test

# Analyze results
ms_print massif.out

# For better visualization, use massif-visualizer (GUI)
massif-visualizer massif.out


CACHEGRIND - CACHE PROFILING:
------------------------------

THEORY:
Cachegrind simulates CPU cache hierarchy and branch prediction.
Reports:
- L1/L2 cache reads and writes
- Cache misses
- Branch instructions and mispredictions

Critical for HFT: Cache misses add 100+ CPU cycles of latency.

PROCESS:
valgrind --tool=cachegrind --cachegrind-out-file=cachegrind.out ./your_hft_app
cg_annotate cachegrind.out

Options:
--I1=32768,8,64         : L1 instruction cache config
--D1=32768,8,64         : L1 data cache config
--LL=8388608,16,64      : Last level cache config
--branch-sim=yes        : Enable branch prediction simulation

HFT C++ EXAMPLE (Cache Optimization):

// cache_example.cpp
#include <iostream>
#include <vector>
#include <algorithm>

// Poor cache locality
void process_column_major(std::vector<std::vector<double>>& matrix) {
    size_t rows = matrix.size();
    size_t cols = matrix[0].size();

    // Access column-wise (poor cache locality)
    for (size_t col = 0; col < cols; ++col) {
        for (size_t row = 0; row < rows; ++row) {
            matrix[row][col] *= 1.01; // Cache miss likely
        }
    }
}

// Good cache locality
void process_row_major(std::vector<std::vector<double>>& matrix) {
    // Access row-wise (good cache locality)
    for (auto& row : matrix) {
        for (auto& val : row) {
            val *= 1.01; // Sequential access, cache friendly
        }
    }
}

int main() {
    // Create 1000x1000 matrix
    std::vector<std::vector<double>> matrix(1000, std::vector<double>(1000, 100.0));

    // Test with poor cache locality
    process_column_major(matrix);

    // Uncomment to test with good cache locality
    // process_row_major(matrix);

    std::cout << "Processing complete\n";
    return 0;
}

WORKFLOW:
---------
# Compile
g++ -g -O2 -std=c++17 cache_example.cpp -o cache_test

# Run Cachegrind
valgrind --tool=cachegrind --branch-sim=yes --cachegrind-out-file=cache.out ./cache_test

# Analyze results
cg_annotate cache.out

# Annotate specific source file
cg_annotate cache.out cache_example.cpp


CALLGRIND - CALL-GRAPH PROFILER:
---------------------------------

THEORY:
Callgrind extends Cachegrind with call-graph generation.
Shows which functions call which, and the cost of each call path.

PROCESS:
valgrind --tool=callgrind --callgrind-out-file=callgrind.out ./your_hft_app
callgrind_annotate callgrind.out

# Visualize with KCachegrind (GUI)
kcachegrind callgrind.out

Options:
--dump-instr=yes        : Include instruction-level profiling
--collect-jumps=yes     : Collect jump information
--separate-threads=yes  : Separate data per thread


INTERPRETING RESULTS:
---------------------

MEMCHECK OUTPUT:
==12345== HEAP SUMMARY:
==12345==     in use at exit: 4,000 bytes in 1 blocks
==12345==   total heap usage: 10 allocs, 9 frees, 74,000 bytes allocated
==12345==
==12345== 4,000 bytes in 1 blocks are definitely lost
==12345==    at 0x4C2E0EF: operator new[](unsigned long)
==12345==    by 0x400A1B: memoryLeak() (in /path/to/program)

Action: Fix the memory leak in memoryLeak() function.

MASSIF PEAK:
    MB
25.0^                                                                       #
    |                                                                     @:#
    |                                                                   :@::#
    |                                                          :::::::@::@::#

Peak shows where maximum memory was used - optimize those allocations.

CACHEGRIND OUTPUT:
I refs:        1,000,000,000
I1 misses:         1,000,000
LLi misses:          100,000
D refs:          500,000,000
D1 misses:        50,000,000  (10% miss rate - HIGH!)
LLd misses:        5,000,000

High D1 miss rate (10%) indicates poor cache locality - needs optimization.


OPTIMIZATION TIPS FOR HFT:
--------------------------
1. Memory leaks:
   - Use smart pointers (unique_ptr, shared_ptr)
   - Implement RAII pattern
   - Use memory pools for frequent allocations

2. High heap allocation rate:
   - Pre-allocate memory pools
   - Use object recycling
   - Avoid allocations in critical path
   - Use stack allocation where possible

3. Cache misses:
   - Improve data locality
   - Use struct-of-arrays instead of array-of-structs
   - Align data structures to cache lines (64 bytes)
   - Prefetch data when access pattern is predictable

PERFORMANCE OVERHEAD:
---------------------
Valgrind has significant overhead:
- Memcheck: 10-50x slowdown
- Massif: 20x slowdown
- Cachegrind: 20-100x slowdown
- Callgrind: 10-50x slowdown

Don't use for latency measurements - use for correctness and optimization!

BEST PRACTICES FOR HFT:
-----------------------
1. Run Memcheck regularly in development
2. Profile heap usage under realistic load
3. Compare cache performance of different implementations
4. Use suppression files for known harmless issues
5. Combine with other profilers (perf, gprof) for complete picture
