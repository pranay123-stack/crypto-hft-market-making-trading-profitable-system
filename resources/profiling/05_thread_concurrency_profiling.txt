================================================================================
THREAD & CONCURRENCY PROFILING FOR HFT C++
================================================================================

THEORY:
-------
Multi-threading in HFT requires:
1. Lock-free/wait-free data structures (minimize contention)
2. CPU pinning (avoid context switches and cache coherency issues)
3. Avoiding false sharing (cache line bouncing between cores)
4. Detecting race conditions and deadlocks
5. Measuring thread synchronization overhead

Common issues:
- Lock contention (multiple threads waiting for same lock)
- False sharing (different threads accessing nearby memory)
- Race conditions (unsynchronized access to shared data)
- Deadlocks (circular wait for locks)
- Priority inversion (low-priority thread blocks high-priority)

TOOLS OVERVIEW:
---------------
1. Helgrind (Valgrind) - Thread error detector
2. DRD (Valgrind) - Data race detector
3. ThreadSanitizer (TSan) - Fast race detector
4. perf - Thread scheduling and contention analysis
5. Intel VTune - Advanced threading analysis
6. mutrace - Mutex contention profiler

================================================================================
1. HELGRIND - THREAD ERROR DETECTOR
================================================================================

THEORY:
Helgrind detects:
- Data races (unsynchronized access to shared memory)
- Lock ordering problems (potential deadlocks)
- Misuse of pthreads API

INSTALLATION:
Included with Valgrind (see valgrind memory profiling guide)

PROCESS:
--------
valgrind --tool=helgrind ./your_threaded_app

Options:
--log-file=helgrind.log          : Save to file
--track-lockorders=yes           : Detect lock ordering issues
--conflict-cache-size=10000000   : Larger cache for complex programs

HFT C++ EXAMPLE (Race Condition):
----------------------------------

// race_condition.cpp
#include <iostream>
#include <thread>
#include <vector>
#include <atomic>

// BUGGY VERSION - Race condition
class OrderCounter_Buggy {
    uint64_t count = 0;  // NOT atomic - race condition!

public:
    void incrementOrders(int num_orders) {
        for (int i = 0; i < num_orders; ++i) {
            count++;  // RACE CONDITION: Non-atomic read-modify-write
        }
    }

    uint64_t getCount() const { return count; }
};

// FIXED VERSION - Using atomic
class OrderCounter_Fixed {
    std::atomic<uint64_t> count{0};

public:
    void incrementOrders(int num_orders) {
        for (int i = 0; i < num_orders; ++i) {
            count.fetch_add(1, std::memory_order_relaxed);
        }
    }

    uint64_t getCount() const {
        return count.load(std::memory_order_relaxed);
    }
};

void testBuggy() {
    OrderCounter_Buggy counter;
    const int num_threads = 4;
    const int orders_per_thread = 100000;

    std::vector<std::thread> threads;

    for (int i = 0; i < num_threads; ++i) {
        threads.emplace_back([&counter, orders_per_thread]() {
            counter.incrementOrders(orders_per_thread);
        });
    }

    for (auto& t : threads) {
        t.join();
    }

    uint64_t expected = num_threads * orders_per_thread;
    uint64_t actual = counter.getCount();

    std::cout << "Buggy version - Expected: " << expected
              << ", Actual: " << actual
              << ", Lost: " << (expected - actual) << std::endl;
}

void testFixed() {
    OrderCounter_Fixed counter;
    const int num_threads = 4;
    const int orders_per_thread = 100000;

    std::vector<std::thread> threads;

    for (int i = 0; i < num_threads; ++i) {
        threads.emplace_back([&counter, orders_per_thread]() {
            counter.incrementOrders(orders_per_thread);
        });
    }

    for (auto& t : threads) {
        t.join();
    }

    uint64_t expected = num_threads * orders_per_thread;
    uint64_t actual = counter.getCount();

    std::cout << "Fixed version - Expected: " << expected
              << ", Actual: " << actual << std::endl;
}

int main() {
    std::cout << "Testing race condition...\n";
    testBuggy();
    testFixed();
    return 0;
}

WORKFLOW:
---------
# Compile with debug symbols and threading
g++ -g -O0 -std=c++17 -pthread race_condition.cpp -o race_test

# Run with Helgrind (will detect race in buggy version)
valgrind --tool=helgrind ./race_test

INTERPRETING HELGRIND OUTPUT:
------------------------------
==12345== Possible data race during write of size 8 at 0x5000001234
==12345== Locks held: none
==12345==    at 0x4012AB: OrderCounter_Buggy::incrementOrders(int) (race_condition.cpp:14)
==12345==    by 0x401567: testBuggy()::{lambda()#1}::operator()() (race_condition.cpp:39)
==12345==    by 0x...: std::thread::_Invoker<...>::_M_invoke(...)
==12345==
==12345== This conflicts with a previous write of size 8 by thread #2
==12345== Locks held: none
==12345==    at 0x4012AB: OrderCounter_Buggy::incrementOrders(int) (race_condition.cpp:14)

Action: Replace non-atomic increment with atomic operation.

================================================================================
2. THREAD SANITIZER (TSan) - FAST RACE DETECTOR
================================================================================

THEORY:
TSan is compiler-based (GCC/Clang) with much lower overhead than Helgrind.
Overhead: 5-15x (vs Helgrind's 20-50x)

PROCESS:
--------
# Compile with TSan
g++ -fsanitize=thread -g -O2 -std=c++17 -pthread your_program.cpp -o your_program

# Run (TSan reports races immediately)
./your_program

# Control TSan behavior
export TSAN_OPTIONS="halt_on_error=0:log_path=tsan.log:second_deadlock_stack=1"
./your_program

HFT C++ EXAMPLE (False Sharing):
---------------------------------

// false_sharing.cpp
#include <iostream>
#include <thread>
#include <vector>
#include <chrono>

// BAD: False sharing - counters share cache line
struct CountersPacked {
    uint64_t counter1;  // Same cache line
    uint64_t counter2;  // Same cache line
};

// GOOD: Padded to separate cache lines
struct alignas(64) CountersAligned {
    uint64_t counter1;
    char padding[64 - sizeof(uint64_t)];  // Force separate cache lines
};

void testFalseSharing() {
    CountersPacked counters{0, 0};

    auto start = std::chrono::high_resolution_clock::now();

    std::thread t1([&]() {
        for (int i = 0; i < 100000000; ++i) {
            counters.counter1++;
        }
    });

    std::thread t2([&]() {
        for (int i = 0; i < 100000000; ++i) {
            counters.counter2++;
        }
    });

    t1.join();
    t2.join();

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "False sharing time: " << duration.count() << " ms\n";
}

void testAligned() {
    CountersAligned c1{0};
    CountersAligned c2{0};

    auto start = std::chrono::high_resolution_clock::now();

    std::thread t1([&]() {
        for (int i = 0; i < 100000000; ++i) {
            c1.counter1++;
        }
    });

    std::thread t2([&]() {
        for (int i = 0; i < 100000000; ++i) {
            c2.counter1++;
        }
    });

    t1.join();
    t2.join();

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "Aligned time: " << duration.count() << " ms\n";
}

int main() {
    testFalseSharing();
    testAligned();
    return 0;
}

WORKFLOW:
---------
# Compile with TSan
g++ -fsanitize=thread -g -O2 -std=c++17 -pthread false_sharing.cpp -o false_test

# Run
./false_test

# TSan won't catch false sharing directly, but you'll see performance difference
# For false sharing detection, use perf c2c:
sudo perf c2c record ./false_test
sudo perf c2c report

TSAN OPTIONS:
-------------
export TSAN_OPTIONS="option1=value:option2=value"

Common options:
halt_on_error=0              : Continue after first race
log_path=tsan.log            : Write to file
second_deadlock_stack=1      : Show both stacks in deadlock
history_size=7               : Stack trace history (2-7)
report_bugs=1                : Report all bug types

================================================================================
3. PERF FOR THREAD PROFILING
================================================================================

THEORY:
Use perf to analyze:
- Context switches (expensive, cache-destroying events)
- CPU migrations (thread moved between cores)
- Scheduler latency (time waiting to run)

PROCESS:
--------
# Monitor context switches
perf stat -e context-switches,cpu-migrations ./your_threaded_app

# Record scheduling events
sudo perf sched record ./your_threaded_app

# Analyze scheduling latency
sudo perf sched latency

# Show scheduling map (which thread ran when)
sudo perf sched map

# Timechart visualization
sudo perf sched timehist

HFT C++ EXAMPLE (Context Switch Analysis):
-------------------------------------------

// context_switches.cpp
#include <iostream>
#include <thread>
#include <vector>
#include <mutex>
#include <chrono>

std::mutex mtx;
int shared_counter = 0;

// BAD: Lock contention causes context switches
void workerWithContention(int iterations) {
    for (int i = 0; i < iterations; ++i) {
        std::lock_guard<std::mutex> lock(mtx);  // Contention!
        shared_counter++;
    }
}

// GOOD: Lock-free atomic
std::atomic<int> atomic_counter{0};

void workerLockFree(int iterations) {
    for (int i = 0; i < iterations; ++i) {
        atomic_counter.fetch_add(1, std::memory_order_relaxed);
    }
}

void testContention() {
    shared_counter = 0;
    std::vector<std::thread> threads;

    auto start = std::chrono::high_resolution_clock::now();

    for (int i = 0; i < 4; ++i) {
        threads.emplace_back(workerWithContention, 100000);
    }

    for (auto& t : threads) {
        t.join();
    }

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "With contention: " << duration.count() << " ms, "
              << "counter: " << shared_counter << std::endl;
}

void testLockFree() {
    atomic_counter = 0;
    std::vector<std::thread> threads;

    auto start = std::chrono::high_resolution_clock::now();

    for (int i = 0; i < 4; ++i) {
        threads.emplace_back(workerLockFree, 100000);
    }

    for (auto& t : threads) {
        t.join();
    }

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "Lock-free: " << duration.count() << " ms, "
              << "counter: " << atomic_counter.load() << std::endl;
}

int main() {
    testContention();
    testLockFree();
    return 0;
}

WORKFLOW:
---------
# Compile
g++ -O2 -g -std=c++17 -pthread context_switches.cpp -o ctx_test

# Count context switches
perf stat -e context-switches,cpu-migrations,cache-misses ./ctx_test

# Detailed scheduling analysis
sudo perf sched record ./ctx_test
sudo perf sched latency

EXPECTED RESULTS:
With contention: Many context-switches (thousands)
Lock-free: Very few context-switches

================================================================================
4. MUTRACE - MUTEX CONTENTION PROFILER
================================================================================

THEORY:
mutrace specifically profiles mutex usage:
- Lock contention
- Time spent waiting for locks
- Number of lock/unlock operations

INSTALLATION:
sudo apt-get install mutrace

PROCESS:
--------
mutrace ./your_threaded_app

HFT C++ EXAMPLE:
----------------
# Using same context_switches.cpp example

# Compile
g++ -O2 -g -std=c++17 -pthread context_switches.cpp -o ctx_test

# Run with mutrace
mutrace ./ctx_test

OUTPUT INTERPRETATION:
----------------------
mutrace: 1.2ms locked (45.2%)
mutrace: Showing statistics for process...

Mutex #1 (0x60abcd):
  Locked: 400000 times
  Contended: 125000 times (31.25%)
  Total wait: 1.2ms
  Average wait: 9.6us

Action: High contention (31%) - consider lock-free alternative

================================================================================
5. CPU PINNING & AFFINITY
================================================================================

THEORY:
Pin threads to specific CPU cores to:
- Reduce context switches
- Improve cache locality
- Ensure deterministic performance

HFT C++ EXAMPLE (CPU Pinning):
-------------------------------

// cpu_pinning.cpp
#include <iostream>
#include <thread>
#include <vector>
#include <pthread.h>
#include <sched.h>

void pinThreadToCore(int core_id) {
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    CPU_SET(core_id, &cpuset);

    pthread_t thread = pthread_self();
    int result = pthread_setaffinity_np(thread, sizeof(cpu_set_t), &cpuset);

    if (result != 0) {
        std::cerr << "Failed to pin thread to core " << core_id << std::endl;
    } else {
        std::cout << "Thread pinned to core " << core_id << std::endl;
    }
}

void worker(int core_id, int iterations) {
    pinThreadToCore(core_id);

    volatile uint64_t sum = 0;
    for (int i = 0; i < iterations; ++i) {
        sum += i;
    }

    std::cout << "Core " << core_id << " completed\n";
}

int main() {
    const int num_threads = 4;
    const int iterations = 100000000;

    std::vector<std::thread> threads;

    for (int i = 0; i < num_threads; ++i) {
        threads.emplace_back(worker, i, iterations);
    }

    for (auto& t : threads) {
        t.join();
    }

    return 0;
}

WORKFLOW:
---------
# Compile
g++ -O2 -g -std=c++17 -pthread cpu_pinning.cpp -o pin_test

# Monitor CPU migrations (should be 0 with pinning)
perf stat -e cpu-migrations ./pin_test

# Compare with unpinned version:
# Comment out pinThreadToCore() call and recompile
# Should see many more CPU migrations without pinning

================================================================================
6. LOCK-FREE PROGRAMMING
================================================================================

THEORY:
Lock-free data structures use atomic operations instead of locks.
Benefits:
- No context switches
- Better scalability
- Predictable latency

Common patterns:
- Compare-and-swap (CAS) loops
- Memory ordering (acquire/release semantics)
- ABA problem mitigation

HFT C++ EXAMPLE (Lock-Free Queue):
-----------------------------------

// lockfree_queue.cpp
#include <iostream>
#include <atomic>
#include <thread>
#include <vector>
#include <chrono>

template <typename T, size_t Size>
class LockFreeQueue {
    struct alignas(64) Slot {
        std::atomic<size_t> seq;
        T data;
    };

    Slot buffer[Size];
    alignas(64) std::atomic<size_t> enqueue_pos{0};
    alignas(64) std::atomic<size_t> dequeue_pos{0};

public:
    LockFreeQueue() {
        for (size_t i = 0; i < Size; ++i) {
            buffer[i].seq.store(i, std::memory_order_relaxed);
        }
    }

    bool enqueue(const T& data) {
        size_t pos = enqueue_pos.load(std::memory_order_relaxed);

        for (;;) {
            Slot& slot = buffer[pos % Size];
            size_t seq = slot.seq.load(std::memory_order_acquire);
            intptr_t diff = static_cast<intptr_t>(seq) - static_cast<intptr_t>(pos);

            if (diff == 0) {
                if (enqueue_pos.compare_exchange_weak(pos, pos + 1,
                                                       std::memory_order_relaxed)) {
                    slot.data = data;
                    slot.seq.store(pos + 1, std::memory_order_release);
                    return true;
                }
            } else if (diff < 0) {
                return false;  // Queue full
            } else {
                pos = enqueue_pos.load(std::memory_order_relaxed);
            }
        }
    }

    bool dequeue(T& data) {
        size_t pos = dequeue_pos.load(std::memory_order_relaxed);

        for (;;) {
            Slot& slot = buffer[pos % Size];
            size_t seq = slot.seq.load(std::memory_order_acquire);
            intptr_t diff = static_cast<intptr_t>(seq) - static_cast<intptr_t>(pos + 1);

            if (diff == 0) {
                if (dequeue_pos.compare_exchange_weak(pos, pos + 1,
                                                       std::memory_order_relaxed)) {
                    data = slot.data;
                    slot.seq.store(pos + Size, std::memory_order_release);
                    return true;
                }
            } else if (diff < 0) {
                return false;  // Queue empty
            } else {
                pos = dequeue_pos.load(std::memory_order_relaxed);
            }
        }
    }
};

// Test
void producer(LockFreeQueue<int, 1024>& queue, int count) {
    for (int i = 0; i < count; ++i) {
        while (!queue.enqueue(i)) {
            // Spin until space available
        }
    }
}

void consumer(LockFreeQueue<int, 1024>& queue, int count) {
    int data;
    for (int i = 0; i < count; ++i) {
        while (!queue.dequeue(data)) {
            // Spin until data available
        }
    }
}

int main() {
    LockFreeQueue<int, 1024> queue;
    const int messages = 1000000;

    auto start = std::chrono::high_resolution_clock::now();

    std::thread prod(producer, std::ref(queue), messages);
    std::thread cons(consumer, std::ref(queue), messages);

    prod.join();
    cons.join();

    auto end = std::chrono::high_resolution_clock::now();
    auto duration = std::chrono::duration_cast<std::chrono::milliseconds>(end - start);

    std::cout << "Processed " << messages << " messages in "
              << duration.count() << " ms\n";
    std::cout << "Throughput: " << (messages * 1000.0 / duration.count())
              << " msg/sec\n";

    return 0;
}

WORKFLOW:
---------
# Compile
g++ -O3 -g -std=c++17 -pthread lockfree_queue.cpp -o lockfree_test

# Profile with TSan to check correctness
g++ -fsanitize=thread -O2 -g -std=c++17 -pthread lockfree_queue.cpp -o lockfree_tsan
./lockfree_tsan

# Benchmark
./lockfree_test

# Monitor context switches (should be minimal)
perf stat -e context-switches ./lockfree_test

================================================================================
BEST PRACTICES FOR HFT THREADING
================================================================================

1. MINIMIZE LOCKING
   - Use lock-free data structures where possible
   - Use read-copy-update (RCU) for read-heavy workloads
   - Prefer atomics over mutexes for simple operations

2. CPU PINNING
   - Pin critical threads to dedicated cores
   - Avoid hyperthreading for latency-sensitive threads
   - Use NUMA-aware allocation

3. AVOID FALSE SHARING
   - Align data to cache lines (64 bytes)
   - Pad structures accessed by different threads
   - Use tools like perf c2c to detect

4. THREAD POOLING
   - Pre-create threads, don't spawn during trading
   - Use dedicated threads for specific tasks
   - Avoid thread creation/destruction overhead

5. SYNCHRONIZATION
   - Use memory_order_relaxed where possible
   - Understand acquire/release semantics
   - Profile synchronization overhead

6. TESTING
   - Use TSan in development
   - Stress test with realistic concurrency
   - Profile under load with perf
