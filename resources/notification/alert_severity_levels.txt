================================================================================
                   ALERT SEVERITY LEVELS - P1/P2/P3/P4
================================================================================

VERSION: 2.1.0
LAST UPDATED: 2025-11-25
STATUS: PRODUCTION
MAINTAINER: Infrastructure Team

================================================================================
                              TABLE OF CONTENTS
================================================================================

1. Overview - Severity Classification
2. P1 - Critical Alerts
3. P2 - High Priority Alerts
4. P3 - Medium Priority Alerts
5. P4 - Low Priority Alerts
6. Severity Assignment Guidelines
7. Real-World Examples
8. Response Time SLAs
9. Escalation Matrix
10. Severity Downgrade/Upgrade
11. Metrics & Reporting
12. Implementation in Code

================================================================================
                  1. OVERVIEW - SEVERITY CLASSIFICATION
================================================================================

The HFT notification system uses a 4-level severity classification:

P1 - CRITICAL    (Red)     Business Impact: Severe
P2 - HIGH        (Orange)  Business Impact: Significant
P3 - MEDIUM      (Yellow)  Business Impact: Moderate
P4 - LOW         (Green)   Business Impact: Minimal

SEVERITY SELECTION CRITERIA:

1. Business Impact
   - Revenue loss
   - Regulatory exposure
   - Reputational damage
   - Customer impact

2. System Impact
   - Complete outage vs. degradation
   - Affected components
   - User impact percentage

3. Time Sensitivity
   - Immediate action required
   - Can wait for business hours
   - Informational only

4. Data Integrity
   - Data loss risk
   - Corruption potential
   - Recovery difficulty

DECISION MATRIX:

                    Critical Function    Important Function    Nice-to-Have
Complete Outage     P1                   P1 or P2              P2 or P3
Major Degradation   P1 or P2             P2                    P3
Minor Degradation   P2 or P3             P3                    P4
No Impact           -                    -                     P4

================================================================================
                        2. P1 - CRITICAL ALERTS
================================================================================

DEFINITION:

System down or major functionality unavailable affecting business operations.
Requires immediate attention 24/7. Automated escalation if not acknowledged.

SEVERITY CRITERIA:

1. Trading Operations Halted
   - Cannot execute trades
   - Order routing down
   - Risk management unavailable

2. Data Corruption or Loss
   - Trade data corrupted
   - Position data inconsistent
   - Audit trail compromised

3. Security Breach
   - Unauthorized access detected
   - Data exfiltration
   - System compromise

4. Regulatory Violation Risk
   - Missing required trades
   - Failing to meet SLA
   - Compliance system down

5. Complete System Outage
   - Primary system unreachable
   - All users affected
   - No workaround available

6. Financial Impact
   - Active revenue loss > $10,000/hour
   - Risk of significant financial loss
   - Margin call imminent

EXAMPLES:

Trading System:
- Trading engine crashed
- All order execution failing
- Cannot connect to exchange
- Risk management system down
- Position tracking unavailable

Market Data:
- All market data feeds down
- Critical exchange disconnected
- Cannot receive price updates
- Quote generation failed

Infrastructure:
- Database server down
- Primary data center offline
- Network completely unavailable
- Critical service unreachable

Security:
- Unauthorized access detected
- Malware/ransomware attack
- Data breach confirmed
- DDoS attack in progress

RESPONSE REQUIREMENTS:

Response Time: 5 minutes (acknowledge), 30 minutes (initial resolution)
Notification: PagerDuty + SMS + Phone + Slack + Email
Recipients: On-call + Team Lead + VP Engineering + Manager
Escalation: Auto-escalate every 10 minutes
Status Updates: Every 15 minutes
Documentation: Detailed incident report required
Post-Mortem: Required within 72 hours

NOTIFICATION CHANNELS (Parallel):

1. PagerDuty: Create P1 incident
2. SMS: To on-call engineer
3. Phone Call: If no acknowledgment in 3 minutes
4. Slack: #hft-critical channel
5. Email: For record-keeping

BYPASS RULES:

- No rate limiting
- No deduplication window
- No quiet hours
- No user preferences (everyone gets notified)

C++ CLASSIFICATION CODE:

bool is_p1_critical(const Alert& alert) {
    // Trading operations
    if (alert.source == "execution_engine" &&
        alert.tags.contains("complete_failure")) {
        return true;
    }

    // Data corruption
    if (alert.tags.contains("data_corruption") ||
        alert.tags.contains("data_loss")) {
        return true;
    }

    // Security
    if (alert.tags.contains("security_breach") ||
        alert.tags.contains("unauthorized_access")) {
        return true;
    }

    // System outage
    if (alert.context.find("uptime_percentage") != alert.context.end()) {
        double uptime = std::stod(alert.context.at("uptime_percentage"));
        if (uptime < 50.0) {
            return true;
        }
    }

    // Financial impact
    if (alert.context.find("revenue_loss_per_hour") != alert.context.end()) {
        double loss = std::stod(alert.context.at("revenue_loss_per_hour"));
        if (loss > 10000.0) {
            return true;
        }
    }

    // Latency beyond critical threshold
    if (alert.context.find("latency_ms") != alert.context.end()) {
        double latency = std::stod(alert.context.at("latency_ms"));
        if (latency > 100.0 && alert.source == "execution_engine") {
            return true;
        }
    }

    return false;
}

================================================================================
                      3. P2 - HIGH PRIORITY ALERTS
================================================================================

DEFINITION:

Degraded system performance or important functionality impaired.
Requires prompt attention during business hours or on-call after hours.

SEVERITY CRITERIA:

1. Significant Performance Degradation
   - >20% throughput reduction
   - Latency increase >50%
   - Error rate >5%

2. Important Feature Unavailable
   - Secondary trading strategy down
   - Reporting system unavailable
   - Monitoring partially down

3. Risk Limit Breached
   - Position limits exceeded
   - VaR limits breached
   - Concentration limits hit

4. High Error Rates
   - >5% order rejection rate
   - Increased system errors
   - Multiple component failures

5. Capacity Issues
   - CPU >90% sustained
   - Memory >90%
   - Disk space <10%
   - Network saturation

EXAMPLES:

Trading System:
- One trading strategy failing
- 30% of orders being rejected
- Latency increased to 50ms (normally 5ms)
- Backup trading engine down
- Secondary risk checks failing

Market Data:
- One market data feed down (others working)
- Delayed data (>5 seconds)
- Missing symbols (subset)
- Incorrect prices (detected)

Infrastructure:
- Backup server down
- Secondary database unavailable
- Cache server crashed
- Load balancer issue

Application:
- Memory leak detected
- High CPU utilization
- Disk space warning
- Connection pool exhausted

RESPONSE REQUIREMENTS:

Response Time: 15 minutes (acknowledge), 2 hours (resolution)
Notification: PagerDuty + Slack + Email
Recipients: On-call + Team Lead + Operations
Escalation: Auto-escalate after 30 minutes
Status Updates: Every 30 minutes
Documentation: Incident summary required
Post-Mortem: Optional (recommended for recurring issues)

NOTIFICATION CHANNELS (Parallel):

1. PagerDuty: Create P2 incident
2. Slack: #hft-critical channel
3. Email: To relevant team

C++ CLASSIFICATION CODE:

bool is_p2_high(const Alert& alert) {
    // Performance degradation
    if (alert.context.find("throughput_reduction_pct") != alert.context.end()) {
        double reduction = std::stod(alert.context.at("throughput_reduction_pct"));
        if (reduction > 20.0) {
            return true;
        }
    }

    // Latency increase
    if (alert.context.find("latency_ms") != alert.context.end()) {
        double latency = std::stod(alert.context.at("latency_ms"));
        double baseline = std::stod(alert.context.at("baseline_latency_ms"));
        if (latency > baseline * 1.5 && latency < 100.0) {
            return true;
        }
    }

    // Error rate
    if (alert.context.find("error_rate_pct") != alert.context.end()) {
        double error_rate = std::stod(alert.context.at("error_rate_pct"));
        if (error_rate > 5.0) {
            return true;
        }
    }

    // Risk limits
    if (alert.tags.contains("risk_limit_breach") ||
        alert.tags.contains("position_limit_exceeded")) {
        return true;
    }

    // Resource usage
    if (alert.context.find("cpu_usage_pct") != alert.context.end()) {
        double cpu = std::stod(alert.context.at("cpu_usage_pct"));
        if (cpu > 90.0) {
            return true;
        }
    }

    return false;
}

================================================================================
                     4. P3 - MEDIUM PRIORITY ALERTS
================================================================================

DEFINITION:

Minor issues or warnings that should be addressed during business hours.
No immediate impact but could escalate if not resolved.

SEVERITY CRITERIA:

1. Minor Performance Issues
   - <20% throughput reduction
   - Latency increase <50%
   - Error rate 1-5%

2. Non-Critical Warnings
   - Configuration drift
   - Certificate expiring (>7 days)
   - Disk space warning (>20%)
   - Resource usage warning

3. Scheduled Maintenance
   - Planned deployments
   - System updates
   - Configuration changes

4. Monitoring Alerts
   - Health check failures (single)
   - Backup job warnings
   - Log processing delays

EXAMPLES:

Trading System:
- Single order rejected
- Minor latency increase (10ms -> 15ms)
- Non-critical algorithm warning
- Test environment issue

Market Data:
- Delayed data from one source (<5 sec)
- Missing non-critical symbols
- Cache miss rate high

Infrastructure:
- Disk space warning (25% remaining)
- Certificate expires in 30 days
- Backup job took longer than usual
- Log volume high

Application:
- Configuration file changed
- User login failed (single)
- API rate limit warning
- Cache efficiency low

RESPONSE REQUIREMENTS:

Response Time: 1 hour (acknowledge), 24 hours (resolution)
Notification: Slack + Email
Recipients: Development Team + Operations
Escalation: Manual only
Status Updates: Daily
Documentation: Optional
Post-Mortem: Not required

NOTIFICATION CHANNELS (Sequential):

1. Slack: #hft-alerts channel
2. Email: To team (if not acked in 1 hour)

C++ CLASSIFICATION CODE:

bool is_p3_medium(const Alert& alert) {
    // Minor performance degradation
    if (alert.context.find("throughput_reduction_pct") != alert.context.end()) {
        double reduction = std::stod(alert.context.at("throughput_reduction_pct"));
        if (reduction > 5.0 && reduction <= 20.0) {
            return true;
        }
    }

    // Moderate latency
    if (alert.context.find("latency_ms") != alert.context.end()) {
        double latency = std::stod(alert.context.at("latency_ms"));
        double baseline = std::stod(alert.context.at("baseline_latency_ms"));
        if (latency > baseline * 1.2 && latency <= baseline * 1.5) {
            return true;
        }
    }

    // Low error rate
    if (alert.context.find("error_rate_pct") != alert.context.end()) {
        double error_rate = std::stod(alert.context.at("error_rate_pct"));
        if (error_rate > 1.0 && error_rate <= 5.0) {
            return true;
        }
    }

    // Warnings
    if (alert.tags.contains("warning") ||
        alert.tags.contains("configuration_drift")) {
        return true;
    }

    // Resource warnings
    if (alert.context.find("disk_space_pct") != alert.context.end()) {
        double disk = std::stod(alert.context.at("disk_space_pct"));
        if (disk < 30.0 && disk >= 20.0) {
            return true;
        }
    }

    return false;
}

================================================================================
                      5. P4 - LOW PRIORITY ALERTS
================================================================================

DEFINITION:

Informational messages, daily reports, and routine notifications.
No action required unless pattern develops.

SEVERITY CRITERIA:

1. Informational Messages
   - System startup/shutdown
   - Configuration reloaded
   - Backup completed
   - Routine tasks finished

2. Daily/Weekly Reports
   - Trading summary
   - Performance metrics
   - System health report
   - Audit logs

3. Compliance Notifications
   - Trade reports filed
   - Regulatory submissions
   - Audit trail generated

4. Low-Level Monitoring
   - Single health check warning
   - Cache statistics
   - Queue depth info
   - Connection pool stats

EXAMPLES:

Trading System:
- System started successfully
- Daily trading summary
- Order statistics
- Algorithm performance report

Market Data:
- Data quality report
- Symbol list updated
- Feed statistics
- Latency histogram

Infrastructure:
- Backup completed successfully
- Log rotation completed
- Certificate renewed
- Configuration updated

Application:
- User logged in
- API usage statistics
- Cache statistics
- Session information

RESPONSE REQUIREMENTS:

Response Time: Best effort (no SLA)
Notification: Email only
Recipients: Team (broadcast)
Escalation: None
Status Updates: None
Documentation: Not required
Post-Mortem: Not required

NOTIFICATION CHANNELS:

1. Email: Daily digest

C++ CLASSIFICATION CODE:

bool is_p4_low(const Alert& alert) {
    // Informational tags
    if (alert.tags.contains("informational") ||
        alert.tags.contains("report") ||
        alert.tags.contains("summary")) {
        return true;
    }

    // Routine operations
    if (alert.title.find("completed successfully") != std::string::npos ||
        alert.title.find("started") != std::string::npos) {
        return true;
    }

    // Very minor issues
    if (alert.context.find("error_rate_pct") != alert.context.end()) {
        double error_rate = std::stod(alert.context.at("error_rate_pct"));
        if (error_rate <= 1.0) {
            return true;
        }
    }

    return false;
}

================================================================================
                  6. SEVERITY ASSIGNMENT GUIDELINES
================================================================================

AUTOMATIC SEVERITY ASSIGNMENT:

enum class AlertSeverity {
    P1,  // Critical
    P2,  // High
    P3,  // Medium
    P4   // Low
};

class SeverityClassifier {
public:
    AlertSeverity classify_alert(const Alert& alert) {
        // Check P1 (most critical first)
        if (is_p1_critical(alert)) {
            return AlertSeverity::P1;
        }

        // Check P2
        if (is_p2_high(alert)) {
            return AlertSeverity::P2;
        }

        // Check P3
        if (is_p3_medium(alert)) {
            return AlertSeverity::P3;
        }

        // Default to P4
        return AlertSeverity::P4;
    }

private:
    bool is_p1_critical(const Alert& alert) {
        // Check critical conditions
        return check_outage(alert) ||
               check_data_corruption(alert) ||
               check_security_breach(alert) ||
               check_financial_impact(alert);
    }

    bool check_outage(const Alert& alert) {
        if (alert.source == "execution_engine") {
            auto it = alert.context.find("service_status");
            if (it != alert.context.end() && it->second == "down") {
                return true;
            }
        }
        return false;
    }

    bool check_data_corruption(const Alert& alert) {
        return alert.tags.contains("data_corruption") ||
               alert.tags.contains("inconsistent_data");
    }

    bool check_security_breach(const Alert& alert) {
        return alert.tags.contains("security_breach") ||
               alert.tags.contains("unauthorized_access") ||
               alert.tags.contains("intrusion_detected");
    }

    bool check_financial_impact(const Alert& alert) {
        auto it = alert.context.find("estimated_loss_usd");
        if (it != alert.context.end()) {
            double loss = std::stod(it->second);
            return loss > 10000.0;
        }
        return false;
    }

    bool is_p2_high(const Alert& alert) {
        return check_degradation(alert) ||
               check_error_rate(alert) ||
               check_resource_usage(alert) ||
               check_risk_limits(alert);
    }

    bool check_degradation(const Alert& alert) {
        auto it = alert.context.find("performance_degradation_pct");
        if (it != alert.context.end()) {
            double degradation = std::stod(it->second);
            return degradation > 20.0;
        }
        return false;
    }

    bool check_error_rate(const Alert& alert) {
        auto it = alert.context.find("error_rate_pct");
        if (it != alert.context.end()) {
            double error_rate = std::stod(it->second);
            return error_rate > 5.0 && error_rate <= 50.0;
        }
        return false;
    }

    bool check_resource_usage(const Alert& alert) {
        // Check CPU
        auto cpu_it = alert.context.find("cpu_usage_pct");
        if (cpu_it != alert.context.end()) {
            double cpu = std::stod(cpu_it->second);
            if (cpu > 90.0) return true;
        }

        // Check memory
        auto mem_it = alert.context.find("memory_usage_pct");
        if (mem_it != alert.context.end()) {
            double memory = std::stod(mem_it->second);
            if (memory > 90.0) return true;
        }

        // Check disk
        auto disk_it = alert.context.find("disk_usage_pct");
        if (disk_it != alert.context.end()) {
            double disk = std::stod(disk_it->second);
            if (disk > 90.0) return true;
        }

        return false;
    }

    bool check_risk_limits(const Alert& alert) {
        return alert.tags.contains("risk_limit_breach") ||
               alert.tags.contains("position_limit_exceeded") ||
               alert.tags.contains("var_limit_breach");
    }

    bool is_p3_medium(const Alert& alert) {
        // Minor issues that don't qualify for P1 or P2
        return !is_p1_critical(alert) &&
               !is_p2_high(alert) &&
               !is_informational(alert);
    }

    bool is_informational(const Alert& alert) {
        return alert.tags.contains("info") ||
               alert.tags.contains("report") ||
               alert.tags.contains("summary") ||
               alert.title.find("completed") != std::string::npos;
    }
};

USAGE EXAMPLE:

#include "severity_classifier.hpp"

void send_alert(const std::string& title,
                const std::string& message,
                const std::string& source,
                const std::map<std::string, std::string>& context) {

    Alert alert;
    alert.id = generate_alert_id();
    alert.title = title;
    alert.message = message;
    alert.source = source;
    alert.context = context;
    alert.timestamp = std::chrono::system_clock::now();

    // Automatically classify severity
    SeverityClassifier classifier;
    alert.severity = classifier.classify_alert(alert);

    // Send alert
    notification_manager.send_alert(alert);
}

// Example usage
send_alert(
    "High Order Rejection Rate",
    "Order rejection rate has exceeded 10%",
    "execution_engine",
    {
        {"error_rate_pct", "10.5"},
        {"total_orders", "1000"},
        {"rejected_orders", "105"}
    }
);
// This will be automatically classified as P2

================================================================================
                   7. RESPONSE TIME SLAs
================================================================================

SEVERITY  ACKNOWLEDGE   INITIAL RESPONSE   RESOLUTION     ESCALATION
------------------------------------------------------------------------
P1        5 minutes     15 minutes         4 hours        Every 10 min
P2        15 minutes    1 hour             24 hours       After 30 min
P3        1 hour        4 hours            3 days         Manual
P4        Best effort   Best effort        Best effort    None

BUSINESS HOURS vs AFTER HOURS:

                Business Hours (9am-6pm)    After Hours (6pm-9am)
P1              5 min / 4 hours             3 min / 2 hours (faster)
P2              15 min / 24 hours           15 min / 8 hours
P3              1 hour / 3 days             Queue for next business day
P4              Best effort                 Queue for next business day

METRICS TRACKING:

- Mean Time to Acknowledge (MTTA)
- Mean Time to Resolve (MTTR)
- SLA Compliance Rate
- Escalation Rate
- False Positive Rate

TARGET SLA COMPLIANCE: 95%

================================================================================
                     END OF ALERT SEVERITY LEVELS GUIDE
================================================================================
