================================================================================
DISASTER RECOVERY - DATABASE REPLICATION & FAILOVER
High-Frequency Trading System
Synchronous Replication | Zero Data Loss | Auto-Failover
================================================================================

TABLE OF CONTENTS
-----------------
1. Database HA Architecture
2. PostgreSQL Streaming Replication
3. Synchronous vs Asynchronous Replication
4. Patroni Auto-Failover Configuration
5. pgBouncer Connection Pooling
6. Replication Monitoring
7. Failover Procedures
8. Failback Procedures
9. Split-Brain Prevention
10. Performance Tuning

================================================================================
1. DATABASE HA ARCHITECTURE
================================================================================

1.1 THREE-NODE CLUSTER TOPOLOGY
--------------------------------

┌──────────────────────────────────────────────────────────────────────┐
│             POSTGRESQL HIGH AVAILABILITY CLUSTER                     │
├──────────────────────────────────────────────────────────────────────┤
│                                                                      │
│                        Applications                                  │
│                             │                                        │
│                             ▼                                        │
│                     ┌───────────────┐                               │
│                     │  pgBouncer VIP│                               │
│                     │  10.0.1.100   │                               │
│                     └───────┬───────┘                               │
│                             │                                        │
│              ┌──────────────┼──────────────┐                        │
│              │              │              │                        │
│         ┌────▼────┐    ┌────▼────┐   ┌────▼────┐                  │
│         │ PRIMARY │    │STANDBY-1│   │STANDBY-2│                  │
│         │ (NYC)   │    │(Chicago)│   │(London) │                  │
│         │         │    │         │   │         │                  │
│         │ Role:   │    │ Role:   │   │ Role:   │                  │
│         │ Leader  │    │ Sync    │   │ Async   │                  │
│         │         │    │ Replica │   │ Replica │                  │
│         └────┬────┘    └────┬────┘   └────┬────┘                  │
│              │              │              │                        │
│              └──────────────┴──────────────┘                        │
│                             │                                        │
│                      ┌──────▼──────┐                                │
│                      │    etcd     │                                │
│                      │   Cluster   │                                │
│                      │(Consensus/  │                                │
│                      │  Leader     │                                │
│                      │  Election)  │                                │
│                      └─────────────┘                                │
│                                                                      │
│  Replication Flow:                                                  │
│  PRIMARY ─sync─▶ STANDBY-1 ─async─▶ STANDBY-2                      │
│                                                                      │
│  Failover Priority:                                                 │
│  1. STANDBY-1 (Synchronous, lowest lag)                            │
│  2. STANDBY-2 (Asynchronous, geographic diversity)                 │
└──────────────────────────────────────────────────────────────────────┘

1.2 REPLICATION METRICS
------------------------

┌────────────────┬───────────┬──────────┬────────────┬──────────────┐
│ Metric         │ Primary   │ Standby-1│ Standby-2  │ Target       │
├────────────────┼───────────┼──────────┼────────────┼──────────────┤
│ Lag (ms)       │ 0         │ <10ms    │ <100ms     │ <100ms       │
│ WAL Delay      │ 0         │ <1MB     │ <10MB      │ <50MB        │
│ Sync State     │ -         │ Sync     │ Async      │ As configured│
│ Connectivity   │ 99.99%    │ 99.99%   │ 99.9%      │ >99.9%       │
│ TPS Capacity   │ 100K      │ 100K     │ 100K       │ >50K         │
└────────────────┴───────────┴──────────┴────────────┴──────────────┘

================================================================================
2. POSTGRESQL STREAMING REPLICATION
================================================================================

2.1 PRIMARY CONFIGURATION
--------------------------

postgresql.conf (Primary Server):
```conf
# -----------------------------
# Connection Settings
# -----------------------------
listen_addresses = '*'
port = 5432
max_connections = 1000
superuser_reserved_connections = 10

# -----------------------------
# Memory Settings (64GB RAM server)
# -----------------------------
shared_buffers = 16GB
effective_cache_size = 48GB
maintenance_work_mem = 2GB
work_mem = 32MB
huge_pages = try
temp_buffers = 32MB

# -----------------------------
# WAL Settings
# -----------------------------
wal_level = replica  # or 'logical' for logical replication
fsync = on
synchronous_commit = remote_apply  # Wait for standby confirmation
full_page_writes = on
wal_compression = on
wal_log_hints = on  # Required for pg_rewind

# WAL Archiving
archive_mode = on
archive_command = 'test ! -f /wal_archive/%f && cp %p /wal_archive/%f'
archive_timeout = 300  # Archive every 5 minutes

# WAL Size
min_wal_size = 2GB
max_wal_size = 8GB
wal_keep_size = 4GB  # Keep for standby recovery

# -----------------------------
# Replication Settings
# -----------------------------
max_wal_senders = 10
max_replication_slots = 10
wal_sender_timeout = 60s

# Synchronous Replication
synchronous_standby_names = 'FIRST 1 (standby1, standby2)'
# This means: wait for at least 1 of the named standbys

# Alternative: Quorum-based
# synchronous_standby_names = 'ANY 2 (standby1, standby2, standby3)'

# -----------------------------
# Checkpoint Settings
# -----------------------------
checkpoint_timeout = 15min
checkpoint_completion_target = 0.9
checkpoint_warning = 30s

# -----------------------------
# Query Planning
# -----------------------------
random_page_cost = 1.1  # For SSD
effective_io_concurrency = 200
default_statistics_target = 100

# -----------------------------
# Parallelism
# -----------------------------
max_worker_processes = 8
max_parallel_workers_per_gather = 4
max_parallel_workers = 8
max_parallel_maintenance_workers = 4

# -----------------------------
# Logging
# -----------------------------
logging_collector = on
log_directory = 'pg_log'
log_filename = 'postgresql-%Y-%m-%d_%H%M%S.log'
log_rotation_age = 1d
log_rotation_size = 1GB
log_line_prefix = '%t [%p]: [%l-1] user=%u,db=%d,app=%a,client=%h '
log_checkpoints = on
log_connections = on
log_disconnections = on
log_duration = off
log_lock_waits = on
log_statement = 'ddl'
log_temp_files = 0
log_autovacuum_min_duration = 0
log_replication_commands = on

# -----------------------------
# Autovacuum
# -----------------------------
autovacuum = on
autovacuum_max_workers = 4
autovacuum_naptime = 30s
autovacuum_vacuum_threshold = 50
autovacuum_analyze_threshold = 50
autovacuum_vacuum_scale_factor = 0.05
autovacuum_analyze_scale_factor = 0.02
autovacuum_vacuum_cost_delay = 2ms

# -----------------------------
# Lock Management
# -----------------------------
deadlock_timeout = 1s
max_locks_per_transaction = 256

# -----------------------------
# Statement Timeout
# -----------------------------
statement_timeout = 30s  # Prevent long-running queries
```

pg_hba.conf (Primary Server):
```conf
# TYPE  DATABASE    USER            ADDRESS                 METHOD

# Local connections
local   all         postgres                                peer
local   all         all                                     peer

# Replication connections
host    replication replicator      10.0.1.0/24             scram-sha-256
host    replication replicator      10.0.2.0/24             scram-sha-256

# Application connections
host    trading     hft_user        10.0.1.0/24             scram-sha-256
host    trading     hft_user        10.0.2.0/24             scram-sha-256

# Monitoring
host    all         monitoring      10.0.0.0/16             scram-sha-256

# Reject all other
host    all         all             0.0.0.0/0               reject
```

2.2 STANDBY CONFIGURATION
--------------------------

postgresql.conf (Standby Server):
```conf
# Same as primary, plus:

# Standby-specific settings
hot_standby = on
hot_standby_feedback = on
wal_receiver_status_interval = 10s
wal_receiver_timeout = 60s
wal_retrieve_retry_interval = 5s
max_standby_streaming_delay = 30s
hot_standby_feedback = on
```

standby.signal (PostgreSQL 12+):
```
# This file indicates server should start in standby mode
# No content needed, just existence of file
```

postgresql.auto.conf (Standby - created by Patroni):
```conf
primary_conninfo = 'host=10.0.1.10 port=5432 user=replicator password=XXX application_name=standby1'
primary_slot_name = 'standby1_slot'
restore_command = 'cp /wal_archive/%f %p'
recovery_target_timeline = 'latest'
```

2.3 REPLICATION SLOT SETUP
----------------------------

On Primary, create replication slots:

```sql
-- Create replication slot for standby1
SELECT pg_create_physical_replication_slot('standby1_slot');

-- Create replication slot for standby2
SELECT pg_create_physical_replication_slot('standby2_slot');

-- View replication slots
SELECT slot_name, slot_type, active, restart_lsn, confirmed_flush_lsn
FROM pg_replication_slots;

-- Monitor slot lag
SELECT slot_name,
       pg_size_pretty(
           pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)
       ) AS replication_lag
FROM pg_replication_slots;
```

2.4 CREATE REPLICATION USER
-----------------------------

```sql
-- Create replication user
CREATE ROLE replicator WITH REPLICATION LOGIN PASSWORD 'SecureReplicationPassword';

-- Grant necessary permissions
GRANT CONNECT ON DATABASE trading TO replicator;

-- For monitoring
CREATE ROLE monitoring WITH LOGIN PASSWORD 'MonitoringPassword';
GRANT pg_monitor TO monitoring;
```

================================================================================
3. PATRONI AUTO-FAILOVER
================================================================================

3.1 PATRONI ARCHITECTURE
-------------------------

Patroni provides:
- Automatic leader election using etcd
- Health monitoring
- Automatic failover
- Configuration management
- REST API for operations

Components:
- Patroni agents (one per PostgreSQL node)
- etcd cluster (distributed consensus)
- HAProxy (connection routing based on Patroni state)

3.2 PATRONI CONFIGURATION
--------------------------

/etc/patroni/patroni.yml (Node 1 - NYC):
```yaml
scope: hft_trading
namespace: /db/
name: postgresql0

restapi:
  listen: 0.0.0.0:8008
  connect_address: 10.0.1.10:8008
  authentication:
    username: patroni
    password: SecurePatroniPassword

etcd:
  hosts: 10.0.0.10:2379,10.0.0.11:2379,10.0.0.12:2379

bootstrap:
  dcs:
    ttl: 30
    loop_wait: 10
    retry_timeout: 10
    maximum_lag_on_failover: 1048576  # 1MB
    master_start_timeout: 300
    synchronous_mode: true
    synchronous_mode_strict: true
    
    postgresql:
      use_pg_rewind: true
      use_slots: true
      
      parameters:
        # Copy relevant parameters from postgresql.conf above
        max_connections: 1000
        shared_buffers: '16GB'
        effective_cache_size: '48GB'
        maintenance_work_mem: '2GB'
        wal_level: replica
        hot_standby: on
        wal_log_hints: on
        max_wal_senders: 10
        max_replication_slots: 10
        wal_keep_size: '4GB'

      # Callbacks for custom actions
      callbacks:
        on_start: /opt/hft/scripts/patroni_on_start.sh
        on_stop: /opt/hft/scripts/patroni_on_stop.sh
        on_role_change: /opt/hft/scripts/patroni_role_change.sh

  initdb:
    - encoding: UTF8
    - data-checksums

  pg_hba:
    - host replication replicator 10.0.0.0/16 scram-sha-256
    - host all all 10.0.0.0/16 scram-sha-256
    - host all all 0.0.0.0/0 reject

  users:
    admin:
      password: AdminPassword
      options:
        - createrole
        - createdb
    replicator:
      password: ReplicationPassword
      options:
        - replication

postgresql:
  listen: 0.0.0.0:5432
  connect_address: 10.0.1.10:5432
  data_dir: /var/lib/postgresql/14/main
  bin_dir: /usr/lib/postgresql/14/bin
  pgpass: /tmp/pgpass0
  
  authentication:
    replication:
      username: replicator
      password: ReplicationPassword
    superuser:
      username: postgres
      password: PostgresPassword
    rewind:
      username: replicator
      password: ReplicationPassword
  
  parameters:
    unix_socket_directories: '/var/run/postgresql'
    
  create_replica_methods:
    - basebackup
  basebackup:
    max-rate: '100M'
    checkpoint: 'fast'

watchdog:
  mode: automatic
  device: /dev/watchdog
  safety_margin: 5

tags:
  nofailover: false
  noloadbalance: false
  clonefrom: false
  nosync: false
```

Similar configurations for Node 2 and Node 3, with different:
- `name`: postgresql1, postgresql2
- `connect_address`: 10.0.1.11, 10.0.1.12

3.3 ETCD CLUSTER SETUP
-----------------------

etcd provides distributed consensus for Patroni leader election.

/etc/etcd/etcd.conf (Node 1):
```yaml
name: 'etcd0'
data-dir: '/var/lib/etcd'
listen-client-urls: 'http://10.0.0.10:2379,http://localhost:2379'
advertise-client-urls: 'http://10.0.0.10:2379'
listen-peer-urls: 'http://10.0.0.10:2380'
initial-advertise-peer-urls: 'http://10.0.0.10:2380'
initial-cluster: 'etcd0=http://10.0.0.10:2380,etcd1=http://10.0.0.11:2380,etcd2=http://10.0.0.12:2380'
initial-cluster-token: 'hft-trading-cluster'
initial-cluster-state: 'new'
```

Verify etcd cluster:
```bash
# Check cluster health
etcdctl cluster-health

# Check members
etcdctl member list

# Check Patroni keys
etcdctl ls /db/hft_trading --recursive
```

3.4 HAPROXY INTEGRATION
------------------------

HAProxy routes connections based on Patroni state (leader/replica).

/etc/haproxy/haproxy.cfg:
```conf
global
    maxconn 1000
    log 127.0.0.1 local0

defaults
    log global
    mode tcp
    retries 2
    timeout client 30m
    timeout connect 4s
    timeout server 30m
    timeout check 5s

# PostgreSQL Primary (Read-Write)
listen postgres_primary
    bind *:5000
    option httpchk
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    
    server postgresql0 10.0.1.10:5432 maxconn 100 check port 8008
    server postgresql1 10.0.1.11:5432 maxconn 100 check port 8008
    server postgresql2 10.0.1.12:5432 maxconn 100 check port 8008

# PostgreSQL Replicas (Read-Only)
listen postgres_replicas
    bind *:5001
    balance roundrobin
    option httpchk GET /replica
    http-check expect status 200
    default-server inter 3s fall 3 rise 2 on-marked-down shutdown-sessions
    
    server postgresql0 10.0.1.10:5432 maxconn 100 check port 8008
    server postgresql1 10.0.1.11:5432 maxconn 100 check port 8008
    server postgresql2 10.0.1.12:5432 maxconn 100 check port 8008

# Patroni REST API Stats
listen patroni_stats
    bind *:7000
    option httpchk
    http-check expect status 200
    
    server patroni0 10.0.1.10:8008 check
    server patroni1 10.0.1.11:8008 check
    server patroni2 10.0.1.12:8008 check

# HAProxy Stats
listen stats
    bind *:7001
    stats enable
    stats uri /
    stats refresh 10s
    stats admin if TRUE
```

Application connection strings:
- Write (primary): postgresql://haproxy:5000/trading
- Read (replicas): postgresql://haproxy:5001/trading

================================================================================
4. REPLICATION MONITORING
================================================================================

4.1 MONITORING QUERIES
-----------------------

On Primary, check replication status:
```sql
-- View connected replicas
SELECT application_name,
       client_addr,
       state,
       sync_state,
       sent_lsn,
       write_lsn,
       flush_lsn,
       replay_lsn,
       pg_wal_lsn_diff(sent_lsn, replay_lsn) AS replication_lag_bytes,
       write_lag,
       flush_lag,
       replay_lag
FROM pg_stat_replication;

-- Check replication slots
SELECT slot_name,
       active,
       restart_lsn,
       pg_size_pretty(
           pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)
       ) AS retained_wal
FROM pg_replication_slots;
```

On Standby, check recovery status:
```sql
-- Check if in recovery (standby) mode
SELECT pg_is_in_recovery();

-- Check replay lag
SELECT EXTRACT(EPOCH FROM (now() - pg_last_xact_replay_timestamp())) AS lag_seconds;

-- Check last received LSN
SELECT pg_last_wal_receive_lsn(),
       pg_last_wal_replay_lsn(),
       pg_wal_lsn_diff(pg_last_wal_receive_lsn(), pg_last_wal_replay_lsn()) AS replay_lag_bytes;
```

4.2 MONITORING SCRIPT
----------------------

Script: /opt/hft/scripts/monitor_db_replication.sh

```bash
#!/bin/bash
# Monitor PostgreSQL Replication

PRIMARY="10.0.1.10"
STANDBY1="10.0.1.11"
STANDBY2="10.0.1.12"

LAG_THRESHOLD_SECONDS=5
LAG_THRESHOLD_BYTES=10485760  # 10MB

check_primary() {
    echo "=== PRIMARY STATUS ==="
    
    psql -h $PRIMARY -U monitoring -d postgres -c "
        SELECT application_name,
               client_addr,
               state,
               sync_state,
               pg_wal_lsn_diff(sent_lsn, replay_lsn) AS lag_bytes
        FROM pg_stat_replication;
    "
    
    # Count active replicas
    ACTIVE_REPLICAS=$(psql -h $PRIMARY -U monitoring -d postgres -t -c "
        SELECT COUNT(*) FROM pg_stat_replication WHERE state = 'streaming';
    " | tr -d ' ')
    
    echo "Active replicas: $ACTIVE_REPLICAS"
    
    if [ $ACTIVE_REPLICAS -lt 1 ]; then
        echo "ALERT: No active replicas!"
        /opt/hft/scripts/send_alert.sh "CRITICAL" "No active DB replicas"
    fi
}

check_standby() {
    STANDBY=$1
    NAME=$2
    
    echo "=== $NAME STATUS ==="
    
    # Check if in recovery
    IN_RECOVERY=$(psql -h $STANDBY -U monitoring -d postgres -t -c "
        SELECT pg_is_in_recovery();
    " | tr -d ' ')
    
    if [ "$IN_RECOVERY" != "t" ]; then
        echo "ERROR: $NAME is not in recovery mode (not a standby)!"
        return 1
    fi
    
    # Check replay lag
    LAG_SECONDS=$(psql -h $STANDBY -U monitoring -d postgres -t -c "
        SELECT EXTRACT(EPOCH FROM (NOW() - pg_last_xact_replay_timestamp()));
    " | tr -d ' ' | cut -d. -f1)
    
    echo "Replication lag: ${LAG_SECONDS}s"
    
    if [ ! -z "$LAG_SECONDS" ] && [ $LAG_SECONDS -gt $LAG_THRESHOLD_SECONDS ]; then
        echo "WARNING: Replication lag exceeds threshold (${LAG_SECONDS}s > ${LAG_THRESHOLD_SECONDS}s)"
        /opt/hft/scripts/send_alert.sh "WARNING" "$NAME replication lag: ${LAG_SECONDS}s"
    fi
}

check_patroni() {
    echo "=== PATRONI CLUSTER STATUS ==="
    patronictl -c /etc/patroni/patroni.yml list
}

# Main execution
main() {
    check_primary
    echo ""
    check_standby $STANDBY1 "STANDBY-1"
    echo ""
    check_standby $STANDBY2 "STANDBY-2"
    echo ""
    check_patroni
}

# Run continuously
while true; do
    main
    echo ""
    echo "Next check in 60 seconds..."
    sleep 60
done
```

================================================================================
5. FAILOVER PROCEDURES
================================================================================

5.1 AUTOMATIC FAILOVER (PATRONI)
----------------------------------

Patroni performs automatic failover when:
1. Primary becomes unreachable
2. Primary health check fails
3. Network partition isolates primary

Failover process (automatic):
```
1. Patroni detects primary failure (within 30s)
2. etcd leader election begins
3. Best replica selected (lowest lag, highest priority)
4. Selected replica promoted to primary
5. Other replicas re-pointed to new primary
6. HAProxy updated automatically
7. Applications reconnect automatically
```

Monitor failover:
```bash
# Watch Patroni logs
tail -f /var/log/patroni/patroni.log

# Watch cluster state
watch -n 1 'patronictl -c /etc/patroni/patroni.yml list'

# Check etcd for leader
etcdctl get /db/hft_trading/leader
```

5.2 MANUAL FAILOVER
--------------------

Scenario: Planned maintenance on primary

```bash
# Step 1: Check cluster status
patronictl -c /etc/patroni/patroni.yml list

# Step 2: Perform controlled switchover
patronictl -c /etc/patroni/patroni.yml switchover \
    --master postgresql0 \
    --candidate postgresql1 \
    --force

# Step 3: Verify new primary
patronictl -c /etc/patroni/patroni.yml list

# Step 4: Check application connectivity
psql -h haproxy -p 5000 -U hft_user -d trading -c "SELECT pg_is_in_recovery();"
# Should return 'f' (false) for primary
```

5.3 EMERGENCY FAILOVER
-----------------------

If Patroni is unavailable:

```bash
# On standby to be promoted:

# Step 1: Stop patroni (to prevent conflicts)
systemctl stop patroni

# Step 2: Promote standby manually
pg_ctl promote -D /var/lib/postgresql/14/main

# Wait for promotion (usually <10 seconds)
sleep 10

# Step 3: Verify promotion
psql -d postgres -c "SELECT pg_is_in_recovery();"
# Should return 'f'

# Step 4: Update HAProxy manually
# Edit /etc/haproxy/haproxy.cfg
# Or use HAProxy runtime API:
echo "set server postgres_primary/postgresql1 state ready" | \
    socat stdio /var/run/haproxy.sock

# Step 5: Reconfigure old primary as standby (when it comes back)
# Create standby.signal file
touch /var/lib/postgresql/14/main/standby.signal

# Update primary_conninfo in postgresql.auto.conf
echo "primary_conninfo = 'host=10.0.1.11 port=5432 user=replicator'" >> \
    /var/lib/postgresql/14/main/postgresql.auto.conf

# Restart
systemctl restart postgresql
```

================================================================================
6. PERFORMANCE TUNING
================================================================================

6.1 REPLICATION PERFORMANCE
-----------------------------

Optimize WAL shipping:
```sql
-- On Primary: Use compression for WAL
ALTER SYSTEM SET wal_compression = on;

-- Increase WAL buffer
ALTER SYSTEM SET wal_buffers = '32MB';

-- Tune checkpoint behavior
ALTER SYSTEM SET checkpoint_completion_target = 0.9;
ALTER SYSTEM SET checkpoint_timeout = '15min';

SELECT pg_reload_conf();
```

Network tuning for replication:
```bash
# Increase TCP buffer sizes
sysctl -w net.core.rmem_max=134217728
sysctl -w net.core.wmem_max=134217728
sysctl -w net.ipv4.tcp_rmem='4096 87380 67108864'
sysctl -w net.ipv4.tcp_wmem='4096 65536 67108864'

# Enable TCP timestamps and window scaling
sysctl -w net.ipv4.tcp_timestamps=1
sysctl -w net.ipv4.tcp_window_scaling=1

# Persist
cat >> /etc/sysctl.conf <<EOFNET
net.core.rmem_max=134217728
net.core.wmem_max=134217728
net.ipv4.tcp_rmem=4096 87380 67108864
net.ipv4.tcp_wmem=4096 65536 67108864
net.ipv4.tcp_timestamps=1
net.ipv4.tcp_window_scaling=1
EOFNET
```

6.2 REPLICATION SLOT MAINTENANCE
----------------------------------

```sql
-- Monitor slot lag
SELECT slot_name,
       active,
       pg_size_pretty(pg_wal_lsn_diff(pg_current_wal_lsn(), restart_lsn)) AS lag
FROM pg_replication_slots;

-- Remove inactive slots (prevents WAL accumulation)
SELECT pg_drop_replication_slot('inactive_slot_name');

-- Advance slot to current position (if replica is far behind)
SELECT pg_replication_slot_advance('standby1_slot', pg_current_wal_lsn());
```

================================================================================
END OF DOCUMENT
================================================================================
