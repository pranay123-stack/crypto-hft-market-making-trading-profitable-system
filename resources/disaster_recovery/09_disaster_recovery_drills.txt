================================================================================
DISASTER RECOVERY - DR DRILLS & TESTING PROTOCOLS
High-Frequency Trading System
Quarterly Full Drills | Monthly Component Testing | Weekly Validation
================================================================================

TABLE OF CONTENTS
-----------------
1. DR Testing Overview & Strategy
2. Monthly Component Testing
3. Quarterly Full DR Drills
4. Annual Comprehensive Simulation
5. Test Automation Framework
6. Drill Evaluation Metrics
7. Lessons Learned Process
8. Drill Documentation & Reporting
9. Compliance & Audit Requirements
10. Continuous Improvement Program

================================================================================
1. DR TESTING OVERVIEW & STRATEGY
================================================================================

1.1 TESTING PYRAMID
--------------------

┌────────────────────────────────────────────────────────────────────┐
│                    DR TESTING PYRAMID                              │
├────────────────────────────────────────────────────────────────────┤
│                                                                    │
│                   ┌──────────────────┐                            │
│                   │   Annual Full    │                            │
│                   │   Simulation     │                            │
│                   │  (8+ hours)      │                            │
│                   └──────────────────┘                            │
│                          │                                         │
│             ┌────────────┴────────────┐                           │
│             │   Quarterly DR Drills   │                           │
│             │   (4 hours)             │                           │
│             └─────────────────────────┘                           │
│                        │                                           │
│           ┌────────────┴────────────┐                             │
│           │   Monthly Component     │                             │
│           │   Tests (1 hour)        │                             │
│           └─────────────────────────┘                             │
│                      │                                             │
│         ┌────────────┴────────────┐                               │
│         │   Weekly Validation     │                               │
│         │   (15 minutes)          │                               │
│         └─────────────────────────┘                               │
│                    │                                               │
│       ┌────────────┴────────────┐                                 │
│       │   Daily Health Checks   │                                 │
│       │   (Automated)           │                                 │
│       └─────────────────────────┘                                 │
└────────────────────────────────────────────────────────────────────┘

1.2 TESTING OBJECTIVES
-----------------------

Primary Objectives:
1. Validate RTO/RPO targets are achievable
2. Verify all runbooks are accurate and complete
3. Train team on DR procedures
4. Identify gaps in DR infrastructure
5. Test inter-team communication
6. Validate monitoring and alerting
7. Ensure data consistency after recovery
8. Meet regulatory compliance requirements

Success Criteria:
☐ RTO met for all critical systems (<15 min full DC)
☐ RPO met (zero data loss for Tier 1)
☐ All services recovered successfully
☐ No data corruption or loss
☐ Team followed procedures correctly
☐ Issues documented and addressed
☐ Stakeholders kept informed
☐ Audit trail complete

================================================================================
2. MONTHLY COMPONENT TESTING
================================================================================

2.1 MONTHLY TEST SCHEDULE
--------------------------

Week 1: Database Failover
Week 2: Application Failover
Week 3: Network Failover
Week 4: Backup Restoration

Monthly Test Checklist:
☐ Schedule test during low-volume period
☐ Notify stakeholders 48 hours in advance
☐ Prepare test environment
☐ Document pre-test state
☐ Execute test procedure
☐ Validate recovery
☐ Restore to normal operations
☐ Document results
☐ Update runbooks if needed
☐ Brief management on results

2.2 DATABASE FAILOVER TEST
----------------------------

Procedure: DR-MONTHLY-DB-001
Duration: 60 minutes
Frequency: Month 1, 5, 9 (quarterly offset)

Script: /opt/hft/scripts/monthly_db_failover_test.sh

```bash
#!/bin/bash
# Monthly Database Failover Test

TEST_LOG="/var/log/hft/monthly_tests/db_failover_$(date +%Y%m%d).log"
mkdir -p $(dirname $TEST_LOG)

exec > >(tee -a $TEST_LOG)
exec 2>&1

echo "========================================="
echo "MONTHLY DATABASE FAILOVER TEST"
echo "========================================="
echo "Date: $(date)"
echo "Operator: $(whoami)"
echo ""

# Pre-test validation
echo "[1/8] Pre-test validation..."
patronictl -c /etc/patroni/patroni.yml list

# Get current leader
CURRENT_LEADER=$(patronictl -c /etc/patroni/patroni.yml list | grep Leader | awk '{print $2}')
echo "Current leader: $CURRENT_LEADER"

# Check replication lag
echo "[2/8] Checking replication lag..."
psql -h $CURRENT_LEADER -U monitoring -d postgres -c "
    SELECT application_name,
           sync_state,
           replay_lag
    FROM pg_stat_replication;
"

# Perform controlled switchover
echo "[3/8] Performing controlled switchover..."
START_TIME=$(date +%s)

patronictl -c /etc/patroni/patroni.yml switchover \
    --master $CURRENT_LEADER \
    --force

# Wait for switchover
sleep 15

END_TIME=$(date +%s)
FAILOVER_DURATION=$((END_TIME - START_TIME))

# Verify new leader
echo "[4/8] Verifying new leader..."
NEW_LEADER=$(patronictl -c /etc/patroni/patroni.yml list | grep Leader | awk '{print $2}')
echo "New leader: $NEW_LEADER"

if [ "$NEW_LEADER" == "$CURRENT_LEADER" ]; then
    echo "ERROR: Switchover failed - leader unchanged"
    exit 1
fi

# Test database connectivity
echo "[5/8] Testing database connectivity..."
psql -h haproxy -p 5000 -U hft_user -d trading -c "SELECT NOW();"

if [ $? -ne 0 ]; then
    echo "ERROR: Cannot connect to database"
    exit 1
fi

# Test write capability
echo "[6/8] Testing write capability..."
psql -h haproxy -p 5000 -U hft_user -d trading -c "
    INSERT INTO health_checks (test_name, test_time, result)
    VALUES ('monthly_db_failover', NOW(), 'PASS');
"

# Verify replication re-established
echo "[7/8] Verifying replication..."
sleep 10

REPLICA_COUNT=$(psql -h $NEW_LEADER -U monitoring -d postgres -t -c "
    SELECT COUNT(*) FROM pg_stat_replication WHERE state = 'streaming';
" | tr -d ' ')

echo "Active replicas: $REPLICA_COUNT"

if [ $REPLICA_COUNT -lt 1 ]; then
    echo "WARNING: No active replicas"
fi

# Test summary
echo "[8/8] Test summary..."
echo "========================================="
echo "TEST RESULTS"
echo "========================================="
echo "Failover Duration: ${FAILOVER_DURATION}s"
echo "Target: <120s"
echo "Status: $([ $FAILOVER_DURATION -le 120 ] && echo 'PASSED' || echo 'FAILED')"
echo "Old Leader: $CURRENT_LEADER"
echo "New Leader: $NEW_LEADER"
echo "Active Replicas: $REPLICA_COUNT"
echo "========================================="

# Send report
/opt/hft/scripts/send_report.sh "Monthly DB Failover Test" "$TEST_LOG"
```

2.3 APPLICATION FAILOVER TEST
-------------------------------

Procedure: DR-MONTHLY-APP-001

Script: /opt/hft/scripts/monthly_app_failover_test.sh

```bash
#!/bin/bash
# Monthly Application Failover Test

echo "=== APPLICATION FAILOVER TEST ==="

# Test trading engine failover
echo "[1/3] Testing trading engine failover..."

# Get current primary
CURRENT_PRIMARY=$(consul kv get service/trading/primary)
echo "Current primary: $CURRENT_PRIMARY"

# Simulate failure
ssh $CURRENT_PRIMARY "systemctl stop hft-trading-engine"

# Wait for automatic failover (should be <60s)
START=$(date +%s)
for i in {1..60}; do
    NEW_PRIMARY=$(consul kv get service/trading/primary)
    if [ "$NEW_PRIMARY" != "$CURRENT_PRIMARY" ]; then
        echo "Failover detected to: $NEW_PRIMARY"
        break
    fi
    sleep 1
done
END=$(date +%s)

FAILOVER_TIME=$((END - START))
echo "Failover time: ${FAILOVER_TIME}s"

# Test new primary
echo "[2/3] Testing new primary..."
curl -f http://$NEW_PRIMARY:8080/health

# Restore original primary
echo "[3/3] Restoring original primary..."
ssh $CURRENT_PRIMARY "systemctl start hft-trading-engine"

echo "Test complete"
```

2.4 BACKUP RESTORATION TEST
-----------------------------

Procedure: DR-MONTHLY-BACKUP-001

Script: /opt/hft/scripts/monthly_backup_restore_test.sh

```bash
#!/bin/bash
# Monthly Backup Restoration Test

echo "=== BACKUP RESTORATION TEST ==="

# Get latest backup
LATEST_BACKUP=$(ls -t /backup/daily/postgres_*.dump | head -1)
echo "Testing backup: $LATEST_BACKUP"

# Restore to test database
echo "[1/4] Restoring to test database..."
createdb trading_restore_test
pg_restore -d trading_restore_test $LATEST_BACKUP

if [ $? -ne 0 ]; then
    echo "ERROR: Restore failed"
    exit 1
fi

# Validate data
echo "[2/4] Validating restored data..."

# Check record counts
ORDERS=$(psql -d trading_restore_test -t -c "SELECT COUNT(*) FROM orders;" | tr -d ' ')
TRADES=$(psql -d trading_restore_test -t -c "SELECT COUNT(*) FROM trades;" | tr -d ' ')

echo "Restored orders: $ORDERS"
echo "Restored trades: $TRADES"

# Compare with production
PROD_ORDERS=$(psql -d trading -t -c "SELECT COUNT(*) FROM orders;" | tr -d ' ')
DIFF=$((PROD_ORDERS - ORDERS))

echo "Difference from production: $DIFF orders"

# Cleanup
echo "[3/4] Cleaning up test database..."
dropdb trading_restore_test

# Test cold storage retrieval
echo "[4/4] Testing cold storage retrieval..."
COLD_BACKUP=$(aws s3 ls s3://hft-backup-archive/ | tail -1 | awk '{print $4}')
echo "Latest cold backup: $COLD_BACKUP"

# Verify accessible
aws s3 head-object --bucket hft-backup-archive --key $COLD_BACKUP

echo "Backup restoration test complete"
```

================================================================================
3. QUARTERLY FULL DR DRILLS
================================================================================

3.1 QUARTERLY DRILL PLAN
--------------------------

Schedule: Last Saturday of March, June, September, December
Time: 02:00 AM - 06:00 AM (4-hour window)
Impact: Non-production hours, minimal business impact

Drill Scope:
- Full primary data center "failure"
- Complete failover to DR site
- 4 hours of DR operations
- Failback to primary

Participants:
- Engineering Team (5-8 people)
- Operations Team (2-3 people)
- Management (1-2 observers)
- External Auditor (annual requirement)

3.2 QUARTERLY DRILL SCRIPT
----------------------------

Script: /opt/hft/scripts/quarterly_dr_drill.sh

```bash
#!/bin/bash
# Quarterly DR Drill - Full Execution

DRILL_DIR="/var/log/hft/dr_drills"
DRILL_DATE=$(date +%Y%m%d)
DRILL_LOG="$DRILL_DIR/drill_$DRILL_DATE.log"

mkdir -p $DRILL_DIR
exec > >(tee -a $DRILL_LOG)
exec 2>&1

echo "=================================================="
echo "       QUARTERLY DISASTER RECOVERY DRILL"
echo "=================================================="
echo "Date: $(date)"
echo "Drill Master: $(whoami)"
echo "Scenario: Primary DC (NYC) Complete Failure"
echo "=================================================="
echo ""

# Configuration
PRIMARY_DC="nyc"
DR_DC="chicago"

# Phase 1: Preparation (15 minutes)
echo "PHASE 1: PREPARATION (0800-0815)"
echo "-----------------------------------"

echo "[1.1] Team check-in..."
echo "Participants:"
echo "  - Drill Master: $(whoami)"
echo "  - Engineering: [List names]"
echo "  - Operations: [List names]"
echo "  - Observer: [Name if applicable]"

echo "[1.2] System health check..."
/opt/hft/scripts/comprehensive_healthcheck.sh

echo "[1.3] Creating pre-drill snapshot..."
/opt/hft/scripts/create_checkpoint.sh

echo "[1.4] Notifying stakeholders..."
/opt/hft/scripts/send_alert.sh "INFO" "Quarterly DR drill starting"

read -p "All teams ready? Press Enter to continue..."

# Phase 2: Failure Simulation (15 minutes)
echo ""
echo "PHASE 2: FAILURE SIMULATION (0815-0830)"
echo "---------------------------------------"

START_DRILL=$(date +%s)

echo "[2.1] Simulating primary DC failure..."
echo "  - Stopping all services in $PRIMARY_DC..."

ssh $PRIMARY_DC-admin "
    systemctl stop hft-trading-engine
    systemctl stop hft-order-management
    systemctl stop hft-risk-engine
    systemctl stop postgresql
    systemctl stop redis
"

echo "[2.2] Simulating network partition..."
ssh $PRIMARY_DC-router "iptables -I INPUT -j DROP; iptables -I OUTPUT -j DROP"

echo "[2.3] Marking primary DC as offline in monitoring..."
consul kv put datacenter/$PRIMARY_DC/status "OFFLINE"

FAILURE_TIME=$(date +%s)
echo "Primary DC marked offline at: $(date)"

# Phase 3: Detection & Alert (5 minutes)
echo ""
echo "PHASE 3: DETECTION & ALERT (0830-0835)"
echo "---------------------------------------"

echo "[3.1] Waiting for automatic detection..."
# Monitoring should detect within 30 seconds
sleep 30

echo "[3.2] Verifying alerts were sent..."
# Check alert logs
tail -20 /var/log/hft/alerts.log

# Phase 4: DR Activation (30 minutes)
echo ""
echo "PHASE 4: DR ACTIVATION (0835-0905)"
echo "----------------------------------"

START_RECOVERY=$(date +%s)

echo "[4.1] Activating DR site..."
/opt/hft/scripts/activate_dr_site.sh $DR_DC

echo "[4.2] Promoting DR database..."
ssh $DR_DC-db "pg_ctl promote -D /var/lib/pgsql/data"
sleep 15

echo "[4.3] Starting DR services..."
ssh $DR_DC-admin "
    systemctl start hft-trading-engine-primary
    systemctl start hft-order-management
    systemctl start hft-risk-engine
"

echo "[4.4] Updating DNS and load balancers..."
/opt/hft/scripts/update_infrastructure.sh $PRIMARY_DC $DR_DC

echo "[4.5] Recovering state..."
ssh $DR_DC-admin "/opt/hft/scripts/recover_trading_state.sh"

END_RECOVERY=$(date +%s)
RTO=$((END_RECOVERY - FAILURE_TIME))

echo "DR activation completed in ${RTO}s"

# Phase 5: Validation (30 minutes)
echo ""
echo "PHASE 5: VALIDATION (0905-0935)"
echo "--------------------------------"

echo "[5.1] Health checks..."
ssh $DR_DC-admin "/opt/hft/scripts/postfailover_check.sh"

echo "[5.2] Data integrity validation..."
ssh $DR_DC-admin "/opt/hft/scripts/validate_state.py"

echo "[5.3] Position reconciliation..."
ssh $DR_DC-admin "/opt/hft/scripts/reconcile_positions.py"

echo "[5.4] Test trading capability..."
ssh $DR_DC-admin "/opt/hft/scripts/test_trading_capability.sh"

echo "[5.5] Performance testing..."
ssh $DR_DC-admin "/opt/hft/scripts/measure_latency.sh"

# Phase 6: DR Operations (2 hours)
echo ""
echo "PHASE 6: DR OPERATIONS (0935-1135)"
echo "-----------------------------------"

echo "Operating on DR site for 2 hours (simulated)"
echo "In production, trading would continue from DR"
echo "Monitoring system performance, alerts, etc."

read -p "Press Enter to proceed to failback phase..."

# Phase 7: Failback Preparation (30 minutes)
echo ""
echo "PHASE 7: FAILBACK PREPARATION (1135-1205)"
echo "------------------------------------------"

echo "[7.1] Restoring primary DC..."
ssh $PRIMARY_DC-router "iptables -F"
ssh $PRIMARY_DC-admin "systemctl start postgresql"
ssh $PRIMARY_DC-admin "systemctl start redis"

sleep 30

echo "[7.2] Synchronizing data DR → Primary..."
ssh $DR_DC-admin "/opt/hft/scripts/sync_to_primary.sh"

echo "[7.3] Verifying primary DC health..."
ssh $PRIMARY_DC-admin "/opt/hft/scripts/comprehensive_healthcheck.sh"

# Phase 8: Failback Execution (30 minutes)
echo ""
echo "PHASE 8: FAILBACK EXECUTION (1205-1235)"
echo "---------------------------------------"

echo "[8.1] Stopping writes on DR..."
ssh $DR_DC-admin "redis-cli SET trading:enabled false"

echo "[8.2] Final data sync..."
ssh $DR_DC-admin "/opt/hft/scripts/sync_to_primary.sh --final"

echo "[8.3] Promoting primary database..."
ssh $PRIMARY_DC-db "pg_ctl promote -D /var/lib/pgsql/data"

echo "[8.4] Starting primary services..."
ssh $PRIMARY_DC-admin "
    systemctl start hft-trading-engine-primary
    systemctl start hft-order-management
    systemctl start hft-risk-engine
"

echo "[8.5] Updating infrastructure..."
/opt/hft/scripts/update_infrastructure.sh $DR_DC $PRIMARY_DC

echo "[8.6] Enabling trading on primary..."
ssh $PRIMARY_DC-admin "redis-cli SET trading:enabled true"

# Phase 9: Post-Failback Validation (30 minutes)
echo ""
echo "PHASE 9: POST-FAILBACK VALIDATION (1235-1305)"
echo "----------------------------------------------"

echo "[9.1] Health checks..."
ssh $PRIMARY_DC-admin "/opt/hft/scripts/postfailover_check.sh"

echo "[9.2] Data consistency validation..."
ssh $PRIMARY_DC-admin "/opt/hft/scripts/validate_state.py"

echo "[9.3] Replication re-establishment..."
patronictl -c /etc/patroni/patroni.yml list

echo "[9.4] End-to-end testing..."
/opt/hft/scripts/e2e_test.sh

# Phase 10: Drill Completion (25 minutes)
echo ""
echo "PHASE 10: DRILL COMPLETION (1305-1330)"
echo "--------------------------------------"

END_DRILL=$(date +%s)
TOTAL_DURATION=$((END_DRILL - START_DRILL))

echo "[10.1] Generating drill report..."

# Calculate metrics
echo ""
echo "=========================================="
echo "        DRILL RESULTS SUMMARY"
echo "=========================================="
echo "Start Time: $(date -d @$START_DRILL)"
echo "End Time: $(date -d @$END_DRILL)"
echo "Total Duration: $((TOTAL_DURATION / 60)) minutes"
echo ""
echo "KEY METRICS:"
echo "  - RTO (Failover): ${RTO}s (Target: <900s)"
echo "  - RTO Status: $([ $RTO -le 900 ] && echo 'MET' || echo 'MISSED')"
echo "  - RPO: 0s (zero data loss)"
echo "  - Services Recovered: All"
echo "  - Data Integrity: Validated"
echo ""
echo "PARTICIPANTS:"
echo "  - Engineering: [count]"
echo "  - Operations: [count]"
echo "  - Management: [count]"
echo ""
echo "ISSUES FOUND: [List any issues]"
echo "  - None (or list issues)"
echo ""
echo "RECOMMENDATIONS:"
echo "  - [List recommendations]"
echo "=========================================="

echo "[10.2] Conducting hot wash debrief..."
echo "Debrief Questions:"
echo "  1. What went well?"
echo "  2. What could be improved?"
echo "  3. Were runbooks accurate?"
echo "  4. Was communication effective?"
echo "  5. Any surprises or unexpected issues?"

read -p "Press Enter after debrief is complete..."

echo "[10.3] Saving drill artifacts..."
tar -czf "$DRILL_DIR/drill_${DRILL_DATE}_artifacts.tar.gz" \
    /var/log/hft/*.log \
    /var/log/patroni/*.log \
    /var/log/haproxy/*.log

echo "[10.4] Sending drill report..."
/opt/hft/scripts/generate_drill_report.sh $DRILL_LOG | \
    mail -s "Q$(date +%q) DR Drill Report" management@trading.com

echo ""
echo "=================================================="
echo "     QUARTERLY DR DRILL COMPLETE"
echo "=================================================="
echo "Report saved: $DRILL_LOG"
echo "Artifacts: $DRILL_DIR/drill_${DRILL_DATE}_artifacts.tar.gz"
echo ""
```

================================================================================
4. DRILL EVALUATION METRICS
================================================================================

4.1 QUANTITATIVE METRICS
--------------------------

┌────────────────────────┬──────────┬──────────┬─────────┐
│ Metric                 │ Target   │ Measured │ Status  │
├────────────────────────┼──────────┼──────────┼─────────┤
│ RTO - Full DC Failover │ <15min   │ [X]min   │ [P/F]   │
│ RTO - DB Failover      │ <2min    │ [X]min   │ [P/F]   │
│ RTO - App Failover     │ <1min    │ [X]min   │ [P/F]   │
│ RPO - Tier 1 Data      │ 0s       │ [X]s     │ [P/F]   │
│ Detection Time         │ <30s     │ [X]s     │ [P/F]   │
│ Decision Time          │ <5min    │ [X]min   │ [P/F]   │
│ Execution Time         │ <10min   │ [X]min   │ [P/F]   │
│ Data Loss              │ 0 trans  │ [X]      │ [P/F]   │
│ Service Availability   │ >99.99%  │ [X]%     │ [P/F]   │
└────────────────────────┴──────────┴──────────┴─────────┘

4.2 QUALITATIVE METRICS
-------------------------

Team Performance:
☐ Runbooks followed correctly
☐ Communication clear and timely
☐ Roles and responsibilities clear
☐ Decision-making effective
☐ Issue escalation appropriate
☐ Documentation accurate
☐ Tools and scripts worked as expected
☐ No critical surprises

System Performance:
☐ Failover automatic as designed
☐ Monitoring/alerting effective
☐ Data integrity maintained
☐ Network failover successful
☐ Application recovery clean
☐ Database promotion successful
☐ Replication re-established
☐ Performance acceptable on DR site

================================================================================
5. CONTINUOUS IMPROVEMENT
================================================================================

5.1 POST-DRILL ACTION ITEMS
-----------------------------

Action Item Template:
```
ID: AI-DRILL-YYYYMMDD-NNN
Title: [Brief description]
Priority: Critical / High / Medium / Low
Category: Documentation / Process / Technology / Training
Description: [Detailed description of issue or improvement]
Root Cause: [Why did this issue occur?]
Recommendation: [Proposed solution]
Owner: [Person responsible]
Due Date: [Target completion date]
Status: Open / In Progress / Completed / Cancelled
```

Track in JIRA/tracking system

5.2 RUNBOOK UPDATES
--------------------

After each drill:
1. Update runbooks based on actual experience
2. Add/remove steps as needed
3. Clarify ambiguous instructions
4. Add screenshots or diagrams
5. Update contact information
6. Version control all changes
7. Review and approve updates
8. Communicate changes to team

5.3 TRAINING IMPROVEMENTS
--------------------------

Identify training needs:
- New team member onboarding
- Skill gaps identified during drill
- New technology or procedures
- Refresher training for existing team

Schedule:
- Quarterly DR procedure training
- Annual comprehensive DR workshop
- Ad-hoc training for new systems

================================================================================
END OF DOCUMENT
================================================================================
