================================================================================
                    LINUX DEBUGGING COMMANDS REFERENCE
                    Advanced Debugging Tools for HFT Systems
================================================================================

TABLE OF CONTENTS
-----------------
1. GDB (GNU Debugger)
2. STRACE (System Call Tracer)
3. LTRACE (Library Call Tracer)
4. PERF (Performance Analysis)
5. VALGRIND (Memory Debugging)
6. Core Dump Analysis
7. Dynamic Tracing (BPF/eBPF)
8. Profiling Tools
9. Memory Analysis
10. Debugging Workflows

================================================================================
1. GDB (GNU DEBUGGER)
================================================================================

1.1 BASIC GDB COMMANDS
----------------------

# Start GDB with executable
gdb ./trading_engine

# Attach to running process
gdb -p $(pidof trading_engine)

# Load core dump
gdb ./trading_engine core.12345

# Run with arguments
gdb --args ./trading_engine --config config.yaml

# Quiet mode (no intro)
gdb -q ./trading_engine

# Batch mode (non-interactive)
gdb -batch -x commands.gdb ./trading_engine

GDB ESSENTIAL COMMANDS:
  run (r)              - Start program
  break (b)            - Set breakpoint
  continue (c)         - Continue execution
  step (s)             - Step into
  next (n)             - Step over
  finish               - Step out
  print (p)            - Print variable
  backtrace (bt)       - Show stack trace
  info                 - Show information
  quit (q)             - Exit GDB

HFT USE CASE: Debug crashed trading engine


1.2 GDB BREAKPOINTS
-------------------

# Break at function
(gdb) break main
(gdb) break OrderProcessor::executeOrder

# Break at line
(gdb) break file.cpp:123

# Break at address
(gdb) break *0x400500

# Conditional breakpoint
(gdb) break order.cpp:50 if order_id == 12345

# Temporary breakpoint
(gdb) tbreak main

# Hardware breakpoint
(gdb) hbreak function

# List breakpoints
(gdb) info breakpoints

# Delete breakpoint
(gdb) delete 1

# Disable/enable breakpoint
(gdb) disable 1
(gdb) enable 1

# Clear all breakpoints
(gdb) delete

HFT USE CASE: Break on specific order ID
EXAMPLE: break executeOrder if order.id == 98765


1.3 GDB INSPECTION
------------------

# Print variable
(gdb) print variable_name
(gdb) p order.price

# Print with format
(gdb) p/x variable  # Hex
(gdb) p/d variable  # Decimal
(gdb) p/t variable  # Binary

# Print structure
(gdb) p order
(gdb) p *order_ptr

# Print array
(gdb) p array[0]@10  # First 10 elements

# Examine memory
(gdb) x/10x 0x400000  # 10 hex words

# Show backtrace
(gdb) bt
(gdb) bt full  # With local variables

# Show frame
(gdb) frame 2
(gdb) info frame

# Show local variables
(gdb) info locals

# Show arguments
(gdb) info args

# Show threads
(gdb) info threads

# Switch thread
(gdb) thread 3

# Show registers
(gdb) info registers
(gdb) info all-registers

HFT USE CASE: Inspect order state during execution


1.4 GDB WATCHPOINTS
-------------------

# Watch variable (read/write)
(gdb) watch variable

# Watch read access
(gdb) rwatch variable

# Watch write access
(gdb) awatch variable

# Watch expression
(gdb) watch order.price > 100.0

# List watchpoints
(gdb) info watchpoints

HFT USE CASE: Track when order price changes
EXAMPLE: watch order.price


1.5 GDB ADVANCED
----------------

# Set variable value
(gdb) set variable = value
(gdb) set order.quantity = 100

# Call function
(gdb) call function()
(gdb) call printOrder(order)

# Generate core dump
(gdb) generate-core-file

# Thread apply
(gdb) thread apply all bt  # Backtrace all threads

# Pretty printing (C++)
(gdb) set print pretty on

# Show source code
(gdb) list
(gdb) list function_name

# Disassemble
(gdb) disassemble function

# Show memory map
(gdb) info proc mappings

# TUI mode (text user interface)
(gdb) tui enable
Ctrl+X, Ctrl+A

HFT USE CASE: Multi-threaded debugging
EXAMPLE: thread apply all bt


1.6 GDB SCRIPT
--------------

# .gdbinit or commands file

set pagination off
set print pretty on
set print array on
set print array-indexes on

# Custom commands
define print_order
    print "Order ID: "
    print $arg0.id
    print "Price: "
    print $arg0.price
    print "Quantity: "
    print $arg0.quantity
end

# Automatically execute
handle SIGUSR1 nostop noprint pass
break OrderProcessor::executeOrder
commands
    silent
    print_order order
    continue
end

run

HFT USE CASE: Automated debugging session


================================================================================
2. STRACE (SYSTEM CALL TRACER)
================================================================================

2.1 BASIC STRACE
----------------

# Trace program
strace ./trading_engine

# Attach to running process
strace -p $(pidof trading_engine)

# Trace specific system calls
strace -e trace=open,read,write ./program

# Trace network system calls
strace -e trace=network ./program

# Trace file operations
strace -e trace=file ./program

# Count system calls
strace -c ./program

# Time each syscall
strace -T ./program

# Timestamp each line
strace -t ./program
strace -tt ./program  # Microseconds
strace -ttt ./program # Unix timestamp

# Output to file
strace -o trace.log ./program

# Follow forks
strace -f ./program

# String size limit
strace -s 1024 ./program

HFT USE CASE: Debug why process is slow
EXAMPLE: strace -c -p $(pidof trading_engine)

PERFORMANCE WARNING: 10-100x slowdown
USE CASE: Debug only, never in production


2.2 STRACE FILTERING
--------------------

# Trace specific syscalls
strace -e trace=open,close,read,write ./program

# Multiple filters
strace -e trace=file -e trace=network ./program

# Exclude signals
strace -e signal=none ./program

# Trace only failed calls
strace -e trace=open -e status=failed ./program

# Filter by descriptor
strace -e read=3,5 ./program  # Only fd 3 and 5

# Decode structures
strace -v ./program

HFT USE CASE: Find why file open fails
EXAMPLE: strace -e trace=open -e status=failed ./app


2.3 STRACE OUTPUT ANALYSIS
---------------------------

# Summary statistics
strace -c ./program

OUTPUT:
% time     seconds  usecs/call     calls    errors syscall
------ ----------- ----------- --------- --------- --------
 42.38    0.001234          12       100           read
 31.54    0.000918           9       102           write
 15.23    0.000443          44        10           futex

# Time-consuming calls
strace -T ./program 2>&1 | grep -E "^[a-z].*<[0-9]" | sort -t'<' -k2 -rn | head

# Failed syscalls
strace -e status=failed ./program

HFT USE CASE: Identify latency sources
ONE-LINER: strace -c -p PID


================================================================================
3. LTRACE (LIBRARY CALL TRACER)
================================================================================

3.1 LTRACE USAGE
----------------

# Trace library calls
ltrace ./program

# Attach to process
ltrace -p $(pidof program)

# Count calls
ltrace -c ./program

# Time each call
ltrace -T ./program

# Timestamp
ltrace -t ./program
ltrace -tt ./program

# Output to file
ltrace -o trace.log ./program

# Filter by function
ltrace -e malloc,free ./program

# String size
ltrace -s 128 ./program

# Follow forks
ltrace -f ./program

HFT USE CASE: Debug library interactions
EXAMPLE: ltrace -e 'malloc+free' ./trading_engine


3.2 LTRACE FILTERING
--------------------

# Trace specific library
ltrace -l /usr/lib/libboost.so ./program

# Exclude library
ltrace -x libc.so.6 ./program

# Multiple functions
ltrace -e 'malloc+free+calloc+realloc' ./program

HFT USE CASE: Track memory allocations
EXAMPLE: ltrace -c -e 'malloc+free' ./program


================================================================================
4. PERF (PERFORMANCE ANALYSIS)
================================================================================

4.1 PERF BASICS
---------------

# Record performance data
perf record ./program
perf record -p $(pidof program)

# Record for duration
perf record -p $(pidof program) sleep 10

# Record all CPUs
perf record -a sleep 10

# Record with call graph
perf record -g ./program

# Record specific events
perf record -e cycles,instructions ./program

# View recording
perf report

# Interactive TUI
perf report --tui

# Show statistics
perf stat ./program

HFT USE CASE: Profile trading engine performance
EXAMPLE: perf record -g -p $(pidof trading_engine) sleep 60


4.2 PERF EVENTS
---------------

# List available events
perf list

# Hardware events
perf stat -e cycles,instructions,cache-references,cache-misses ./program

# Cache events
perf stat -e L1-dcache-loads,L1-dcache-load-misses,LLC-loads,LLC-load-misses ./program

# Branch prediction
perf stat -e branches,branch-misses ./program

# TLB events
perf stat -e dTLB-loads,dTLB-load-misses ./program

# Context switches
perf stat -e context-switches,cpu-migrations ./program

# Page faults
perf stat -e page-faults ./program

# Custom event groups
perf stat -e '{cycles,instructions,cache-misses}' ./program

HFT USE CASE: Measure cache efficiency
EXAMPLE: perf stat -e cache-references,cache-misses ./program

OUTPUT INTERPRETATION:
  IPC > 2.0 = Good instruction throughput
  Cache miss rate < 3% = Good cache usage
  Branch miss rate < 5% = Good prediction


4.3 PERF RECORD OPTIONS
------------------------

# Sampling frequency
perf record -F 1000 ./program  # 1000 Hz

# Record with dwarf call graph
perf record --call-graph dwarf ./program

# Record with LBR (Last Branch Record)
perf record --call-graph lbr ./program

# Record all threads
perf record -t $(pidof program | tr ' ' ',')

# Record with higher precision
perf record -e cycles:pp ./program  # Precise event

# Record off-CPU time
perf record -e sched:sched_switch -a sleep 10

HFT USE CASE: Deep performance analysis


4.4 PERF ANALYSIS
-----------------

# Show report
perf report

# Report with source
perf annotate

# Differential profile
perf diff before.perf after.perf

# Flame graph (requires flamegraph.pl)
perf script | stackcollapse-perf.pl | flamegraph.pl > flame.svg

# Top functions
perf top

# Top with specific event
perf top -e cycles

# Script output
perf script

HFT USE CASE: Identify hot spots
EXAMPLE: perf report --stdio | head -50


4.5 PERF PROBE
--------------

# Add dynamic probe
perf probe -x /opt/trading/bin/engine executeOrder

# Add with arguments
perf probe -x /path/to/binary 'function arg1 arg2'

# List probes
perf probe -l

# Delete probe
perf probe -d probe_name

# Record with probe
perf record -e probe_name -a

HFT USE CASE: Trace custom function calls


================================================================================
5. VALGRIND (MEMORY DEBUGGING)
================================================================================

5.1 MEMCHECK (Memory Error Detection)
--------------------------------------

# Basic memory check
valgrind ./program

# Detailed leak check
valgrind --leak-check=full ./program

# Show reachable blocks
valgrind --leak-check=full --show-reachable=yes ./program

# Track origins
valgrind --track-origins=yes ./program

# Verbose output
valgrind -v --leak-check=full ./program

# Log to file
valgrind --log-file=valgrind.log ./program

# Suppress known issues
valgrind --suppressions=suppress.txt ./program

HFT USE CASE: Find memory leaks in trading engine
PERFORMANCE WARNING: 10-50x slowdown

MEMORY ERRORS DETECTED:
  - Invalid memory access
  - Use of uninitialized values
  - Memory leaks
  - Double free
  - Mismatched new/delete


5.2 CALLGRIND (Call Graph Profiling)
-------------------------------------

# Profile with callgrind
valgrind --tool=callgrind ./program

# Output file: callgrind.out.PID

# Visualize with kcachegrind
kcachegrind callgrind.out.12345

# Dump during execution
callgrind_control -d

# Zero counters
callgrind_control -z

HFT USE CASE: Identify call graph bottlenecks


5.3 CACHEGRIND (Cache Profiling)
---------------------------------

# Cache profiling
valgrind --tool=cachegrind ./program

# Output file: cachegrind.out.PID

# Annotate source
cg_annotate cachegrind.out.12345

# Specify cache sizes
valgrind --tool=cachegrind --I1=32768,8,64 ./program

HFT USE CASE: Optimize cache usage
OUTPUT: L1/LL cache misses and instruction references


5.4 HELGRIND (Thread Error Detection)
--------------------------------------

# Detect thread errors
valgrind --tool=helgrind ./program

DETECTS:
  - Data races
  - Lock ordering problems
  - Incorrect use of POSIX threads API

HFT USE CASE: Find race conditions in multi-threaded code


5.5 MASSIF (Heap Profiling)
----------------------------

# Heap profiling
valgrind --tool=massif ./program

# Visualize
ms_print massif.out.12345

# Stack and heap
valgrind --tool=massif --stacks=yes ./program

HFT USE CASE: Track heap memory growth over time


================================================================================
6. CORE DUMP ANALYSIS
================================================================================

6.1 CORE DUMP CONFIGURATION
----------------------------

# Enable core dumps
ulimit -c unlimited

# Set core pattern
echo "core.%e.%p.%t" > /proc/sys/kernel/core_pattern

# Set core dump location
echo "/var/crash/core.%e.%p" > /proc/sys/kernel/core_pattern

# Persistent in /etc/sysctl.conf
kernel.core_pattern = /var/crash/core.%e.%p.%t
kernel.core_uses_pid = 1

# Check current settings
cat /proc/sys/kernel/core_pattern

HFT USE CASE: Collect crash dumps for post-mortem analysis


6.2 CORE DUMP ANALYSIS
-----------------------

# Load in GDB
gdb ./trading_engine core.12345

# Quick analysis
gdb -batch -ex bt -ex quit ./trading_engine core.12345

# Get backtrace from all threads
gdb -batch -ex "thread apply all bt" -ex quit ./trading_engine core.12345

# Extract info
(gdb) bt full
(gdb) info threads
(gdb) thread apply all bt
(gdb) info registers
(gdb) info sharedlibrary

HFT USE CASE: Analyze production crash
WORKFLOW:
  1. Load core dump
  2. Get backtrace: bt full
  3. Inspect variables: info locals
  4. Check all threads: thread apply all bt


6.3 AUTOMATIC CORE ANALYSIS
----------------------------

#!/bin/bash
# Automatic core dump analyzer

CORE_FILE=$1
BINARY=$2

gdb -batch \
    -ex "bt full" \
    -ex "info threads" \
    -ex "thread apply all bt" \
    -ex "info registers" \
    -ex "quit" \
    $BINARY $CORE_FILE > core_analysis.txt 2>&1

echo "Analysis saved to core_analysis.txt"


================================================================================
7. DYNAMIC TRACING (BPF/eBPF)
================================================================================

7.1 BCC TOOLS
-------------

# Install BCC tools
apt install bpfcc-tools

# Trace system calls
execsnoop  # New processes
opensnoop  # File opens
tcpconnect # TCP connections
tcpaccept  # TCP accepts

# Performance tools
profile    # CPU profiling
stackcount # Count stack traces
funccount  # Count function calls

# Latency tools
biolatency # Block I/O latency
tcplife    # TCP connection lifespan

HFT USE CASE: Zero-overhead production tracing
EXAMPLE: tcpconnect -p $(pidof trading_engine)


7.2 BPFTRACE
------------

# Install bpftrace
apt install bpftrace

# One-liners
bpftrace -e 'tracepoint:syscalls:sys_enter_open { @[comm] = count(); }'

# Trace function calls
bpftrace -e 'uprobe:/path/to/binary:function { printf("Called\n"); }'

# Count syscalls by process
bpftrace -e 'tracepoint:raw_syscalls:sys_enter { @[comm] = count(); }'

# Latency histogram
bpftrace -e 'kprobe:do_sys_open { @ts[tid] = nsecs; }
             kretprobe:do_sys_open /@ts[tid]/ { @lat = hist(nsecs - @ts[tid]); delete(@ts[tid]); }'

HFT USE CASE: Production performance analysis with minimal overhead


7.3 FTRACE
----------

# Enable function tracing
echo function > /sys/kernel/debug/tracing/current_tracer

# Trace specific function
echo do_sys_open > /sys/kernel/debug/tracing/set_ftrace_filter

# Read trace
cat /sys/kernel/debug/tracing/trace

# Clear trace
echo > /sys/kernel/debug/tracing/trace

# Disable tracing
echo nop > /sys/kernel/debug/tracing/current_tracer

HFT USE CASE: Kernel-level debugging


================================================================================
8. PROFILING TOOLS
================================================================================

8.1 GPROF (GNU Profiler)
-------------------------

# Compile with profiling
g++ -pg -o program program.cpp

# Run program (generates gmon.out)
./program

# Generate profile
gprof program gmon.out > analysis.txt

# Call graph
gprof -q program gmon.out

HFT USE CASE: Profile CPU usage by function
LIMITATION: Sampling-based, may miss short functions


8.2 GOOGLE PERFTOOLS
--------------------

# CPU profiling
LD_PRELOAD=/usr/lib/libprofiler.so CPUPROFILE=prof.out ./program

# Analyze
pprof --text ./program prof.out
pprof --pdf ./program prof.out > profile.pdf

# Heap profiling
LD_PRELOAD=/usr/lib/libtcmalloc.so HEAPPROFILE=heap.out ./program

HFT USE CASE: Low-overhead production profiling


8.3 INTEL VTUNE (Commercial)
-----------------------------

# Hotspot analysis
vtune -collect hotspots ./program

# Memory access analysis
vtune -collect memory-access ./program

# Threading analysis
vtune -collect threading ./program

HFT USE CASE: Advanced CPU microarchitecture analysis


================================================================================
9. DEBUGGING WORKFLOWS
================================================================================

9.1 CRASH INVESTIGATION
-----------------------

1. Collect core dump
   ulimit -c unlimited
   # Wait for crash or kill -ABRT PID

2. Analyze with GDB
   gdb ./program core.PID
   (gdb) bt full
   (gdb) info threads
   (gdb) thread apply all bt

3. Check for common issues
   - Null pointer dereference
   - Stack overflow
   - Heap corruption
   - Double free
   - Segmentation fault

4. Reproduce with debugging
   gdb ./program
   (gdb) run
   (gdb) bt full


9.2 PERFORMANCE INVESTIGATION
------------------------------

1. Measure with perf
   perf record -g ./program
   perf report

2. Identify hot spots
   perf top

3. Check cache misses
   perf stat -e cache-references,cache-misses ./program

4. Profile with valgrind
   valgrind --tool=callgrind ./program

5. Analyze results
   - High CPU in specific function
   - Cache miss rate
   - Branch misprediction
   - System call overhead


9.3 MEMORY LEAK INVESTIGATION
------------------------------

1. Run with valgrind
   valgrind --leak-check=full --show-leak-kinds=all ./program

2. Analyze leak summary
   - Definitely lost
   - Indirectly lost
   - Possibly lost
   - Still reachable

3. Fix leaks
   - Track allocation/deallocation
   - Use smart pointers
   - Check RAII patterns

4. Verify fix
   valgrind --leak-check=full ./program


9.4 LATENCY SPIKE INVESTIGATION
--------------------------------

1. Profile with perf
   perf record -g -p PID sleep 60

2. Check system calls
   strace -c -p PID

3. Monitor context switches
   pidstat -w -p PID 1

4. Check for:
   - Page faults (memory not resident)
   - Lock contention
   - I/O waits
   - System calls
   - GC pauses (for managed languages)


================================================================================
BEST PRACTICES
================================================================================

1. Debugging Strategy
   - Reproduce issue first
   - Collect data before debugging
   - Use least invasive tools first
   - Document findings
   - Test fixes thoroughly

2. Tool Selection
   - GDB: Interactive debugging
   - strace: System call issues
   - perf: Performance profiling
   - valgrind: Memory errors (development)
   - eBPF: Production tracing

3. Performance Considerations
   - strace/ltrace: Very high overhead
   - valgrind: 10-50x slowdown
   - perf: 1-5% overhead
   - eBPF: < 1% overhead
   - Choose based on use case

4. Production Debugging
   - Minimize overhead
   - Use sampling-based tools
   - Enable core dumps
   - Keep debug symbols separate
   - Use eBPF for dynamic tracing

5. HFT-Specific
   - Profile with representative load
   - Measure worst-case latency
   - Check for system call overhead
   - Verify memory is locked
   - Monitor context switches

SAFETY WARNINGS
---------------
- High overhead tools can affect production
- Core dumps can be very large
- Some tools require root access
- Debug symbols increase binary size
- Profiling can affect timing-sensitive code

================================================================================
Last Updated: 2025-01-25
Version: 2.0
Maintainer: HFT Engineering Team
================================================================================
