================================================================================
CONTINUOUS PERFORMANCE MONITORING
REAL-TIME METRICS, ALERTING, AND OBSERVABILITY
================================================================================

MONITORING ARCHITECTURE OVERVIEW:
=================================
- Prometheus for metrics collection
- Grafana for visualization
- AlertManager for alerting
- Jaeger for distributed tracing
- Custom low-latency metrics framework

METRICS COLLECTION INTERVAL: 1 second
RETENTION PERIOD: 90 days (hot), 2 years (cold)
ALERT EVALUATION: Every 10 seconds

================================================================================
1. REAL-TIME PERFORMANCE METRICS
================================================================================

LATENCY METRICS (Updated every second):
---------------------------------------
hft_order_entry_latency_ns{percentile="50"}
hft_order_entry_latency_ns{percentile="95"}
hft_order_entry_latency_ns{percentile="99"}
hft_order_entry_latency_ns{percentile="99.9"}

hft_market_data_latency_ns{percentile="50"}
hft_market_data_latency_ns{percentile="99"}

hft_end_to_end_latency_ns{percentile="99"}

THROUGHPUT METRICS:
------------------
hft_orders_per_second
hft_messages_per_second
hft_matches_per_second
hft_risk_checks_per_second

RESOURCE METRICS:
----------------
hft_cpu_utilization_percent{core="0..63"}
hft_memory_usage_bytes
hft_network_rx_bytes_per_second
hft_network_tx_bytes_per_second
hft_disk_iops

QUALITY METRICS:
---------------
hft_order_rejection_rate
hft_error_rate
hft_message_loss_count

================================================================================
2. PROMETHEUS CONFIGURATION
================================================================================

# prometheus.yml
global:
  scrape_interval: 1s
  evaluation_interval: 10s

scrape_configs:
  - job_name: 'hft_system'
    static_configs:
      - targets: ['localhost:9090']
    metric_relabel_configs:
      - source_labels: [__name__]
        regex: 'hft_.*'
        action: keep

rule_files:
  - 'alerts/latency_alerts.yml'
  - 'alerts/throughput_alerts.yml'
  - 'alerts/resource_alerts.yml'

alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']

================================================================================
3. ALERT RULES
================================================================================

# alerts/latency_alerts.yml
groups:
  - name: latency_sla
    interval: 10s
    rules:
      - alert: HighP99Latency
        expr: hft_order_entry_latency_ns{percentile="99"} > 10000
        for: 1m
        labels:
          severity: critical
          component: order_entry
        annotations:
          summary: "P99 latency exceeds SLA"
          description: "P99 latency is {{ $value }}ns (SLA: 10000ns)"

      - alert: LatencyDegradation
        expr: |
          (hft_order_entry_latency_ns{percentile="99"} / 
           hft_order_entry_latency_ns{percentile="99"} offset 1h) > 1.2
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Latency increased by 20%"

# alerts/throughput_alerts.yml
groups:
  - name: throughput_sla
    interval: 10s
    rules:
      - alert: LowThroughput
        expr: rate(hft_orders_per_second[1m]) < 400000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Throughput below SLA target"
          description: "Current: {{ $value }} ops/s (Target: 500K)"

# alerts/resource_alerts.yml
groups:
  - name: resource_usage
    interval: 10s
    rules:
      - alert: HighCPUUsage
        expr: hft_cpu_utilization_percent > 90
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "CPU utilization above 90%"

      - alert: HighMemoryUsage
        expr: (hft_memory_usage_bytes / 274877906944) > 0.85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Memory usage above 85%"

================================================================================
4. GRAFANA DASHBOARDS
================================================================================

DASHBOARD 1: LATENCY OVERVIEW
-----------------------------
Panels:
- P50/P95/P99 latency timeseries (1-minute resolution)
- Latency heatmap
- SLA compliance gauge
- Latency distribution histogram

Query Example:
rate(hft_order_entry_latency_ns_bucket[1m])

DASHBOARD 2: THROUGHPUT & CAPACITY
-----------------------------------
Panels:
- Orders per second
- Messages per second
- CPU/Memory/Network utilization
- Headroom remaining

DASHBOARD 3: SLA COMPLIANCE
---------------------------
Panels:
- SLA compliance percentage (99.99% target)
- SLA violations count
- Uptime percentage
- Error rates

DASHBOARD 4: REAL-TIME OPERATIONS
----------------------------------
Refresh: 1 second
Panels:
- Live latency (last 60 seconds)
- Live throughput
- Active connections
- Order queue depth

================================================================================
5. CUSTOM METRICS COLLECTION FRAMEWORK
================================================================================

// Low-overhead metrics collection
// File: metrics_collector.hpp

#ifndef METRICS_COLLECTOR_HPP
#define METRICS_COLLECTOR_HPP

#include <atomic>
#include <array>
#include <algorithm>

namespace hft {
namespace metrics {

// Lock-free histogram for latency tracking
class LatencyHistogram {
private:
    static constexpr size_t NUM_BUCKETS = 1000;
    std::array<std::atomic<uint64_t>, NUM_BUCKETS> buckets_;
    
public:
    void record(uint64_t latency_ns) {
        size_t bucket = std::min(latency_ns / 1000, NUM_BUCKETS - 1);
        buckets_[bucket].fetch_add(1, std::memory_order_relaxed);
    }
    
    uint64_t get_percentile(double p) const {
        uint64_t total = 0;
        for (const auto& bucket : buckets_) {
            total += bucket.load(std::memory_order_relaxed);
        }
        
        uint64_t target = static_cast<uint64_t>(total * p);
        uint64_t sum = 0;
        
        for (size_t i = 0; i < NUM_BUCKETS; ++i) {
            sum += buckets_[i].load(std::memory_order_relaxed);
            if (sum >= target) {
                return i * 1000;  // Convert bucket to nanoseconds
            }
        }
        
        return (NUM_BUCKETS - 1) * 1000;
    }
};

// Metrics exporter for Prometheus
class PrometheusExporter {
public:
    std::string export_metrics() {
        std::ostringstream ss;
        
        // Latency metrics
        ss << "# TYPE hft_order_entry_latency_ns gauge\n";
        ss << "hft_order_entry_latency_ns{percentile=\"50\"} "
           << order_entry_histogram_.get_percentile(0.50) << "\n";
        ss << "hft_order_entry_latency_ns{percentile=\"99\"} "
           << order_entry_histogram_.get_percentile(0.99) << "\n";
        
        // Throughput metrics
        ss << "# TYPE hft_orders_per_second counter\n";
        ss << "hft_orders_per_second " << orders_count_.load() << "\n";
        
        return ss.str();
    }
    
private:
    LatencyHistogram order_entry_histogram_;
    std::atomic<uint64_t> orders_count_{0};
};

} // namespace metrics
} // namespace hft

#endif // METRICS_COLLECTOR_HPP

================================================================================
6. DISTRIBUTED TRACING CONFIGURATION
================================================================================

// Jaeger tracing for critical paths
// File: tracing_config.cpp

#include <jaegertracing/Tracer.h>

void setup_tracing() {
    auto config = jaegertracing::Config(
        false,  // disabled
        jaegertracing::samplers::Config(
            "probabilistic",
            0.001  // Sample 0.1% of requests (low overhead)
        ),
        jaegertracing::reporters::Config(
            true,  // logSpans
            "localhost",
            6831   // agent port
        )
    );
    
    auto tracer = jaegertracing::Tracer::make(
        "hft_system",
        config,
        jaegertracing::logging::consoleLogger()
    );
    
    opentracing::Tracer::InitGlobal(
        std::static_pointer_cast<opentracing::Tracer>(tracer)
    );
}

// Usage in critical path
void process_order(const Order& order) {
    auto span = opentracing::Tracer::Global()->StartSpan("process_order");
    span->SetTag("order_id", order.id);
    
    {
        auto child_span = opentracing::Tracer::Global()->StartSpan(
            "validate_order", {opentracing::ChildOf(&span->context())});
        validate_order(order);
    }
    
    {
        auto child_span = opentracing::Tracer::Global()->StartSpan(
            "send_to_exchange", {opentracing::ChildOf(&span->context())});
        send_to_exchange(order);
    }
}

================================================================================
7. LOG AGGREGATION & ANALYSIS
================================================================================

ELK STACK CONFIGURATION:
-----------------------
Elasticsearch: Store logs for 90 days
Logstash: Parse and enrich logs
Kibana: Visualization and search

Log Format (JSON):
{
  "timestamp": "2025-11-25T10:30:45.123456789Z",
  "level": "INFO",
  "component": "order_entry",
  "message": "Order processed",
  "order_id": "ORD123456",
  "latency_ns": 6500,
  "thread_id": 42
}

Critical Log Queries:
- Errors in last 5 minutes
- P99 latency violations
- Order rejections
- System anomalies

================================================================================
8. ANOMALY DETECTION
================================================================================

STATISTICAL ANOMALY DETECTION:
------------------------------
- 3-sigma rule for latency spikes
- ARIMA forecasting for trend detection
- Isolation Forest for multivariate anomalies
- Machine learning models for pattern recognition

EXAMPLE ANOMALY ALERTS:
----------------------
- Latency spike > 3x standard deviation
- Throughput drop > 20% from baseline
- Error rate increase > 100% from normal
- Memory leak detection (gradual increase)

================================================================================
9. PERFORMANCE REGRESSION MONITORING
================================================================================

AUTOMATED REGRESSION DETECTION:
-------------------------------
- Compare each commit against baseline
- Alert if P99 latency increases > 10%
- Alert if throughput decreases > 10%
- Alert if memory usage increases > 20%

CONTINUOUS BENCHMARKING:
-----------------------
- Run micro-benchmarks on every commit
- Compare against last 100 commits
- Identify performance trends
- Auto-rollback on critical regressions

================================================================================
10. MONITORING BEST PRACTICES
================================================================================

DO:
- Monitor P99 and P99.9, not just averages
- Use low-overhead metrics collection
- Alert on trends, not just thresholds
- Include SLO budget in dashboards
- Test alerting regularly

DON'T:
- Monitor too many metrics (alert fatigue)
- Use synchronous metrics collection
- Rely on averages for latency
- Ignore tail latencies
- Alert on noisy metrics

MONITORING OVERHEAD TARGET: < 1% CPU

================================================================================
END OF CONTINUOUS PERFORMANCE MONITORING
================================================================================
