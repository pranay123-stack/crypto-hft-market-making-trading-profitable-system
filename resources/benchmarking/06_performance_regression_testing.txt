================================================================================
PERFORMANCE REGRESSION TESTING FOR HFT SYSTEMS
AUTOMATED DETECTION AND PREVENTION OF PERFORMANCE DEGRADATION
================================================================================

TABLE OF CONTENTS
1. Regression Testing Framework
2. Baseline Management
3. Statistical Change Detection
4. Automated Regression Detection
5. CI/CD Integration
6. Performance Regression Root Cause Analysis
7. Historical Performance Tracking
8. Regression Prevention Strategies

================================================================================
1. REGRESSION TESTING FRAMEWORK
================================================================================

// Performance Regression Detection System
// File: regression_framework.hpp

#ifndef REGRESSION_FRAMEWORK_HPP
#define REGRESSION_FRAMEWORK_HPP

#include <vector>
#include <map>
#include <string>
#include <chrono>
#include <algorithm>
#include <numeric>
#include <cmath>
#include <fstream>
#include <json/json.h>

namespace hft {
namespace regression {

// Performance baseline
struct PerformanceBaseline {
    std::string test_name;
    std::string commit_hash;
    uint64_t timestamp;

    // Latency metrics (nanoseconds)
    double p50_latency_ns;
    double p95_latency_ns;
    double p99_latency_ns;
    double p99_9_latency_ns;
    double mean_latency_ns;
    double stddev_latency_ns;

    // Throughput metrics
    double throughput_ops_per_sec;
    double cpu_utilization;
    double memory_mb;

    // Resource metrics
    size_t peak_memory_mb;
    double avg_cpu_percent;

    // Save to JSON
    Json::Value to_json() const {
        Json::Value root;
        root["test_name"] = test_name;
        root["commit_hash"] = commit_hash;
        root["timestamp"] = Json::Value::UInt64(timestamp);
        root["p50_latency_ns"] = p50_latency_ns;
        root["p95_latency_ns"] = p95_latency_ns;
        root["p99_latency_ns"] = p99_latency_ns;
        root["p99_9_latency_ns"] = p99_9_latency_ns;
        root["mean_latency_ns"] = mean_latency_ns;
        root["stddev_latency_ns"] = stddev_latency_ns;
        root["throughput_ops_per_sec"] = throughput_ops_per_sec;
        root["cpu_utilization"] = cpu_utilization;
        root["memory_mb"] = memory_mb;
        root["peak_memory_mb"] = Json::Value::UInt64(peak_memory_mb);
        root["avg_cpu_percent"] = avg_cpu_percent;
        return root;
    }

    static PerformanceBaseline from_json(const Json::Value& root) {
        PerformanceBaseline baseline;
        baseline.test_name = root["test_name"].asString();
        baseline.commit_hash = root["commit_hash"].asString();
        baseline.timestamp = root["timestamp"].asUInt64();
        baseline.p50_latency_ns = root["p50_latency_ns"].asDouble();
        baseline.p95_latency_ns = root["p95_latency_ns"].asDouble();
        baseline.p99_latency_ns = root["p99_latency_ns"].asDouble();
        baseline.p99_9_latency_ns = root["p99_9_latency_ns"].asDouble();
        baseline.mean_latency_ns = root["mean_latency_ns"].asDouble();
        baseline.stddev_latency_ns = root["stddev_latency_ns"].asDouble();
        baseline.throughput_ops_per_sec = root["throughput_ops_per_sec"].asDouble();
        baseline.cpu_utilization = root["cpu_utilization"].asDouble();
        baseline.memory_mb = root["memory_mb"].asDouble();
        baseline.peak_memory_mb = root["peak_memory_mb"].asUInt64();
        baseline.avg_cpu_percent = root["avg_cpu_percent"].asDouble();
        return baseline;
    }
};

// Regression detection result
struct RegressionResult {
    bool regression_detected;
    std::string test_name;
    std::vector<std::string> regressions;
    double severity_score;  // 0.0 to 1.0

    struct MetricChange {
        std::string metric_name;
        double baseline_value;
        double current_value;
        double percent_change;
        bool is_regression;
        std::string severity;  // "minor", "moderate", "severe", "critical"
    };

    std::vector<MetricChange> metric_changes;

    void add_regression(const std::string& message) {
        regressions.push_back(message);
        regression_detected = true;
    }

    void print_report() const {
        std::cout << "\n=== Performance Regression Report ===\n";
        std::cout << "Test: " << test_name << "\n";
        std::cout << "Regression Detected: "
                  << (regression_detected ? "YES" : "NO") << "\n";

        if (regression_detected) {
            std::cout << "Severity Score: " << std::fixed << std::setprecision(2)
                      << (severity_score * 100) << "%\n\n";

            std::cout << "Metric Changes:\n";
            std::cout << "--------------------------------------------------------\n";
            for (const auto& change : metric_changes) {
                if (change.is_regression) {
                    std::cout << "[" << change.severity << "] "
                              << change.metric_name << "\n";
                    std::cout << "  Baseline: " << change.baseline_value << "\n";
                    std::cout << "  Current:  " << change.current_value << "\n";
                    std::cout << "  Change:   " << std::showpos
                              << change.percent_change << "%\n\n";
                }
            }

            std::cout << "Details:\n";
            for (const auto& msg : regressions) {
                std::cout << "  - " << msg << "\n";
            }
        } else {
            std::cout << "No regressions detected. Performance is stable.\n";
        }
    }
};

// Regression detector
class RegressionDetector {
private:
    std::map<std::string, std::vector<PerformanceBaseline>> baselines_;
    std::string baseline_file_;

    // Thresholds for regression detection
    struct Thresholds {
        double p50_percent = 10.0;    // 10% degradation
        double p95_percent = 15.0;    // 15% degradation
        double p99_percent = 20.0;    // 20% degradation
        double p99_9_percent = 25.0;  // 25% degradation
        double throughput_percent = 10.0;
        double memory_percent = 20.0;
    } thresholds_;

public:
    RegressionDetector(const std::string& baseline_file)
        : baseline_file_(baseline_file) {
        load_baselines();
    }

    void set_thresholds(const Thresholds& t) {
        thresholds_ = t;
    }

    // Add a new baseline
    void add_baseline(const PerformanceBaseline& baseline) {
        baselines_[baseline.test_name].push_back(baseline);
        save_baselines();
    }

    // Get the most recent baseline for a test
    PerformanceBaseline get_latest_baseline(const std::string& test_name) const {
        auto it = baselines_.find(test_name);
        if (it == baselines_.end() || it->second.empty()) {
            throw std::runtime_error("No baseline found for test: " + test_name);
        }
        return it->second.back();
    }

    // Get rolling average baseline (last N runs)
    PerformanceBaseline get_rolling_baseline(const std::string& test_name,
                                             size_t window = 10) const {
        auto it = baselines_.find(test_name);
        if (it == baselines_.end() || it->second.empty()) {
            throw std::runtime_error("No baseline found for test: " + test_name);
        }

        const auto& history = it->second;
        size_t start_idx = history.size() > window ?
                          history.size() - window : 0;

        PerformanceBaseline avg_baseline;
        avg_baseline.test_name = test_name;

        size_t count = history.size() - start_idx;

        for (size_t i = start_idx; i < history.size(); ++i) {
            avg_baseline.p50_latency_ns += history[i].p50_latency_ns;
            avg_baseline.p95_latency_ns += history[i].p95_latency_ns;
            avg_baseline.p99_latency_ns += history[i].p99_latency_ns;
            avg_baseline.p99_9_latency_ns += history[i].p99_9_latency_ns;
            avg_baseline.mean_latency_ns += history[i].mean_latency_ns;
            avg_baseline.throughput_ops_per_sec += history[i].throughput_ops_per_sec;
            avg_baseline.memory_mb += history[i].memory_mb;
        }

        avg_baseline.p50_latency_ns /= count;
        avg_baseline.p95_latency_ns /= count;
        avg_baseline.p99_latency_ns /= count;
        avg_baseline.p99_9_latency_ns /= count;
        avg_baseline.mean_latency_ns /= count;
        avg_baseline.throughput_ops_per_sec /= count;
        avg_baseline.memory_mb /= count;

        return avg_baseline;
    }

    // Detect regressions by comparing against baseline
    RegressionResult detect_regression(const PerformanceBaseline& current) {
        RegressionResult result;
        result.test_name = current.test_name;
        result.regression_detected = false;
        result.severity_score = 0.0;

        try {
            auto baseline = get_rolling_baseline(current.test_name);

            // Check each metric
            check_metric(result, "P50 Latency", baseline.p50_latency_ns,
                        current.p50_latency_ns, thresholds_.p50_percent, true);

            check_metric(result, "P95 Latency", baseline.p95_latency_ns,
                        current.p95_latency_ns, thresholds_.p95_percent, true);

            check_metric(result, "P99 Latency", baseline.p99_latency_ns,
                        current.p99_latency_ns, thresholds_.p99_percent, true);

            check_metric(result, "P99.9 Latency", baseline.p99_9_latency_ns,
                        current.p99_9_latency_ns, thresholds_.p99_9_percent, true);

            check_metric(result, "Throughput", baseline.throughput_ops_per_sec,
                        current.throughput_ops_per_sec,
                        thresholds_.throughput_percent, false);

            check_metric(result, "Memory Usage", baseline.memory_mb,
                        current.memory_mb, thresholds_.memory_percent, true);

            // Calculate overall severity
            if (!result.metric_changes.empty()) {
                double total_severity = 0.0;
                for (const auto& change : result.metric_changes) {
                    if (change.is_regression) {
                        total_severity += std::abs(change.percent_change) / 100.0;
                    }
                }
                result.severity_score = std::min(1.0,
                    total_severity / result.metric_changes.size());
            }

        } catch (const std::exception& e) {
            std::cerr << "Error detecting regression: " << e.what() << "\n";
        }

        return result;
    }

    // Statistical change detection using Mann-Whitney U test
    bool detect_distribution_change(const std::vector<double>& baseline_samples,
                                   const std::vector<double>& current_samples) {
        // Simplified implementation - use proper statistical test in production
        double baseline_median = calculate_median(baseline_samples);
        double current_median = calculate_median(current_samples);

        double percent_change = ((current_median - baseline_median) /
                                baseline_median) * 100.0;

        return std::abs(percent_change) > 10.0;  // 10% threshold
    }

private:
    void check_metric(RegressionResult& result,
                     const std::string& metric_name,
                     double baseline_value,
                     double current_value,
                     double threshold_percent,
                     bool higher_is_worse) {

        RegressionResult::MetricChange change;
        change.metric_name = metric_name;
        change.baseline_value = baseline_value;
        change.current_value = current_value;

        double percent_change = ((current_value - baseline_value) /
                                baseline_value) * 100.0;
        change.percent_change = percent_change;

        // Determine if it's a regression
        bool is_regression = false;
        if (higher_is_worse) {
            is_regression = percent_change > threshold_percent;
        } else {
            is_regression = percent_change < -threshold_percent;
        }

        change.is_regression = is_regression;

        // Classify severity
        double abs_change = std::abs(percent_change);
        if (abs_change > 50.0) {
            change.severity = "CRITICAL";
        } else if (abs_change > 30.0) {
            change.severity = "SEVERE";
        } else if (abs_change > 20.0) {
            change.severity = "MODERATE";
        } else {
            change.severity = "MINOR";
        }

        result.metric_changes.push_back(change);

        if (is_regression) {
            std::ostringstream msg;
            msg << metric_name << " regression: " << std::fixed
                << std::setprecision(1) << percent_change << "% ("
                << baseline_value << " → " << current_value << ")";
            result.add_regression(msg.str());
        }
    }

    double calculate_median(std::vector<double> data) const {
        if (data.empty()) return 0.0;
        std::sort(data.begin(), data.end());
        return data[data.size() / 2];
    }

    void load_baselines() {
        std::ifstream file(baseline_file_);
        if (!file.is_open()) {
            std::cerr << "No existing baseline file found\n";
            return;
        }

        Json::Value root;
        file >> root;

        for (const auto& test_name : root.getMemberNames()) {
            std::vector<PerformanceBaseline> test_baselines;

            for (const auto& baseline_json : root[test_name]) {
                test_baselines.push_back(
                    PerformanceBaseline::from_json(baseline_json));
            }

            baselines_[test_name] = test_baselines;
        }
    }

    void save_baselines() const {
        Json::Value root;

        for (const auto& [test_name, test_baselines] : baselines_) {
            Json::Value baselines_array(Json::arrayValue);

            for (const auto& baseline : test_baselines) {
                baselines_array.append(baseline.to_json());
            }

            root[test_name] = baselines_array;
        }

        std::ofstream file(baseline_file_);
        file << root;
    }
};

} // namespace regression
} // namespace hft

#endif // REGRESSION_FRAMEWORK_HPP

================================================================================
2. AUTOMATED REGRESSION TESTING SCRIPT
================================================================================

#!/bin/bash
# File: run_regression_tests.sh

set -e

BENCHMARK_DIR="/home/pranay-hft/Desktop/1.AI_LLM_c++_optimization/HFT_system/benchmarking"
BASELINE_FILE="${BENCHMARK_DIR}/performance_baselines.json"
RESULTS_DIR="${BENCHMARK_DIR}/regression_results"
TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
COMMIT_HASH=$(git rev-parse HEAD 2>/dev/null || echo "unknown")

mkdir -p "${RESULTS_DIR}"

echo "================================================"
echo "Performance Regression Testing Suite"
echo "Commit: ${COMMIT_HASH}"
echo "Timestamp: ${TIMESTAMP}"
echo "================================================"

# Compile regression test suite
g++ -std=c++17 -O3 -march=native -mtune=native \
    -o "${BENCHMARK_DIR}/regression_test" \
    "${BENCHMARK_DIR}/regression_test_main.cpp" \
    -ljsoncpp -pthread

# Run regression tests
"${BENCHMARK_DIR}/regression_test" \
    --baseline="${BASELINE_FILE}" \
    --commit="${COMMIT_HASH}" \
    --output="${RESULTS_DIR}/results_${TIMESTAMP}.json"

RESULT=$?

if [ $RESULT -eq 0 ]; then
    echo "✓ No performance regressions detected"
    exit 0
else
    echo "✗ Performance regression detected!"
    echo "See ${RESULTS_DIR}/results_${TIMESTAMP}.json for details"
    exit 1
fi

================================================================================
3. CI/CD INTEGRATION
================================================================================

# GitHub Actions Workflow
# File: .github/workflows/performance_regression.yml

name: Performance Regression Testing

on:
  pull_request:
    branches: [ main, develop ]
  push:
    branches: [ main ]

jobs:
  performance-test:
    runs-on: ubuntu-latest
    timeout-minutes: 30

    steps:
    - uses: actions/checkout@v3
      with:
        fetch-depth: 0  # Full history for baseline comparison

    - name: Setup C++ Environment
      run: |
        sudo apt-get update
        sudo apt-get install -y g++-11 libjsoncpp-dev

    - name: Download Performance Baselines
      run: |
        # Download from artifact storage or S3
        aws s3 cp s3://hft-benchmarks/baselines/performance_baselines.json \
          ./benchmarking/performance_baselines.json

    - name: Run Performance Benchmarks
      run: |
        cd benchmarking
        ./run_regression_tests.sh

    - name: Check for Regressions
      id: regression_check
      run: |
        if [ -f ./benchmarking/regression_detected.flag ]; then
          echo "::set-output name=regression::true"
          exit 1
        else
          echo "::set-output name=regression::false"
        fi

    - name: Upload Results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: performance-results
        path: benchmarking/regression_results/

    - name: Comment on PR
      if: github.event_name == 'pull_request' && steps.regression_check.outputs.regression == 'true'
      uses: actions/github-script@v6
      with:
        script: |
          github.rest.issues.createComment({
            issue_number: context.issue.number,
            owner: context.repo.owner,
            repo: context.repo.repo,
            body: '⚠️ Performance regression detected! Please review the benchmark results.'
          })

    - name: Update Baseline (on main branch)
      if: github.ref == 'refs/heads/main' && steps.regression_check.outputs.regression == 'false'
      run: |
        aws s3 cp ./benchmarking/performance_baselines.json \
          s3://hft-benchmarks/baselines/performance_baselines_$(date +%Y%m%d).json

================================================================================
4. JENKINS PIPELINE
================================================================================

// Jenkinsfile for performance regression testing
// File: Jenkinsfile.performance

pipeline {
    agent {
        label 'performance-test-node'
    }

    parameters {
        booleanParam(name: 'UPDATE_BASELINE',
                    defaultValue: false,
                    description: 'Update performance baseline after successful run')
    }

    environment {
        BASELINE_FILE = 'benchmarking/performance_baselines.json'
        RESULTS_DIR = 'benchmarking/regression_results'
    }

    stages {
        stage('Checkout') {
            steps {
                checkout scm
            }
        }

        stage('Build Benchmarks') {
            steps {
                sh '''
                    cd benchmarking
                    g++ -std=c++17 -O3 -march=native -mtune=native \
                        -o regression_test regression_test_main.cpp \
                        -ljsoncpp -pthread
                '''
            }
        }

        stage('Download Baseline') {
            steps {
                sh '''
                    aws s3 cp s3://hft-benchmarks/baselines/performance_baselines.json \
                        ${BASELINE_FILE}
                '''
            }
        }

        stage('Run Performance Tests') {
            steps {
                sh '''
                    cd benchmarking
                    ./run_regression_tests.sh
                '''
            }
        }

        stage('Analyze Results') {
            steps {
                script {
                    def regressionDetected = fileExists('benchmarking/regression_detected.flag')

                    if (regressionDetected) {
                        currentBuild.result = 'UNSTABLE'
                        error('Performance regression detected!')
                    }
                }
            }
        }

        stage('Update Baseline') {
            when {
                expression { params.UPDATE_BASELINE && currentBuild.result == 'SUCCESS' }
            }
            steps {
                sh '''
                    TIMESTAMP=$(date +%Y%m%d_%H%M%S)
                    aws s3 cp ${BASELINE_FILE} \
                        s3://hft-benchmarks/baselines/performance_baselines_${TIMESTAMP}.json
                '''
            }
        }
    }

    post {
        always {
            archiveArtifacts artifacts: 'benchmarking/regression_results/**/*',
                           allowEmptyArchive: true

            publishHTML([
                reportDir: 'benchmarking/regression_results',
                reportFiles: 'regression_report.html',
                reportName: 'Performance Regression Report'
            ])
        }

        unstable {
            emailext(
                subject: "Performance Regression Detected - ${env.JOB_NAME} #${env.BUILD_NUMBER}",
                body: '''Performance regression detected in build ${BUILD_NUMBER}.

Please review the attached report.

Build URL: ${BUILD_URL}''',
                to: 'performance-team@company.com',
                attachmentsPattern: 'benchmarking/regression_results/regression_report.html'
            )
        }
    }
}

================================================================================
5. HISTORICAL PERFORMANCE TRACKING
================================================================================

// Performance History Analyzer
// File: performance_history.hpp

#ifndef PERFORMANCE_HISTORY_HPP
#define PERFORMANCE_HISTORY_HPP

#include <vector>
#include <map>
#include <string>

namespace hft {
namespace regression {

class PerformanceHistory {
private:
    std::map<std::string, std::vector<PerformanceBaseline>> history_;

public:
    void add_result(const PerformanceBaseline& result) {
        history_[result.test_name].push_back(result);
    }

    // Detect long-term trends
    struct Trend {
        std::string test_name;
        std::string metric_name;
        double slope;  // Change per day
        double r_squared;  // Goodness of fit
        bool is_degrading;
    };

    std::vector<Trend> analyze_trends(size_t min_samples = 10) const {
        std::vector<Trend> trends;

        for (const auto& [test_name, results] : history_) {
            if (results.size() < min_samples) continue;

            // Analyze P99 latency trend
            trends.push_back(calculate_trend(test_name, "P99 Latency",
                results, [](const PerformanceBaseline& b) {
                    return b.p99_latency_ns;
                }));

            // Analyze throughput trend
            trends.push_back(calculate_trend(test_name, "Throughput",
                results, [](const PerformanceBaseline& b) {
                    return b.throughput_ops_per_sec;
                }));
        }

        return trends;
    }

    void generate_html_report(const std::string& output_file) const {
        std::ofstream html(output_file);

        html << R"(
<!DOCTYPE html>
<html>
<head>
    <title>Performance History Report</title>
    <script src="https://cdn.plot.ly/plotly-latest.min.js"></script>
    <style>
        body { font-family: Arial, sans-serif; margin: 20px; }
        .chart { margin: 20px 0; }
        table { border-collapse: collapse; width: 100%; }
        th, td { border: 1px solid #ddd; padding: 8px; text-align: left; }
        th { background-color: #4CAF50; color: white; }
    </style>
</head>
<body>
    <h1>Performance History Report</h1>
)";

        // Generate charts for each test
        for (const auto& [test_name, results] : history_) {
            html << "<div class='chart' id='chart_" << test_name << "'></div>\n";
            html << "<script>\n";
            html << "var trace = {\n";
            html << "  x: [";

            for (size_t i = 0; i < results.size(); ++i) {
                if (i > 0) html << ", ";
                html << results[i].timestamp;
            }

            html << "],\n  y: [";

            for (size_t i = 0; i < results.size(); ++i) {
                if (i > 0) html << ", ";
                html << results[i].p99_latency_ns;
            }

            html << "],\n  type: 'scatter',\n  name: 'P99 Latency'\n};\n";
            html << "var layout = { title: '" << test_name << " - P99 Latency Over Time' };\n";
            html << "Plotly.newPlot('chart_" << test_name << "', [trace], layout);\n";
            html << "</script>\n";
        }

        html << "</body></html>\n";
    }

private:
    template<typename Func>
    Trend calculate_trend(const std::string& test_name,
                         const std::string& metric_name,
                         const std::vector<PerformanceBaseline>& results,
                         Func value_getter) const {

        // Simple linear regression
        size_t n = results.size();
        double sum_x = 0, sum_y = 0, sum_xy = 0, sum_xx = 0;

        for (size_t i = 0; i < n; ++i) {
            double x = static_cast<double>(i);
            double y = value_getter(results[i]);

            sum_x += x;
            sum_y += y;
            sum_xy += x * y;
            sum_xx += x * x;
        }

        double slope = (n * sum_xy - sum_x * sum_y) / (n * sum_xx - sum_x * sum_x);

        Trend trend;
        trend.test_name = test_name;
        trend.metric_name = metric_name;
        trend.slope = slope;
        trend.is_degrading = slope > 0;  // Positive slope = getting worse

        // Calculate R²
        double mean_y = sum_y / n;
        double ss_tot = 0, ss_res = 0;

        for (size_t i = 0; i < n; ++i) {
            double y = value_getter(results[i]);
            double y_pred = slope * i + (sum_y - slope * sum_x) / n;

            ss_tot += (y - mean_y) * (y - mean_y);
            ss_res += (y - y_pred) * (y - y_pred);
        }

        trend.r_squared = 1.0 - (ss_res / ss_tot);

        return trend;
    }
};

} // namespace regression
} // namespace hft

#endif // PERFORMANCE_HISTORY_HPP

================================================================================
REGRESSION TESTING BEST PRACTICES
================================================================================

BASELINE MANAGEMENT:
===================
1. Use rolling averages (10-20 runs) as baseline
2. Update baselines only after code review and approval
3. Maintain historical baselines for trend analysis
4. Tag baselines with commit hash and timestamp
5. Store baselines in version control or artifact storage

THRESHOLD CONFIGURATION:
=======================
Metric          | Warning Threshold | Critical Threshold
----------------|-------------------|-------------------
P50 Latency     | +10%             | +20%
P95 Latency     | +15%             | +25%
P99 Latency     | +20%             | +30%
P99.9 Latency   | +25%             | +40%
Throughput      | -10%             | -20%
Memory Usage    | +20%             | +30%

CONTINUOUS MONITORING:
=====================
- Run regression tests on every pull request
- Daily regression runs on main branch
- Weekly comprehensive benchmark suite
- Monthly trend analysis and reporting
- Quarterly baseline review and update

ALERT ESCALATION:
=================
1. Warning: Notify developer, log to dashboard
2. Critical: Block merge, notify team lead
3. Sustained degradation: Escalate to architecture team
4. Production impact: Immediate rollback procedure

================================================================================
END OF PERFORMANCE REGRESSION TESTING
================================================================================
