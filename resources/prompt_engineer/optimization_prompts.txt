================================================================================
AI PROMPTS FOR PERFORMANCE OPTIMIZATION
================================================================================

These prompts are designed to extract maximum performance from HFT C++ code.
Use with Claude, GPT-4, or other LLMs for optimization guidance.

⚠️ ALWAYS profile before and after optimization to validate improvements!

================================================================================
PROMPT 1: OPTIMIZE CRITICAL PATH LATENCY
================================================================================

```
Optimize the critical path latency of this HFT code:

[Paste your code here]

Current Performance:
- Average latency: [X] microseconds
- P99 latency: [Y] microseconds
- P99.9 latency: [Z] microseconds
- Instructions per cycle (IPC): [value]
- Cache miss rate: [L1/L2/L3 percentages]
- Branch misprediction rate: [percentage]

Profiling Data:
[Paste perf stat output or other profiling results]

Optimization Goals:
1. Target latency: <[X] nanoseconds
2. Minimize cache misses
3. Reduce branch mispredictions
4. Improve instruction-level parallelism
5. Eliminate unnecessary allocations
6. Optimize memory access patterns

Please provide:
- Optimized code with detailed comments on each change
- Explanation of each optimization technique used
- Expected performance improvement for each change
- Trade-offs and potential side effects
- Compiler flags for maximum performance
- Assembly analysis (if relevant)
- Alternative approaches with pros/cons
```

================================================================================
PROMPT 2: MEMORY LAYOUT OPTIMIZATION
================================================================================

```
Optimize the memory layout of these data structures for cache efficiency:

[Paste struct/class definitions]

Current Issues:
- Cache misses: [percentage or count]
- False sharing detected: [yes/no, where]
- Structure sizes: [bytes for each structure]
- Alignment: [current alignment]

Usage Pattern:
- Access frequency: [which fields are hot]
- Access order: [sequential vs random]
- Thread access: [single vs multi-threaded]
- Typical operations: [list common operations]

Hardware:
- Cache line size: 64 bytes
- L1 cache: [size]
- L2 cache: [size]
- L3 cache: [size]
- NUMA: [yes/no, nodes]

Optimize for:
1. Cache-line alignment (64 bytes)
2. Hot/cold data separation
3. False sharing elimination
4. Structure packing
5. Array-of-structures vs structure-of-arrays
6. NUMA-aware placement

Provide:
- Optimized structure layouts
- Memory layout diagrams (visual ASCII art)
- Size calculations and padding explanation
- Access pattern analysis
- Before/after cache hit rate estimates
- Thread safety considerations
- Example usage code
```

================================================================================
PROMPT 3: SIMD OPTIMIZATION
================================================================================

```
Optimize this computation-heavy code using SIMD instructions:

[Paste computational code]

Computation Details:
- Operation type: [addition, multiplication, comparison, etc.]
- Data type: [int32, int64, float, double]
- Array sizes: [typical lengths]
- Throughput: [operations per second]

Target CPU:
- Architecture: [x86-64, ARM]
- SIMD support: [SSE4.2, AVX2, AVX-512, NEON]
- Vector width: [128-bit, 256-bit, 512-bit]

Optimization Requirements:
1. Use appropriate SIMD intrinsics (AVX2/AVX-512)
2. Vectorize loops where possible
3. Handle remainder elements (non-multiple of vector width)
4. Maintain numerical accuracy
5. Add compile-time CPU feature detection
6. Provide scalar fallback implementation

Provide:
- SIMD-optimized code with intrinsics
- Explanation of vectorization strategy
- Performance comparison (scalar vs SIMD)
- Portability considerations (x86 vs ARM)
- Compiler auto-vectorization hints
- Testing strategy for correctness
- Throughput improvements (ops/second)
```

================================================================================
PROMPT 4: LOCK-FREE ALGORITHM OPTIMIZATION
================================================================================

```
Optimize this lock-based code to be lock-free or wait-free:

[Paste code using mutexes/locks]

Current Performance:
- Lock contention: [percentage or count]
- Average lock hold time: [nanoseconds]
- Throughput: [operations per second]
- Threads: [number of competing threads]

Usage Pattern:
- Producer threads: [count]
- Consumer threads: [count]
- Operation types: [read, write, read-modify-write]
- Data structure: [queue, stack, hashmap, etc.]

Concurrency Requirements:
1. Thread safety (obviously)
2. Progress guarantees: [lock-free or wait-free]
3. Memory ordering: [relaxed, acquire-release, seq_cst]
4. ABA problem handling
5. Memory reclamation strategy

Provide:
- Lock-free implementation using std::atomic
- Memory ordering annotations and justification
- ABA problem solution (if applicable)
- Memory reclamation approach (hazard pointers, epoch-based)
- Correctness proof or reasoning
- Performance comparison (locks vs lock-free)
- Failure modes and edge cases
- Testing strategy (race condition detection)
```

================================================================================
PROMPT 5: BRANCH PREDICTION OPTIMIZATION
================================================================================

```
Optimize branch prediction for this code:

[Paste code with many branches]

Profiling Data:
- Branch misprediction rate: [percentage]
- Hot branches: [which branches mispredict most]
- Branch patterns: [predictable vs unpredictable]

Context:
- Execution frequency: [how often this code runs]
- Data characteristics: [predictable vs random]
- Performance impact: [cycles lost to mispredictions]

Optimization Strategies:
1. Eliminate branches with branchless code (bit tricks)
2. Reorder branches (likely first)
3. Use [[likely]] and [[unlikely]] attributes (C++20)
4. Replace branches with lookup tables
5. Use CMOV instructions (conditional move)
6. Profile-Guided Optimization (PGO)

Provide:
- Optimized code with reduced branches
- Branchless alternatives where applicable
- Explanation of each optimization
- Assembly comparison (before/after)
- PGO compilation instructions
- Performance impact estimation
- Trade-offs (code complexity vs performance)
```

================================================================================
PROMPT 6: SYSTEM-LEVEL OPTIMIZATION
================================================================================

```
Optimize system-level configuration for this HFT application:

Application Details:
- Threading model: [single-threaded, thread pool, dedicated threads]
- Critical threads: [list and purpose]
- Network I/O: [polling, interrupts, kernel bypass]
- Memory usage: [typical and peak]
- CPU affinity: [current configuration]

Current System Config:
- CPU: [model, cores, frequency]
- OS: [Linux kernel version]
- NIC: [model, driver]
- NUMA nodes: [count and configuration]

Performance Issues:
- Latency spikes: [tail latency issues]
- Context switches: [count per second]
- CPU migrations: [frequency]
- IRQ handling: [CPU usage]
- Memory allocation: [page faults]

Optimize:
1. CPU isolation (isolcpus kernel parameter)
2. Thread affinity (pin critical threads)
3. IRQ affinity (pin network IRQs)
4. Kernel tuning (scheduler, network stack)
5. Huge pages configuration
6. NUMA policy
7. CPU frequency scaling (disable turbo boost)
8. Disable unnecessary services

Provide:
- Complete kernel boot parameters
- Runtime tuning commands (sysctl, etc.)
- Thread affinity configuration code
- NUMA policy code
- Performance monitoring commands
- Before/after latency comparison
- Testing methodology
- Rollback procedures
```

================================================================================
PROMPT 7: COMPILER OPTIMIZATION
================================================================================

```
Optimize compiler flags and code for maximum performance:

Code: [Paste code to optimize]

Current Build:
- Compiler: [GCC, Clang, ICC version]
- Flags: [current compiler flags]
- Optimization level: [-O2, -O3, -Ofast]

Target CPU:
- Architecture: [specific CPU model]
- Features: [AVX2, AVX-512, etc.]

Optimization Goals:
1. Maximum performance (latency-critical)
2. Predictable performance (avoid variability)
3. Binary size: [not a concern / minimize]
4. Compilation time: [not a concern / minimize]

Optimize for:
- Target CPU architecture (-march=native)
- Loop optimizations (unrolling, vectorization)
- Inlining (aggressive, controlled)
- Link-time optimization (LTO)
- Profile-guided optimization (PGO)
- Whole program optimization
- Debug symbol handling

Provide:
- Optimal compiler flags for each build type (dev, production)
- Code annotations for optimization hints
- Profile-guided optimization workflow
- Verification steps (assembly inspection)
- Performance comparison (different flag combinations)
- Trade-offs and recommendations
- Continuous integration script
```

================================================================================
PROMPT 8: DATABASE QUERY OPTIMIZATION
================================================================================

```
Optimize these database queries for low-latency access:

[Paste SQL queries or database access code]

Database:
- Type: [PostgreSQL, TimescaleDB, ClickHouse, Redis]
- Size: [row count, data volume]
- Indexes: [current indexes]
- Query frequency: [queries per second]

Current Performance:
- Query latency: [average and P99]
- Throughput: [queries per second]
- Database CPU: [percentage]
- I/O wait: [percentage]

Access Patterns:
- Read/Write ratio: [percentage]
- Hot data: [most frequently accessed data]
- Query complexity: [simple vs complex joins]
- Time range: [recent data vs historical]

Optimize:
1. Index design (B-tree, hash, covering)
2. Query structure (joins, subqueries)
3. Partitioning strategy (time-based, hash)
4. Caching layer (Redis, in-memory)
5. Connection pooling
6. Prepared statements
7. Batch operations
8. Materialized views

Provide:
- Optimized queries with EXPLAIN ANALYZE
- Index creation statements
- Partitioning strategy
- Cache invalidation strategy
- Connection pool configuration
- Performance comparison (before/after)
- Monitoring queries
```

================================================================================
PROMPT 9: NETWORK OPTIMIZATION
================================================================================

```
Optimize network performance for this HFT application:

Network Details:
- Protocol: [TCP, UDP, multicast]
- Message rate: [messages per second]
- Message size: [average and distribution]
- Latency requirement: [target latency]
- NIC: [model and capabilities]

Current Performance:
- Network latency: [one-way, round-trip]
- Packet loss: [percentage]
- CPU usage for network I/O: [percentage]
- Interrupt rate: [per second]

Application Code:
[Paste network I/O code]

Optimize:
1. Socket options (TCP_NODELAY, SO_RCVBUF, etc.)
2. Kernel bypass (DPDK, Solarflare OpenOnload)
3. Zero-copy techniques
4. Busy polling vs interrupts
5. Receive-side scaling (RSS)
6. Interrupt coalescing
7. Ring buffer sizing
8. NIC offloading features

Provide:
- Optimized socket configuration code
- Kernel tuning parameters (sysctl)
- NIC configuration (ethtool)
- Kernel bypass integration (if applicable)
- IRQ affinity configuration
- Performance benchmarks (netperf, iperf3)
- Latency measurement code
- Monitoring and diagnostics
```

================================================================================
PROMPT 10: ALGORITHMIC OPTIMIZATION
================================================================================

```
Optimize the algorithmic complexity of this code:

[Paste algorithm implementation]

Current Complexity:
- Time complexity: [O(n), O(n log n), O(n²), etc.]
- Space complexity: [O(1), O(n), etc.]
- Input size: [typical and worst-case]
- Execution time: [microseconds, milliseconds]

Context:
- Frequency: [how often this runs]
- Input characteristics: [sorted, random, etc.]
- Memory constraints: [any limits]
- Parallelization: [possible or not]

Optimization Goals:
1. Reduce time complexity (better algorithm)
2. Reduce space complexity (in-place operations)
3. Exploit input characteristics
4. Use better data structures
5. Parallelize if possible (SIMD, threads)

Alternative Algorithms to Consider:
- [List potential algorithms for the problem]

Provide:
- Optimized algorithm with better complexity
- Detailed complexity analysis (big-O notation)
- Explanation of why it's faster
- Trade-offs (time vs space, simplicity vs performance)
- Implementation in C++
- Benchmark comparison (multiple input sizes)
- Corner cases and edge case handling
```

================================================================================
OPTIMIZATION WORKFLOW
================================================================================

Standard optimization process:

1. **Profile First** (ALWAYS!)
   - Use perf, gprof, or other profilers
   - Identify hot spots (top 3 functions)
   - Measure cache misses, branch mispredictions
   - Establish baseline metrics

2. **Optimize Hot Paths**
   - Focus on top 3 bottlenecks
   - One optimization at a time
   - Measure after each change

3. **Validate Results**
   - Re-profile to verify improvement
   - Check for regressions elsewhere
   - Validate correctness (unit tests)
   - Test edge cases

4. **Iterate**
   - If target not met, repeat
   - Consider system-level optimizations
   - Review algorithm choices

5. **Document**
   - Record optimizations made
   - Document trade-offs
   - Note compiler flags used
   - Save profiling results

================================================================================
OPTIMIZATION ANTI-PATTERNS (AVOID THESE!)
================================================================================

❌ Optimizing without profiling (guessing bottlenecks)
❌ Premature optimization (optimize before it's a problem)
❌ Micro-optimizations that hurt readability
❌ Optimizing cold paths (rarely executed code)
❌ Ignoring algorithmic complexity (polishing a slow algorithm)
❌ Breaking correctness for minor speed gains
❌ Over-engineering (adding complexity for theoretical benefits)
❌ Optimizing in isolation (ignoring system context)
❌ Forgetting to validate (assuming optimization worked)
❌ Optimizing without version control (can't rollback)

================================================================================
PERFORMANCE TARGETS FOR HFT
================================================================================

Industry-standard latency targets:

- Order generation: <100 nanoseconds
- Pre-trade risk check: <5 microseconds
- Order serialization: <1 microsecond
- Network transmission: <10 microseconds (kernel bypass)
- End-to-end (tick-to-trade): <100 microseconds

Cache miss rates:
- L1: <3%
- L2: <10%
- L3: <20%

CPU metrics:
- Instructions per cycle (IPC): >2.0
- Branch misprediction: <5%
- Context switches: <100 per second

Memory:
- Page faults: 0 in hot path
- Allocations: 0 in hot path (after initialization)

================================================================================
